ID,Name,Incident Type,Status,Severity,Summary,Incident Manager,Reporter,Comms Lead,IM SLT,Tech Lead,Impacted Platform,Impacted Client(s),Resolution (Fix) ,Impact ,Fault Responsibility,Zoom Meeting Link,VoucherEngine Component,Root Cause Analysis Summary,Repeat Incident,Monitoring,Detection & Triage,Investigation & Resolution,Client Communication,Affected services,Environment Type,Post-Mortem Document,Created At,Reported at,Accepted at,Declined at,Merged at,Canceled at,Resolved at,Impact started at,Identified at,Fixed at,Documented at,Reviewed at,Closed at,Incident duration,Updated At,Creator Alert ID,Visibility,Workload Total (minutes),Workload In Working Hours (minutes),Workload In Late Hours (minutes),Workload In Sleeping Hours (minutes),Channel name,Channel link,Source,Opted out of post-incident flow
18,"API calls  - Customers are not able to access the service like card details, balance, and others",Service Incident,Closed,P1,"Customers are not able to access the service like card details, balance, and others",Snothile Dlamini,Vicnesh Baskar,,,,VoucherEngine,Vodacom Tanzania,Unknown (fix implemented by client),"Customers are not able to access the service like card details, balance, and others",,,,,,,,,,,,,2024-12-18T05:34:44.546Z,2024-12-18T05:34:44.546Z,,,,,2024-12-18T11:37:12.854Z,2024-12-17T12:40:00.000Z,2024-12-18T11:37:12.854Z,,,,2024-12-18T11:37:12.854Z,21748,2024-12-18T11:37:13.035Z,,public,219.7,207.6,0,12.1,18dec-p1-critical-18-api-calls-customers-are-not-able-to-access-the-service,https://slack.com/app_redirect?team=T03SMJWS8&channel=C085HLCDV5K,Reported by Vicnesh Baskar,false
24,No Transactions processing for C24,Service Incident,Closed,P1,No actions taken by PTY. Issue to be investigated with Pulsant and MC,Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,"C24, Gnosis, Coverflex",,,TBD,,,,,,,,,,,,2024-12-19T12:23:35.135Z,2024-12-19T12:23:35.135Z,,,,,2024-12-19T13:21:24.307Z,2024-12-19T12:11:00.000Z,2024-12-19T12:39:13.986Z,2024-12-19T13:21:24.307Z,,,2024-12-19T13:21:24.307Z,3469,2024-12-19T13:50:33.255Z,,public,733.2,537.9,165.4,30,19dec-p1-critical-24-no-transactions-processing-for-c24,https://slack.com/app_redirect?team=T03SMJWS8&channel=C085UAXT3FD,Reported by Snothile Dlamini,false
27,Intermittent transaction timeout,Service Incident,Closed,P2,"AS to further investigate the 09 second delay in Rule Engine, causing the timeouts. ",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,"Nomo Bank, Utility Warehouse",None,BB2: 12 failed transactions 87 successful transactions UWH 115 failed transactions 792 successful transactions,,,,,,,,,,,,,2024-12-20T12:58:36.418Z,2024-12-20T10:30:00.000Z,,,,,2024-12-20T10:30:00.000Z,2024-12-20T10:27:00.000Z,2024-12-20T10:30:00.000Z,,,,2024-12-20T10:59:00.000Z,,2024-12-20T13:01:48.570Z,,public,86.3,68.7,17.6,0,20dec-p2-high-27-intermittent-transaction-timeout,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0860F503K5,Reported by Snothile Dlamini,false
28,Partial Outage on Transaction Processing for UW,Service Incident,Closed,P2,"There was a brief interruption to transaction processing (partial outage) from 10:31 UTC to 10:32 UTC on 24th Dec. 60 failed transactions / 150 successful transactions. Issue resolved itself, no action taken by any support team.  Currently transactions are processing as expected with no issues. AS team is investigating further to identify the root cause of the issue, suspected it is related to rules engine taking longer than normal. Monitoring ongoing. ",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,Utility Warehouse,No Action taken by any support team. Issue resolved itself.,Partial Outage on Transaction Processing  ,Paymentology,,,,,,,,,,,,2024-12-24T11:27:58.898Z,2024-12-24T11:27:58.898Z,,,,,2024-12-24T11:30:35.237Z,2024-12-24T02:31:00.000Z,2024-12-24T11:30:35.237Z,,,,2024-12-24T11:30:35.237Z,156,2024-12-24T11:30:35.468Z,,public,96.3,54.7,41.6,0,24dec-p2-high-28-partial-outage-on-transaction-processing-for-uw,https://slack.com/app_redirect?team=T03SMJWS8&channel=C086AA66MKP,Reported by Kaisar Mahmood,false
29,Partial Outage of Transaction Processing for D360 ,Service Incident,Closed,P1,Marking the incident as closed. Issue was resolved on 5th Jan. Incident Report (IR) and RCA Report (RCAR) have already been shared with the client.,Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,D360,Final fix implementation to restore service was completed at 14:55 UTC on 5th January. Problematic DNS server name entries were removed from system configurations on the Primary and Secondary PayPower application servers.,D360 has been observing a partial outage on transaction processing since 29th Dec 2024. We have not hit processing SLA since 29th Dec. Specially from 12:45 AM UTC 3rd Jan to 10:25 AM UTC 3rd Jan we have received 13495 (1200 - MADA transaction). In that 107 transaction are identified as impacted.,Paymentology,,,,,,,,,,,,2025-01-03T10:52:16.705Z,2025-01-03T10:52:16.705Z,,,,,2025-01-10T12:25:49.748Z,2024-12-28T16:00:00.000Z,2025-01-10T12:25:49.748Z,,,,2025-01-10T12:25:49.748Z,610413,2025-01-10T12:25:49.905Z,,public,4462.1,3235.2,710.6,516.2,03jan-p1-critical-29-partial-outage-of-transaction-processing-for-d360,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0878GVS4AG,Reported by Kaisar Mahmood,false
33,SH02 Timeout due to a rule engine delay.,Service Incident,Closed,P3 - Critical,"16-second timeout caused by a rule engine delay between 13:51:00 and 13:51:16 UTC. The clients affected included UW (14 transactions) and BB2 (3 transactions), and the overall impact exceeded 30%, prompting escalation to P1 priority. The root cause is suspected to be related to database contention or background jobs affecting the rule engine's performance. While the issue resolved itself during the initial outage, an investigation is ongoing to determine the exact cause.",Snothile Dlamini,Snothile Dlamini,,,,,"Utility Warehouse, Nomo Bank",None,,Paymentology,,,,,,,,,,,,2025-01-10T08:24:04.731Z,2025-01-09T11:51:00.000Z,,,,,2025-01-09T11:51:00.000Z,2025-01-09T11:51:00.000Z,,,2025-01-10T08:27:52.585Z,2025-01-10T08:31:22.078Z,2025-01-10T08:31:22.078Z,,2025-01-10T09:41:03.495Z,,public,44.2,44.2,0,0,10jan-p3-medium-33-sh02-timeout-due-to-a-rule-engine-delay,https://slack.com/app_redirect?team=T03SMJWS8&channel=C088FUXDWE5,Reported by Snothile Dlamini,true
31,Full Outage of Mastercard Transactions for BOJ ,Service Incident,Closed,P1,"There was a full outage of MC transactions from 11:32 UTC 9th Jan till 11:42 UTC 9th Jan for BOJ. As of 11:42 UTC on 9th Jan we are processing as expected. To restore service, Advanced Support team restarted PayRoute instances. Root cause investigation is in progress. ",Snothile Dlamini,Kaisar Mahmood,,,,Banking.Live,Bank of Jordan,Advanced Support team restarted PayRoute instances,Full outage of Mastercard transactions for BOJ for 10 mins ,Paymentology,,,,,,,,,,,,2025-01-09T11:53:30.478Z,2025-01-09T11:53:30.478Z,,,,,2025-01-09T12:06:58.051Z,2025-01-09T03:32:00.000Z,2025-01-09T12:06:58.051Z,,,,2025-01-09T12:06:58.051Z,807,2025-01-09T12:10:09.567Z,,public,262.8,206.2,56.6,0,09jan-p1-critical-31-full-outage-of-mastercard-transactions-for-boj,https://slack.com/app_redirect?team=T03SMJWS8&channel=C087Y38BQFL,Reported by Kaisar Mahmood,false
32,Partial Outage of Transaction processing for DoPay,Service Incident,Closed,P1,"On 09 Dec 18:35 UTC, the Paymentology support team received an alert from the internal monitoring system that Paymentology was not receiving a response from the AFS (American Financial Services) scheme. The AFS PayRoute application was restarted, and new transactions were observed from 09 Dec 19:06 onwards.",Vicnesh Baskar,Vicnesh Baskar,,,,,Dopay LLC,AFS PayRoute application was restarted,,Third-Party - Client,,,,,,,,,,,,2025-01-10T07:34:26.223Z,2025-01-10T07:34:26.223Z,,,,,2025-01-09T11:06:00.000Z,2025-01-09T10:35:00.000Z,2025-01-09T10:35:00.000Z,2025-10-09T11:05:00.000Z,,,,,2026-02-02T10:34:12.560Z,,public,128.9,102.5,26.4,0,10jan-p1-32-partial-outage-of-transaction-processing-for-dopay,https://slack.com/app_redirect?team=T03SMJWS8&channel=C087NGLJ0B1,Reported by Vicnesh Baskar,false
34,Create Card Stack Issue for MOX ,Service Incident,Closed,P1,Issue resolved at 14:01 UTC on 10th January 2025 after Advanced Support team manually increased the card stack size/limit. In total 26 create card requests were impacted. Monitoring and root cause investigation in progress.,Daniel Velado,Kaisar Mahmood,,,,Banking.Live,MOX,Advanced Support manually increased the card stack size/limit,Create card issue for one product - in total 26 requests were impacted. ,Paymentology,,,,,,,,,,,,2025-01-10T14:09:44.334Z,2025-01-10T14:09:44.334Z,,,,,2025-01-10T14:39:56.352Z,2025-01-10T05:47:00.000Z,2025-01-10T14:39:56.352Z,,,,2025-01-10T14:39:56.352Z,1812,2025-01-10T16:09:32.977Z,,public,410.5,225.4,136.2,48.9,10jan-p1-critical-34-create-card-stack-issue-for-mox,https://slack.com/app_redirect?team=T03SMJWS8&channel=C087Y49LMUN,Reported by Kaisar Mahmood,false
35,PayRoute disconnection from UAE Switch,Service Incident,Closed,P1,"On January 10, from 22:38 to 22:56 UTC, a disconnection occurred between Payroute and the UAE Switch, impacting transaction processing during this period. Although the connection has since been restored and transactions are now processing normally, the exact number of impacted transactions remains unclear due to a lack of advisements. The issue has been escalated as a P1 incident, and investigations are underway with the infrastructure and tooling teams to determine the root cause and prevent recurrence. A Zendesk ticket (#898181) has been created for tracking, and the client has been informed that updates will be provided as more information becomes available.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,Wio,,No advisements received for UAE,Third-Party - Client,,,,,,,,,,,,2025-01-11T06:42:11.466Z,2025-01-10T23:36:00.000Z,,,,,2025-01-10T20:56:00.000Z,2025-01-10T20:34:00.000Z,2025-01-11T06:44:47.968Z,,,,2025-01-11T06:44:47.968Z,,2026-02-02T10:35:20.804Z,,public,231.3,152,69.3,10,11jan-p1-35-payroute-disconnection-from-uae-switch,https://slack.com/app_redirect?team=T03SMJWS8&channel=C088LRV3473,Reported by Snothile Dlamini,false
37,Transaction and API Processing Issue,Service Incident,Closed,P1,"An issue was reported with ADQ transaction processing, showing errors and 100% CPU usage on the Paycore database. Paycontrol became unresponsive, prompting a restart of PayPower and PayAPI as a precautionary measure.",Daniel Velado,Snothile Dlamini,,,,Banking.Live,Wio,"Indexes were created on the p_card_wallet_token table to improve transaction processing speeds, and transactions are now processing fine.",,Paymentology,,,,,,,,,,,,2025-01-14T13:26:02.248Z,2025-01-14T11:18:00.000Z,,,,,2025-01-14T11:25:00.000Z,2025-01-14T11:15:00.000Z,2025-01-14T14:16:34.294Z,2025-01-14T14:28:07.280Z,,,2025-01-14T14:28:07.280Z,420,2025-01-14T15:21:24.237Z,,public,558.3,406.1,119.9,32.3,14jan-p1-critical-37-transaction-and-api-processing-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C088N6P9KU4,Reported by Snothile Dlamini,false
38,Transaction processing issue on PROD_ALTC_4B.,Service Incident,Post-Incident Tasks & Documentation,P1,No additional errors were observed.,Daniel Velado,Daniel Velado,,,,VoucherEngine,SBSA Virtual Card,Altec to generate a new key on failovers,,Third-Party - Paymentology,,,,,,,,,,,,2025-01-15T15:55:12.195Z,2025-01-15T15:55:12.195Z,,,,,2025-01-15T16:42:01.646Z,2015-01-15T21:47:00.000Z,2025-01-15T16:04:37.421Z,,,,,2809,2025-01-15T16:42:02.542Z,,public,157.7,147.7,0,10,15jan-p1-critical-38-transaction-processing-issue-on-prod_altc_4b,https://slack.com/app_redirect?team=T03SMJWS8&channel=C088SSKD7A7,Reported by Daniel Velado,false
39,Transaction Processing Issue,Service Incident,Closed,P1,"Mastercard sent notifications regarding a transaction issue, and the AS team observed that no transactions were being received for one of the Mastercard PayRoute 4IR channels. The team restarted the PayRoute, which resolved the issue. The RCA is currently under investigation.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,Tweeq,,,Third-Party - Paymentology,,,,,,,,,,,,2025-01-20T14:00:31.131Z,2025-01-20T14:00:31.131Z,,,,,2025-01-20T11:35:00.000Z,2025-01-20T08:40:00.000Z,2025-01-20T14:38:08.781Z,,,,2025-01-20T14:38:08.781Z,,2025-01-20T14:38:08.874Z,,public,126.8,115.8,11,0,20jan-p1-critical-39-transaction-processing-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C089H8NK8FN,Reported by Snothile Dlamini,false
40,Transaction Processing Issue for OM client,Service Incident,Closed,P1,"Issue has been resolved, Mastercard implemented a fix at 07:40 UTC on 22nd Jan. We can now see successful transactions. Pending for Mastercard to share the full RCA report.",Kaisar Mahmood,Daniel Velado,,,,Banking.Live,Old Mutual Bank / Olympus,Mastercard implemented a fix,,Third-Party - Paymentology,,,,,,,,,,,,2025-01-20T15:19:27.828Z,2025-01-20T15:19:27.828Z,,,,,2025-01-22T11:47:30.417Z,,2025-01-20T20:10:55.771Z,2025-01-22T11:47:30.417Z,,,2025-01-22T11:47:30.417Z,138721,2025-01-22T11:47:30.593Z,,public,1885.1,1603.1,227.6,54.4,20jan-p1-critical-40-transaction-processing-issue-for-om-client,https://slack.com/app_redirect?team=T03SMJWS8&channel=C089F2DHHL3,Reported by Daniel Velado,false
42,Partial Outage of Transaction Processing for BAE,Service Incident,Closed,P1,"1 minute partial outage on transaction processing occurred from 12:49 UTC till 12:50 UTC on 22nd Jan 2025. Disconnection occurred between Paycore DB and PayPower.  Issue resolved itself, no action was taken by any support teams. Transactions are processing fine since 12:50 UTC. Investigation into the root cause of the disconnection is in progress. AS team has raised tickets to DB and Dev team for further investigation.",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,Bank Al Etihad,"Issue resolved itself, no action taken",Partial Outage for 1 minute - 66 transactions successfully processed / 39 transactions failed. ,Paymentology,,,,,,,,,,,,2025-01-22T13:14:20.838Z,2025-01-22T13:14:20.838Z,,,,,2025-01-22T14:21:52.354Z,2025-01-22T04:49:00.000Z,2025-01-22T14:21:52.354Z,,,,2025-01-22T14:21:52.354Z,4051,2025-01-22T14:21:52.601Z,,public,189.1,79.7,109.4,0,22jan-p1-critical-42-partial-outage-of-transaction-processing-for-bae,https://slack.com/app_redirect?team=T03SMJWS8&channel=C089PBHLNVB,Reported by Kaisar Mahmood,false
44,Partial Outage of ATM transaction processing for Mukuru & Nupay,Service Incident,Closed,P1,"On 25 January 2025, at 06:11 UTC, Paymentology’s support team was alerted to an issue where ATM transactions were not disbursing funds while still deducting balances from cardholder accounts.

Upon investigation, the Paymentology system successfully approved transactions but failed to dispense cash at ATMs. The incident affected multiple banks’ ATM networks, with notable issues reported for South African Standard Bank, FNB, and ABSA ATMs. This behaviour was consistent across all reported cases, suggesting an issue downstream in the ATM or acquirer system.

ATM withdrawals started processing normally, with the first confirmation on 25 January at 08:07 UTC. Monitoring confirmed successful transaction processing thereafter. Reversal activity for Mukuru was completed by 29th January and for Nupay by 1st February.

The incident was escalated to both ACS and Mastercard. Mastercard confirmed declines due to system errors (Response Code 96), root cause analysis is currently in progress.

Further updates on corrective actions will be provided in the Root Cause Analysis Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,,VoucherEngine,"Mukuru, Altron FinTech - NuPay",,"ATM transactions for both clients are showing successful, but cash is not dispensed.",Third-Party - Client,,,,,,,,,,,,2025-01-25T08:00:36.055Z,2025-01-25T08:00:36.055Z,,,,,2025-02-05T03:57:19.441Z,,2025-01-25T09:32:36.952Z,,,,2025-02-05T03:57:19.441Z,935803,2026-02-02T10:36:10.312Z,,public,139.8,118.8,21.1,0,25jan-p1-44-partial-outage-of-atm-transaction-processing-for-mukuru-nupay,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08A2HR3AF8,Reported by Vicnesh Baskar,false
45,Partial Outage of transaction processing for D360,Service Incident,Root Cause Analysis and Corrective Actions,P2,"**Problem:** Transactions are intermittently timing out due to FAST Active delay. Rule Engine and Tokenisation delays were also observed for some transactions.

**Impact:** Approximately 21% of transactions experienced timeouts, affecting transaction processing.

**Causes:** To Be Decided

**Steps to resolve:** Restarting MADA & VISA PayPower resolved tokenisation and rule engine delays, and reduced FAST ACTIVE delays. Transactions have been processing normally without further timeouts for the past 60 minutes.",Vicnesh Baskar,Vicnesh Baskar,,,,Banking.Live,D360,Restarting MADA & VISA PayPower,Approximately 20% of transactions in the past 1 hour has been intermittently timing out.,Paymentology,,,,,,,,,,,,2025-01-26T04:35:18.223Z,2025-01-26T04:35:18.223Z,,,,,2025-01-26T06:34:37.024Z,2025-01-25T19:07:00.000Z,2025-01-26T05:33:37.860Z,,2025-01-27T05:37:18.311Z,,,7158,2026-02-02T10:38:32.205Z,,public,707.8,623.1,54.7,30,26jan-p2-45-partial-outage-of-transaction-processing-for-d360,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08AALKUSBD,Reported by Vicnesh Baskar,false
46,MADA Transactions Processing Outage,Service Incident,Closed,P1,All MADA transactions failing for D360 (Visa transactions unaffected) ,Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,D360,,,Inconclusive,,,,,,,,,,,,2025-01-27T11:55:08.075Z,2025-01-27T11:55:08.075Z,,,,,2025-02-12T08:18:50.381Z,2025-01-27T09:32:00.000Z,2025-02-12T08:18:50.381Z,,,,2025-02-12T08:18:50.381Z,1369422,2026-02-02T10:39:12.583Z,,public,1540.5,1208.8,232.4,99.3,27jan-p1-46-mada-transactions-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08ABHXMCNR,Reported by Snothile Dlamini,false
47,Partial Processing Issue,Service Incident,Post-Incident Tasks & Documentation,P1,"**Problem:** Partial processing issues across several VE nodes started at 18:51 UTC.

**Impact:** Customers experienced a significant increase in 9XXX errors, with a success rate dropping to 39.97%.

**Causes:** The issue was caused by AWS-RED-01 and AWS-RED-02 server memory being overwhelmed with large items from different processes.

**Steps to resolve:** The initial investigation led to DB locks, after additional troubleshooting we identified that traffic to AWS-RED-01 and AWS-RED-02 was stopped and the server's memory was manually cleared before restarting them. The traffic was then resumed and it started to show a healthy state across all servers.",Daniel Velado,Daniel Velado,,,,VoucherEngine,,,,Paymentology,,,,,,,,,,,,2025-01-27T19:12:43.899Z,2025-01-27T19:12:43.899Z,,,,,2025-01-27T23:06:16.836Z,2025-01-28T01:00:00.000Z,2025-01-27T22:12:28.716Z,,,,,14012,2025-01-27T23:06:17.692Z,,public,326.9,256.9,50,20,27jan-p1-critical-47-partial-processing-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08AH3JKR42,Reported by Daniel Velado,false
48,Partial Outage of Transaction processing,Service Incident,Closed,P1,"On 29 Jan 22:28 UTC, Paymentology’s support team observed a high number of transaction timeouts affecting ISB, Mettle, and ADQ Mastercard via monitoring alerts. Initial investigations indicated that transactions were delayed before being received by PayRoute, potentially due to an issue between Mastercard/Visa and PayRoute.

Further analysis confirmed no connectivity issues between Azure and the UK data centre. Resource utilisation was stable across PayRoute servers, except for a CPU spike on PR-01 (ISB PayRoute server) during the incident window.

The issue self-recovered by 22:32 UTC, with transaction processing returning to normal. The incident was declared resolved on 29 Jan 22:35 UTC.",Daniel Velado,Vicnesh Baskar,,,,,"Wio, MOX, Mettle, Islandsbanki",The issue self-recovered,,Paymentology,,,,,,,,,,,,2025-01-30T06:11:56.416Z,2025-01-29T22:36:00.000Z,,,,,2025-01-29T22:32:00.000Z,2025-01-29T22:28:00.000Z,,2025-01-29T22:28:00.000Z,,,2025-01-30T06:11:56.416Z,,2025-01-30T06:51:45.325Z,,public,549.7,475.4,40,34.3,30jan-p1-critical-48-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08AHPQKQT1,Reported by Vicnesh Baskar,false
49,Transaction Processing Issue,Service Incident,Post-Incident Tasks & Documentation,P1,"**Problem:** A connection timeout error was encountered with PayRoute and prod-switch.paymentology.org due to the VPN to AFS being down, impacting transaction processing.

**Impact:** This has resulted in a full disruption of transaction processing.

**Causes:** The issue was caused by a failure of the VPN connection to AFS. Additional details will be provided soon.

**Steps to resolve: A call with** AFS took place to resolve their connection issues, and we restarted our application to reconnect. Communication with clients has been maintained, and transactions have started to be processed again.",Daniel Velado,Snothile Dlamini,,,,Banking.Live,Dopay LLC,,,Third-Party - Client,https://paymentology.zoom.us/j/81303225236?pwd=4ZxUdL8c24pMcB0on1MNMp8NEEHBEt.1,,,,,,,,,,,2025-01-30T15:19:06.777Z,2025-01-30T15:19:06.777Z,,,,,2025-01-30T16:17:58.088Z,2025-01-30T12:40:00.000Z,2025-01-30T16:17:58.088Z,,,,,3531,2026-02-02T10:59:38.168Z,,public,242.2,222.2,10,10,30jan-p1-49-transaction-processing-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08AU6KBU0N,Reported by Snothile Dlamini,false
50,PIN Format Errors,Service Incident,Closed,P1,"An alert was triggered indicating an increase in error codes (9120) on the **Crimson system** for the **prod_altc_4b** node, specifically affecting transactions with BIN **533822** (Standard Bank PayCard). The errors were identified as **PIN format errors** (error code **9120**), which occurred due to **out-of-sync keys** between two Altec endpoints.

The root cause was traced to Altec bringing both endpoints (**prod_altc_4b** and **altc_3A**) online simultaneously without properly synchronizing their keys. This caused the **HSM** to fail PIN validation for transactions, as the keys used for encryption and decryption were mismatched. The issue was intermittent, with the alert auto-resolving at times but reoccurring as new transactions triggered the same errors.",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,SBSA PayCard,,,Third-Party - Paymentology,,,,,,,,,,,,2025-02-03T12:50:42.394Z,2025-02-03T12:50:42.394Z,,,,,2025-02-03T13:35:14.735Z,2025-02-03T11:03:00.000Z,2025-02-03T12:50:57.570Z,,,,2025-02-03T13:35:14.735Z,2672,2025-02-03T13:35:14.861Z,,public,154.8,154.8,0,0,03feb-p1-critical-50-pin-format-errors,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08BK52KEAY,Reported by Snothile Dlamini,false
52,Partial Outage of Transaction Processing for multiple clients,Service Incident,Closed,P1,"Multiple transactions are processed in StandIn mode due to a high latency on ve-prod-crimson. Databased and application teams mobilized, investigation in progress.",Vicnesh Baskar,Vicnesh Baskar,,,,VoucherEngine,"Agoda, Albo, GoTyme, DolarApp, Fondeadora/Mibo, Ecocash, Vodacom DRC, Vodafone Fiji, Safaricom, Nelo, Paycard Guinea, Octopus, Mascom Botswana, KiplePay, Inswitch, Quicko, Mukuru, SBSA Ucount, Altron FinTech - NuPay",Database team identified and optimized a query which reduced the high latency,Multiple transactions were impacted for multiple clients. Additional details are being gathered.,Paymentology,https://paymentology.zoom.us/j/84356262159?pwd=7yulrAXU28DTKOUokVKDPvKda8uLs4.1,,,,,,,,,,,2025-02-06T04:53:56.877Z,2025-02-06T04:53:56.877Z,,,,,2025-02-06T06:26:54.622Z,,2025-02-06T05:07:05.725Z,,,,2025-02-06T06:26:54.622Z,5577,2025-02-07T08:30:36.057Z,,public,408.7,367.8,0,40.9,06feb-p1-critical-52-partial-outage-of-transaction-processing-for-multiple,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08C0PT8V6E,Reported by Vicnesh Baskar,false
53,Full Outage of Trasanction Processing,Service Incident,Closed,P1,"Transaction processing experienced a full outage due to a connectivity problem with the ISP for the London DC, starting at 23:56 UTC on 11 Feb. The connectivity issue was resolved, and normal transaction processing resumed at 00:09 UTC on 12 Feb. ",Vicnesh Baskar,Vicnesh Baskar,,,,Banking.Live,"Mettle, Coverflex, Rain",The issue self-recovered,148 transactions timed-out due to FAST ACTIVE Request and FAST ACTIVE Response failures,Third-Party - Paymentology,,,,,,,,,,,,2025-02-12T00:47:12.486Z,2025-02-12T00:06:00.000Z,,,,,2025-02-12T01:31:19.828Z,2025-02-11T15:56:00.000Z,2025-02-11T23:56:00.000Z,2025-03-12T00:09:00.000Z,,2025-02-12T08:05:42.723Z,2025-02-12T08:05:42.723Z,5119,2025-02-12T08:05:42.826Z,,public,199.1,175,4.1,20,12feb-p1-critical-53-full-outage-of-trasanction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08CY9XU59S,Reported by Vicnesh Baskar,true
54,Transaction Processing Issue,Service Incident,Post-Incident Tasks & Documentation,P1,An alert was received from ACS regarding an influx of declines with error code 91 for BIN: 526722. The issue is linked to a disconnected port number (50045). The last successful transactions were recorded at 09:35. Altech restarted the nodes PROD_ALTC_4B and PROD_ALTC_3A from their end which resolved the issue. More details will be provided,Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,,Altech restarted the nodes PROD_ALTC_4B and PROD_ALTC_3A from their end which resolved the issue. ,,Third-Party - Paymentology,https://paymentology.zoom.us/j/82398210110?pwd=r9MNyW7b1Ny5RnKnuX7BkTJukIQmbN.1,,,,,,,,,,,2025-02-12T08:29:30.037Z,2025-02-12T08:29:30.037Z,,,,,2025-02-12T08:48:41.731Z,2025-02-12T08:25:00.000Z,2025-02-12T08:48:41.731Z,,,,,1151,2025-02-12T08:48:42.562Z,,public,243.6,236.4,7.2,0,12feb-p1-critical-54-transaction-processing-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08CQN56DJS,Reported by Snothile Dlamini,false
55,Partial Outage of MADA Transactions for D360 ,Service Incident,Post-Incident Tasks & Documentation,P2,Reversals numbers are back to normal. No action taken by Paymentology. Pending for further updates from Saudi Payments on the root cause and what remediation actions were taken. AS team will continue to monitor the reversal count.,Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,"D360, Tweeq",Pending update from Saudi Payments,High number of reversals since 10:05 UTC 14th Feb. Total 1224 reversals from 10:05 UTC till 11:40 UTC. ,Third-Party - Client,https://paymentology.zoom.us/j/8536652946,,,,,,,,,,,2025-02-14T11:43:43.959Z,2025-02-14T11:43:43.959Z,,,,,2025-02-14T13:26:53.750Z,2025-02-14T02:05:00.000Z,2025-02-14T13:26:53.750Z,,,,,6189,2025-02-14T13:26:54.563Z,,public,227.9,134.6,93.3,0,14feb-p2-high-55-partial-outage-of-mada-transactions-for-d360,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08D92GH4RK,Reported by Kaisar Mahmood,false
56,Transaction Processing Issue for WIO ,Service Incident,Post-Incident Tasks & Documentation,P1,"As of 13:08 UTC 17th Feb, we are processing as expected with no issues. 5 instances of PayPower (1 UAE, 2 VISA, 2 Mastercard) were restarted, the restarts were completed at 13:08 UTC. Root cause is unknown at this stage, investigation and monitoring is ongoing.",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,Wio,PayPower instances were restarted.,Transaction Processing issue ,Paymentology,https://paymentology.zoom.us/j/8536652946,,,,,,,,,,,2025-02-17T12:51:54.950Z,2025-02-17T12:51:54.950Z,,,,,2025-02-17T15:39:57.835Z,2025-02-17T04:13:00.000Z,2025-02-17T13:30:53.319Z,,,,,10082,2025-02-17T15:39:58.627Z,,public,595.4,487,99,9.5,17feb-p1-critical-56-transaction-processing-issue-for-wio,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08DQMVHYCC,Reported by Kaisar Mahmood,false
57,INTERNAL - Postgres Zero Day exploit. ,Service Incident,Closed,P4,"This is NOT a  zero day exploit 

A patch has been identified for the postgres environment and is in test, prior to being rolled out across the estate via normal CCM process1st week of March. ",Kaisar Mahmood,Stuart Nelmes,,,,,,Patch of software was provided,,Third-Party - Client,https://paymentology.zoom.us/my/bigstu,,,,,,,,,,,2025-02-17T13:59:14.336Z,2025-02-17T13:59:14.336Z,,,,,2025-02-19T12:48:22.676Z,2025-02-17T13:57:00.000Z,2025-02-17T14:10:36.574Z,,,,2025-02-19T12:48:22.676Z,80346,2025-02-19T12:48:22.829Z,,public,264.5,151.1,78.6,34.8,17feb-p4-low-57-internal-postgres-zero-day-exploit,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08DR1UHAQ4,Reported by Stuart Nelmes,false
58,Unable to connect to VoucherExpress,Service Incident,Post-Incident Tasks & Documentation,P1,"**Problem:** Clients are unable to connect to VoucherExpress due to the servers not accepting connections.

**Impact:** Clients cannot perform calls for checking balances and uploading sales.

**Causes: W**eb servers were confirmed to be down by Infra team.

**Steps to resolve:** Our Network and Infra teams have manually brought the servers back up.
The servers should have been rebooted automatically, our team is checking if the automatic reboots were triggered and other possible root causes.",Daniel Velado,Daniel Velado,,,,VoucherEngine,,,,Paymentology,https://paymentology.zoom.us/j/9841627498,,,,,,,,,,,2025-02-17T16:23:38.410Z,2025-02-17T16:23:38.410Z,,,,,2025-02-17T16:52:45.600Z,,2025-02-17T16:52:45.600Z,,,,,1747,2026-02-02T11:02:26.963Z,,public,51.9,51.9,0,0,17feb-p1-58-unable-to-connect-to-voucherexpress,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08DP658FFD,Reported by Daniel Velado,false
59,Full Outage of transaction processing,Service Incident,Post-Incident Tasks & Documentation,P1,"Transaction flow normalised at 13:08 UTC after the connectivity was restored with Altech via node ALTC_4B. Additional customers were also impacted based on reversals and advisements flowing in after reconnection. Altech will be contacted for further investigation, and the final impact assessment is in progress.",Vicnesh Baskar,Vicnesh Baskar,,,,VoucherEngine,Altron FinTech - NuPay,TBC,"Transactions are not processing as both PROD_ALTC_3A, PROD_ALTC_4B are Down",Third-Party - Paymentology,https://paymentology.zoom.us/j/86522908569?pwd=p1qbFAd6bMwpP0dpbvnqtExqhqRwbx.1,,,,,,,,,,,2025-02-18T00:26:54.814Z,2025-02-18T00:26:54.814Z,,,,,2025-02-18T06:45:22.203Z,2025-02-18T00:06:00.000Z,2025-02-18T01:36:10.501Z,,,,,22707,2025-02-18T06:45:22.672Z,,public,616.9,498,20,98.9,18feb-p1-critical-59-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08DR0GRVKL,Reported by Vicnesh Baskar,false
60,Transaction Processing Issue,Service Incident,Closed,P1,"Transactions dropped for NOMO, Coverflex, Constantinople. Mettle and ISB
Issue started: 19-Feb-2025 17:26:57.813
Finished: 19-Feb-2025 17:34:48.835
",Daniel Velado,Daniel Velado,,,,Banking.Live,,,,Third-Party - Paymentology,,,,,,,,,,,,2025-02-19T18:00:30.512Z,,,,,,2025-02-19T23:34:00.000Z,2025-02-19T23:26:00.000Z,,,,,,,2026-02-02T11:03:56.520Z,,public,648,522,116,10,19feb-p1-60-transaction-processing-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08ERF8V988,Reported by Daniel Velado,false
61,Full Outage on API Processing,Service Incident,Closed,P1,"API processing stopped for ADQ due to a high CPU utilization.
Impact:
API 01
Stopped at 2025-02-19 21:47:13.815
Started at 2025-02-19 22:05:46.830

API 02
Stopped at 2025-02-19 21:45:24.733
Started at 2025-02-19 22:06:07.343",Daniel Velado,Daniel Velado,,,,Banking.Live,Wio,,,Paymentology,,,,,,,,,,,,2025-02-19T23:34:08.313Z,,,,,,2025-02-20T04:06:00.000Z,2025-02-20T03:45:00.000Z,,,,,,,2026-02-02T11:05:39.445Z,,public,171.2,140.5,10.7,20,19feb-p1-61-full-outage-on-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08ET850XNU,Reported by Daniel Velado,false
63,Full Outage of transaction Processing,Service Incident,Closed,P1,"On February 24, at 05:03 UTC, Paymentology’s support team was alerted to a disconnection of the ALTC nodes (PROD_ALTC_3A and PROD_ALTC_4B), resulting in a temporary halt in transaction processing. The issue was identified as a network-level disconnection with a local switching provider.

The connection was automatically restored at 05:08 UTC, and normal transaction processing resumed with no further issues reported. Switching provider contacted for a detailed root cause analysis.

Further updates, including preventive actions, will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,,VoucherEngine,"Wow Marketing, SBSA PayCard, Canal Walk (Hyprop), Tygervalley (Pareto)",,,Third-Party - Paymentology,,,,,,,,,,,,2025-02-24T06:34:31.750Z,2025-02-24T05:03:00.000Z,,,,,2025-02-24T05:08:00.000Z,2025-02-24T05:03:00.000Z,,,,,2025-02-24T06:34:31.750Z,300,2026-02-02T11:28:10.989Z,,public,53.6,53.6,0,0,24feb-p1-63-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08ELF9DAA1,Reported by Vicnesh Baskar,false
64,Partial Transaction Processing Issue,Service Incident,Closed,P1,"Around 18:55 UTC, we received reports for a drop on Visa transactions for D360, issue seems to be related to Rule Engine taking too much time.
Impact has ceased and we are investigating the root cause.",Daniel Velado,Daniel Velado,,,,Banking.Live,,,,Inconclusive,,,,,,,,,,,,2025-02-26T19:38:45.419Z,2025-02-26T19:38:45.419Z,,,,,,,,,,,2025-02-26T19:38:45.419Z,,2026-02-02T11:28:26.036Z,,public,263.3,242.5,10.8,10,26feb-p1-64-partial-transaction-processing-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08F86SR5M0,Reported by Daniel Velado,false
65,Partial Outage - High DB utilisation causing intermittent transaction timeouts,Service Incident,Closed,P1,"High DB usage on MOX Master DB, resulting in high process time for MOX PayPower, in turn causing intermittent transaction timeouts.",Kaisar Mahmood,Vicnesh Baskar,,,,Banking.Live,MOX,,Intermittent transaction timeouts,Paymentology,https://paymentology.zoom.us/j/87836072251?pwd=dMWVMf0PZEap3e9eKaNQKRMabuqTnU.1,,,,,,,,,,,2025-02-27T05:32:58.490Z,2025-02-27T05:32:58.490Z,,,,,2025-04-17T05:48:33.911Z,,2025-02-27T07:15:02.505Z,,2025-04-17T05:49:49.118Z,2025-04-17T05:49:58.212Z,2025-04-17T05:49:58.212Z,4234535,2026-02-02T11:29:09.068Z,,public,10940.3,9370.4,1119.2,450.7,27feb-p1-65-partial-outage-high-db-utilisation-causing-intermittent-transac,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08EUEP93PH,Reported by Vicnesh Baskar,false
68,ATM transaction processing Issue - approved / cash not dispensed ,Service Incident,Closed,P1,Mukuru has reported cases where transaction was approved but the cash was not dispensed. Exact impact is being determined. We are proactively raising as a P1. ,Kaisar Mahmood,Kaisar Mahmood,,,,VoucherEngine,Mukuru,,Mukuru has reported cases where transaction were approved but the cash was not dispensed. Exact impact is being determined. We are proactively raising as a P1. ,Third-Party - Client,https://paymentology.zoom.us/j/8536652946,,,,,,,,,,,2025-03-01T14:18:47.788Z,2025-03-01T14:18:47.788Z,,,,,2025-04-17T05:48:15.808Z,2025-02-27T20:00:00.000Z,2025-04-17T05:48:15.808Z,,2025-04-17T05:51:14.896Z,2025-04-17T05:51:23.505Z,2025-04-17T05:51:23.505Z,4030168,2026-02-02T14:18:15.470Z,,public,2628.8,2114.5,258.4,255.8,01mar-p1-68-atm-transaction-processing-issue-approved-cash-not-dispensed,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08FM2WULAZ,Reported by Kaisar Mahmood,false
69,Partial Outage of Transaction processing due to FAST Active Response: FAILED,Service Incident,Closed,P4,There is a partial outage of transaction processing due to FAST Active Response: FAILED for some transactions. No issues have been observed thus far from application or infra logs.  The client confirmed an issue on their end and has since fixed it. The last decline was observed at 01:32 UTC with no further issues during monitoring.,Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,MOX,Client performed a fix on their infra,Transactions timeouts due to no response from client,Client,https://paymentology.zoom.us/j/89246185411?pwd=OBDjtPJ3UoHX9cVY9PDz3TEA3appea.1,,,,,,,,,,,2025-03-05T01:27:42.492Z,2025-03-05T01:14:00.000Z,,,,,2025-03-05T01:32:00.000Z,2025-03-05T01:04:00.000Z,2025-03-05T01:14:00.000Z,,,,2025-03-05T05:15:29.152Z,1080,2025-03-05T05:15:29.234Z,,public,117.5,107.5,0,10,05mar-p4-69-partial-outage-of-transaction-processing-due-to-fast-active-res,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08G3H90JJH,Reported by Vicnesh Baskar,false
74,Transaction Processing Issue,Service Incident,Post-Incident Tasks & Documentation,P1,"**Problem:** MC transactions dropped after 18:57 UTC due to a possible disconnection from MC side, affecting multiple clients.

**Impact:** Clients experienced no MC transactions processing,

**Causes:** The current root cause is still under investigation, we have created a ticket with MC to check on the disconnection.

**Steps to resolve:** The AS team restarted the affected client's PayRoute and PayPower applications one by one to re-establish the MC connections.",Daniel Velado,Daniel Velado,,,,Banking.Live,,,,Inconclusive,https://paymentology.zoom.us/j/9841627498,,,,,,,,,,https://app.incident.io/paymentology/incidents/01JNPEA4YQFWB8DG6S288REAS3?tab=postmortem_document,2025-03-06T19:18:27.031Z,2025-03-06T19:18:27.031Z,,,,,2025-03-06T20:50:48.898Z,,2025-03-06T20:50:48.898Z,,,,,5541,2026-02-02T14:21:17.676Z,,public,1774.3,1317.9,118.1,338.3,06mar-p1-74-transaction-processing-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08GBGQU8MC,Reported by Daniel Velado,false
75,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 6th March at 22:29 UTC, the support team observed disconnections on one VISA PR, intermittently affecting transaction processing for some clients. Log analysis indicates that the disconnection started at 19:23 UTC. Further investigation showed one of the VISA PR is down, which was then restarted. Transaction processing normalised, with the last disconnection at 22:57 UTC. Detailed impact correlation and root cause analysis currently underway.",Daniel Velado,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,"Wio, Bank Al Etihad, Rain, Zing",Restarted VISA PR,,Paymentology,,,,,,,,,,,,2025-03-07T03:03:19.329Z,2025-03-07T03:03:19.329Z,,,,,2025-03-06T22:56:00.000Z,2025-03-06T19:23:00.000Z,2025-03-06T22:29:00.000Z,2025-03-06T22:55:00.000Z,,,,,2026-02-02T14:30:48.901Z,,public,230,185.3,0,44.7,07mar-p1-75-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08G5R6FG0P,Reported by Vicnesh Baskar,false
77,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"At approximately 0710 UTC, the client reported that they observed a dip in authorisation health from their side. Further investigation revealed 45 transactions were either processed in standin or timed-out, starting from 0623 UTC. Transactions are processing as normal as of  0644 UTC and no other issues were observed. Investigation is ongoing on possible root cause.",Vicnesh Baskar,Vicnesh Baskar,,,Wafa Ben Nacef,VoucherEngine,Agoda,Transaction processing self normalized,35 transactions were processed in standin and 8 transactions timed-out,Paymentology,,Transaction Processing,Issue is suspected due to a temporary slowness in Voucher Engine,No,Red - No monitoring alerts were triggered to indicate the issue,Red - Inicident was reported by client and transaction processing was back to normal,Red - Current investigation did not point to any issues and no resolution as transaction processing self normalized ,Green - Client reported the issue,,,,2025-03-07T09:04:41.260Z,2025-03-07T07:10:00.000Z,,,,,2025-03-07T06:44:00.000Z,2025-03-07T06:23:00.000Z,2025-03-07T07:10:00.000Z,2025-03-07T06:44:00.000Z,,,,7499,2026-02-02T14:26:33.935Z,,public,88.6,88.6,0,0,07mar-p3-critical-77-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08HB939SFJ,Reported by Vicnesh Baskar,false
78,Partial Outage on Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P2,"We noticed a partial transaction processing issue which stopped at
23:48 UTC.
Currently investigating root cause and impact",Daniel Velado,Daniel Velado,,,,VoucherEngine,,,,Paymentology,https://paymentology.zoom.us/j/9841627498,,,,,,,,,,,2025-03-07T23:53:31.604Z,2025-03-07T23:53:31.604Z,,,,,2025-03-07T23:54:45.354Z,,2025-03-07T23:54:45.354Z,,,,,73,2026-02-02T14:30:03.528Z,,public,272.7,228,10,34.7,07mar-p2-78-partial-outage-on-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08GVTYL8J0,Reported by Daniel Velado,false
79,Mastercard Connection Issue on all MC Clients,Service Incident,Closed,P1,"**Problem:** A critical P1 incident involving Mastercard connection issues impacting all Mastercard clients started on March 9, 2025, at 7:07 AM UTC, with a second occurrence starting at 7:197 AM UTC and is ongoing.

**Impact:** The issue impacts transactions across multiple clients, including:
- MOX
- TWEEQ
- Yondr
- Manigo
- ISB
- BOJ
- UNION54
- Mettle
- Nomo
- ADQ
- Xendit
- Constantinople
- OM

**Causes:** The problem was caused by an internal global networking issue at Mastercard affecting all cloud MIPs.

**Steps to resolve:** Mastercard is working to resolve the internal issue. In response, the Service Delivery team is checking configurations to possibly transfer clients to the UK MIPs. Configuration changes are being implemented to shift connections to physical MIPs for some clients.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,MasterCard,,,Third-Party - Paymentology,https://paymentology.zoom.us/j/85951841523?pwd=0b29KK8LMyVR67lLbmSqFFaWnRNdvp.1,,,,,,,,,,,2025-03-09T07:56:51.854Z,2025-03-09T07:56:51.854Z,,,,,2025-03-09T14:13:04.536Z,2025-03-09T07:07:00.000Z,2025-03-09T08:49:17.924Z,2025-03-09T10:40:37.612Z,2025-03-13T05:46:24.250Z,2025-03-13T05:46:33.325Z,2025-03-15T11:33:33.190Z,22572,2025-03-15T11:33:33.297Z,,public,1375.9,1195,30,150.9,09mar-p1-79-mastercard-connection-issue-on-all-mc-clients,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08GWSYTF9R,Reported by Snothile Dlamini,false
80,Partial Outage of Transaction Processing for Tweeq,Service Incident,Root Cause Analysis and Corrective Actions,P1,"At 01:08 AM UTC on March 10, 2025, the Paymentology support team received an alert from Mastercard indicating transactions were being processed in standin since 01:08 AM UTC.

Both PayPower and PayRoute instances for the client were restarted twice, and transactions started processing normally since 0340 UTC.

Root cause analysis is underway to determine the cause as no disconnects were observed during investigation.",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,Tweeq,Restart of Paypower and PayRoute instances,Transactions were processed in standin by MC,Paymentology,,,,,,,,,,,https://app.incident.io/paymentology/incidents/01JNZ96TWEQVNM6T3790X1HCY0?tab=postmortem_document,2025-03-10T05:42:25.422Z,2025-03-10T05:42:25.422Z,,,,,2025-03-10T05:59:50.081Z,2025-03-10T01:08:00.000Z,2025-03-10T05:59:50.081Z,,2025-03-12T07:16:56.707Z,,,1044,2025-03-19T08:11:48.132Z,,public,148.1,128.1,10,10,10mar-p1-80-partial-outage-of-transaction-processing-for-tweeq,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08H8FSHUBT,Reported by Vicnesh Baskar,false
81,Full Outage of Transaction Processing for UNION54,Service Incident,Root Cause Analysis and Corrective Actions,P1,"At 00:08 UTC on March 10, 2025, the Paymentology support team received an alert from Mastercard indicating transactions were being processed in standin since 23:39 UTC. Both PayPower and PayRoute instances for the client were restarted, and transactions started processing normally at 00:39 UTC. No disconnects were observed from the logs investigation, root cause analysis is underway.",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,Union54 / Zazu Zambia,Restart of PayPower and PayRoute,Transactions were processed in standin by MC,Third-Party - Client,,,"At 00:38, 10 March, Paymentology support team restarted both the PayPower and PayRoute instances to reset the connectivity. Transaction processing normalised at 00:39, 10 March.  The issue is suspected due to a disconnect with Mastercard, which is being investigated for further evidence alongside log reviews from other application and infra components.   A detailed root cause analysis and further updates will be provided in the RCA Report (RCAR).",Yes - 930121 - Partial Outage on MC Transaction Processing,Red - Incident was detected and reported by MC,Red - Incident was detected and reported by MC. IM was not involved till later,Green - The team was quick to find disconnection and to perform a restart of PayPower and PayRoute instances for recovery,Green - Client was informed on time,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8590000165/INC-81+Full+Outage+of+Transaction+Processing+for+UNION54,2025-03-10T06:12:32.766Z,2025-03-10T06:12:32.766Z,,,,,2025-03-10T06:24:54.412Z,2025-03-09T23:39:00.000Z,2025-03-10T06:24:54.412Z,,2025-03-12T07:17:46.922Z,,,741,2026-02-02T14:34:50.510Z,,public,182.3,172.3,10,0,10mar-p1-81-full-outage-of-transaction-processing-for-union54,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08GH5XBB2T,Reported by Vicnesh Baskar,false
84,Partial Outage of Transacation Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"Support team was notified of errors on PROD_ALTC_4B, impacting some transactions wityh error 9120. Investigation ongoing.",Vicnesh Baskar,Vicnesh Baskar,,,Ardiansyah Ar,VoucherEngine,SBSA PayCard,Restart of PROD_ALTC_4B endpoint,Some transactions were getting declined with error 9120,TBD,https://paymentology.zoom.us/j/83701903432?pwd=4n3yYLtgaT749JDNE00iVLd17LyKmH.1,Transaction Processing,,Yes - 913684,Red - Endpoint switched at 23:38 and PIN validation error started,,Amber - The team were quick to identify the issue and similar occurence,,,,https://app.incident.io/paymentology/incidents/01JP6XJRPPMPZSR8F59019KJHW?tab=postmortem_document,2025-03-13T04:53:08.950Z,2025-03-13T04:38:00.000Z,,,,,2025-03-13T05:12:00.000Z,2025-03-12T23:38:00.000Z,2025-03-13T04:35:00.000Z,2025-03-13T05:12:00.000Z,2025-03-18T06:07:17.380Z,,,2040,2025-03-18T06:07:17.742Z,,public,414.1,384.1,10,20,13mar-p1-84-partial-outage-of-transacation-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08HK9D85FV,Reported by Vicnesh Baskar,false
85,Lost Connection on Cortex,Service Incident,Closed,P4,"**Problem:** There was a loss of connection to CORTEX on the host pt-boj-proute01.paymentology.org within the CORTEX-prod-uk-dc-PayRoute system, preventing any CORTEX transactions from being received.

**Impact:** No CORTEX transactions were received until the issue was resolved.

**Steps to resolve:** Connection to CORTEX was restarted on the client side.",Daniel Velado,Snothile Dlamini,,,Ramon Gouveia,Banking.Live,Bank of Jordan,,,,https://paymentology.zoom.us/j/82763763804?pwd=fuzQz9DTHwejoFoH5rJzITqoiknOMz.1,,,,,,,,,,,2025-03-13T15:28:55.947Z,2025-03-13T15:28:55.947Z,,,,,2025-03-13T15:45:53.587Z,2025-03-13T15:02:00.000Z,2025-03-13T15:45:53.587Z,,2025-03-13T15:58:41.493Z,,2025-03-13T15:58:41.493Z,1017,2025-03-13T15:58:41.600Z,,public,132.6,103,29.6,0,13mar-p4-85-lost-connection-on-cortex,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08HX5MUA3B,Reported by Snothile Dlamini,true
86,Partial Outage on MADA Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"Client D360 reached out due to MADA disconnection that affected transaction processing.
Our Advanced Support team was able to confirm the disconnections on the 4 MADA instances.
Disconnection Timings in UTC: 

**No manual intervention done (2 MADA Trxns Impacted):**

```
MADA Instance 1 - 21:57 - 22:01
MADA Instance 2 - 21:57 - 22:01
```

**Manually restarted the MADA PRs (18 MADA Trxns Impacted):**

```
MADA Instance 3 - 22:03 - 22:59
MADA Instance 4 - 22:03 - 22:55
```

**No manual intervention done (1 MADA Trxns Impacted):**

```
MADA Instance 1 - 23:03 - 23:06
MADA Instance 2 - 23:03 - 23:06
```

A Jira ticket will be logged for our team to investigate the disconnection's root cause.",Daniel Velado,Daniel Velado,,,,Banking.Live,D360,,,Third-Party - Client,,,,,,,,,,,,2025-03-13T23:17:17.379Z,2025-03-13T23:17:17.379Z,,,,,2025-03-13T23:19:34.321Z,,2025-03-13T23:19:34.321Z,,,,,136,2026-02-02T14:35:50.435Z,,public,120.4,79.7,0,40.7,13mar-p2-86-partial-outage-on-mada-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08HG18K5P0,Reported by Daniel Velado,false
87,Full Outage on Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On March 14th at 17:56 UTC, Paymentology Support team received a notification from client Old Mutual that transactions were failing with Do Not Honour.

Advanced Support team confirmed the echo tests to MC we succesfull and no disconnection was observed on our side.Support team requested additional information regarding the attempts in order to escalate the issue to MC; client provided the last two attempts and requested us to refresh the MIP connection.

Advanced Support team proceeded to restart instance by instance (4 in total), transansactions started to arrive after the first instance was restarted.

Additional investigation will be conducted between MC and Paymentology to determine the root cause.",Daniel Velado,Daniel Velado,,,,Banking.Live,Old Mutual Bank / Olympus,,,Paymentology,,,,,,,,,,,,2025-03-14T22:06:32.114Z,2025-03-14T22:06:32.114Z,,,,,,,,,,,,,2026-02-02T14:36:20.317Z,,public,60.6,50.6,10,0,14mar-p1-87-full-outage-on-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08HW23S9C3,Reported by Daniel Velado,false
88,Partial Outage on Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"On March 14th at 22:16 UTC, our Support team received a notification from our client regarding a rule incorrectly declining transactions.
Initial investigation determined that the rule itself was not triggering the decline, it was rather Rule Engine having an error to parse the information due to faulty historical DE22.
Database team was engage to amed the historical data for the affected client.

Additonal investigaton will be conducted to determine the root cause.",Daniel Velado,Daniel Velado,,,,,,,,Paymentology,,,,,,,,,,,,2025-03-14T22:36:22.605Z,2025-03-14T22:36:22.605Z,,,,,,,,,,,,,2026-02-02T14:36:57.069Z,,public,70.6,39.5,20.3,10.8,14mar-p2-88-partial-outage-on-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08HT5E9ATF,Reported by Daniel Velado,false
89,Partial Outage on Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On March 15th at 01:20 UTC, our Advanced Support team received alerts regarding an increase in advices and connection errors on db for Mox. Upon further investigation, we were able to confirm a connection error on the db which resolved itself without Paymentology's intervention.
An internal ticket has been created for our team to investigate the disconnection root cause.

Started: 2025-03-15 01:12:57 UTC
Ended: 2025-03-15 01:14:01 UTC
Impacted transactions (advises): 99 transactions
Successful transaction during incident: 178 transactions",Daniel Velado,Daniel Velado,,,,Banking.Live,,,,Third-Party - Paymentology,,,,,,,,,,,,2025-03-15T01:55:15.745Z,2025-03-15T01:55:15.745Z,,,,,,,,,,,,,2026-02-02T14:37:50.411Z,,public,132.2,78,34.2,20,15mar-p1-89-partial-outage-on-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08HWGYRHPD,Reported by Daniel Velado,false
91,Intermittent API Processing Issue,Service Incident,Post-Incident Tasks & Documentation,P1,"The issue involves intermittent API processing failures on payAPI-02, which existed even on the previous host machine (pt-adq-payapi02.paymentology.org) and version. However, the service would auto-restart on the previous host, minimizing client impact. After migrating to the new host machine, the service is no longer auto-restarting and is getting stuck, leading to client API call failures. A JIRA has been raised to the API team to investigate the root cause.",Snothile Dlamini,Snothile Dlamini,,,Kamal Thapa,Banking.Live,Wio,,API,Paymentology,https://paymentology.zoom.us/j/87659146527?pwd=DeFW6i0WubzmsvRCc5eMq5AgKxWMBs.1,,,,,,,,,,,2025-03-19T09:01:02.467Z,2025-03-19T09:01:02.467Z,,,,,2025-03-19T10:41:05.829Z,2025-03-19T08:32:00.000Z,2025-03-19T10:41:05.829Z,,,,,6003,2025-03-19T12:48:48.704Z,,public,169.7,169.7,0,0,19mar-p1-91-intermittent-api-processing-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08JP4N1W1F,Reported by Snothile Dlamini,false
92,Partial Outage of Transacation Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On March 20, 2025, a partial outage of Mastercard transaction processing was observed due to connection timeouts. The incident began at 22:04 UTC and lasted for approximately 1 hour and 21 minutes, ending at 23:25 UTC.
The issue was resolved by restarting the PR services, and transactions were successfully processed after the restart. The root cause is suspected as socket disconnection on MC, further analysis in progress.",Vicnesh Baskar,Vicnesh Baskar,,,Ardiansyah Ar,Banking.Live,Tweeq,Restarting the PR services,Some transactions were processed in StandIn due to disconnection,Inconclusive,,,,,,,,,,,,2025-03-21T00:37:13.184Z,2025-03-21T00:37:13.184Z,,,,,2025-03-21T00:46:49.055Z,2025-03-20T22:04:00.000Z,2025-03-21T00:46:49.055Z,,2025-03-21T10:39:28.131Z,,,575,2026-02-02T14:38:56.372Z,,public,352.9,340.2,0,12.7,21mar-p1-92-partial-outage-of-transacation-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08K080UERX,Reported by Vicnesh Baskar,false
93,Issues uploading multiple PAN files due to parsing errors,Service Incident,Closed,P3 - Non-Critical,"Multiple PAN files are experiencing parsing errors during file upload to card manufacturers.
No WinSCP logs are created, so unable to tell if file was actually uploaded successfully.

There are instances of perso file observed existing in manufacturer's SFTP when manually connecting to verify.
Not all can be verified as some files ""disappear"" after upload, likely due to some automation on manufacturer's end.",Wilson Keneshiro,Dominic Wee,,,Wilson Keneshiro,VoucherEngine,,,,,,,,,,,,,,,,2025-03-22T01:50:29.638Z,2025-03-22T01:50:29.638Z,,,,,2025-05-21T23:05:13.740Z,,2025-05-21T23:05:13.740Z,,,,2025-05-21T23:05:13.740Z,5260484,2025-05-21T23:05:13.866Z,,public,1363.1,1035.3,135.8,192,22mar-p3-non-critical-93-issues-uploading-multiple-pan-files-due-to-parsing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08JZDS3JKW,Reported by Dominic Wee,false
94,Union54 - Job GenerateManufacturerCard failure,Service Incident,Closed,P3 - Non-Critical,Union 54 Job GenerateManufacturerCard failed due to ERROR: smallint out of range on function ptok_cm_create_card_file_generic_v2. ,Muneer Gundlur,Jenifer Cachero,,,Muneer Gundlur,Banking.Live,Union54 / Zazu Zambia,,Perso File generation delay,,,,,,,,,,,,,2025-03-22T02:05:56.968Z,2025-03-22T02:05:56.968Z,,,,,2025-05-21T23:05:26.500Z,2025-03-22T00:00:00.000Z,2025-05-21T23:05:26.500Z,,,,2025-05-21T23:05:26.500Z,5259569,2025-05-21T23:05:26.635Z,,public,312.1,218.2,10,83.9,22mar-p3-non-critical-94-union54-job-generatemanufacturercard-failure,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08K69CNJG1,Reported by Jenifer Cachero,false
95,Failed EP processing due to service issues on TT-PR-APP-01,Service Incident,Closed,P3 - Non-Critical,"**Problem:** EP processing on TT-PR-APP-01 is experiencing multiple failures with return code 20, impacting several sources.

**Impact:** Delayed VISA EP reports for all VE clients.

**Causes:** Delayed processing of VISA EP reports.

**Steps to resolve:** The Log On error for the Edit Package service was fixed by changing its Properties settings, allowing the service to start. Each CIB was checked individually. The Visa batch job for CIB 438247 has completed, triggering automatic processing for the remaining CIBs. Monitoring continues.",Wilson Keneshiro,Wilson Keneshiro,,,Wilson Keneshiro,VoucherEngine,,,EP reports unable to be processed due to Edit Package suddenly stopped. Possible a delay,,,,,,,,,,,,,2025-03-24T00:10:26.035Z,2025-03-24T00:10:26.035Z,,,,,2025-03-24T02:10:18.034Z,2025-03-23T23:19:00.000Z,2025-03-24T00:18:13.836Z,2025-03-24T02:10:18.034Z,,,2025-03-24T02:10:18.034Z,7191,2025-03-24T02:10:18.166Z,,public,108.5,39.4,0,69.1,24mar-p3-non-critical-95-failed-ep-processing-due-to-service-issues-on-tt-p,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08K9TA24BT,Reported by Wilson Keneshiro,false
96,Firewall Disconnection,Service Incident,Closed,P1,"At 09:21 - 09:25 UTC, multiple disconnection alerts were observed, indicating an impact on transactions for ISB, BOJ, MOX, SH02, UW, Nomo, and Mettle. Investigation revealed that a firewall failure caused traffic to switch to the secondary firewall, temporarily disrupting transaction processing.

Recovery alerts were received, and transactions resumed processing as expected. The network team confirmed the firewall issue, and all impacted environments have since stabilized. Affected clients have been notified, and further monitoring is ongoing to ensure system stability.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,"Islandsbanki, Bank of Jordan, Nomo Bank, Mettle, Utility Warehouse, Wio",Traffic automatically failed over to the secondary firewall,,Third-Party - Paymentology,,,,,,,,,,,,2025-03-25T10:09:47.909Z,2025-03-25T10:09:47.909Z,,,,,,,,,,,2025-03-25T10:09:47.909Z,,2026-02-02T14:41:14.411Z,,public,421.9,356.6,55.3,10,25mar-p1-96-firewall-disconnection,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08JSLPU1F1,Reported by Snothile Dlamini,false
98,Full Outage on Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On March 27th at 17:35 UTC, our monitoring team received an alert regarding high response times to FAST calls. This caused a full  16 outage on transaction processing for client WIO

The Advanced Support team was able to confirm a disconnection on the application level, the impact stopped at 17:44 UTC without Paymentology's intervention",Daniel Velado,Daniel Velado,,,,Banking.Live,,,,Inconclusive,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8630173701/Retrospective+-+INC-98+Full+Outage+on+Transaction+Processing,2025-03-27T21:45:39.619Z,2025-03-27T21:45:39.619Z,,,,,,,,,,,,,2026-02-02T14:41:43.681Z,,public,161.9,133.3,28.6,0,27mar-p1-98-full-outage-on-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08K97933M5,Reported by Daniel Velado,false
99,Partial Outage of Transaction Processing,Service Incident,Closed,P1,"On March 28, 2025 at 02:38 UTC, the monitoring team received an alert on connectivity issues on a PayRoute instance, impacting transaction processing. Initial investigation showed that some transactions between 02:38 UTC and 02:47 UTC timed out, suspected due to network delays between Mastercard and Paymentology. Transaction processing normalized after 02:47 UTC with no further advisements. Relevent internal teams and Mastercard have been engaged for a detailed analysis.

Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Dominic Wee,Banking.Live,"Islandsbanki, Wio, Bank of Jordan, Mettle","Issue self normalized, no actions were taken",,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8629518487/Retrospective+-+INC-99+Partial+Outage+of+Transaction+Processing,2025-03-28T03:31:39.665Z,2025-03-28T03:31:39.665Z,,,,,2025-03-28T04:27:40.186Z,2025-03-28T03:30:00.000Z,2025-03-28T04:27:40.186Z,,2025-05-13T22:58:50.757Z,2025-05-13T22:59:08.361Z,2025-05-13T22:59:08.361Z,3360,2026-02-02T14:42:45.839Z,,public,688.9,639.4,20,29.5,28mar-p1-99-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08LCJDBV1N,Reported by Vicnesh Baskar,false
100,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"On 28 March 2025 at 02:38 UTC, the Support team was alerted to an increase in advisements due to transaction timeouts.. Initial investigation showed transaction timeouts due to delays in Fast Active response and transport delays between Mastercard and Paymentology. Transaction processing normalised after 02:46 UTC with no further issues. No issues were found on the application layer, and both Network Team and Mastercard have been engaged for further investigation. 

Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Ardiansyah Ar,Banking.Live,MOX,"No actions takesn, issue self normalized",Partial outage of transaction processing due to timeouts,Inconclusive,,,,,,,,,,,,2025-03-28T04:02:36.678Z,2025-03-28T04:02:36.678Z,,,,,2025-03-28T04:39:24.780Z,2025-03-28T02:46:00.000Z,2025-03-28T04:39:24.780Z,,,,,2208,2025-03-28T05:02:07.752Z,,public,96.4,96.4,0,0,28mar-p2-100-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08KNBUNGSH,Reported by Vicnesh Baskar,false
101,Missing DSR files for multiple clients due to scheduled task issue,Service Incident,Closed,P3 - Non-Critical,"Monitoring team got the PD [Scheduled Task is missing files](https://tutuka.pagerduty.com/alerts/Q0PNDDNOJC48YU ""Scheduled Task is missing files"")
for missing most of DSR for multiple clients.

AS team checked the cause and found it's due to `Error Executing Database Query. Error Executing Database Query.`
[Error - Error Executing Database Query. Error Executing Database Query. - Raygun](https://app.raygun.com/crashreporting/jnx7gn/errors/179454993831?search=settlement&dateFrom=2025-03-22T02%3A24%3A00.000Z&dateTo=2025-03-29T02%3A24%3A33.000Z ""Error - Error Executing Database Query. Error Executing Database Query. - Raygun"")

AS team have already rerun the DSR task for all clients and waiting for completion (usually in 2 hours).",Wilson Keneshiro,Wilson Keneshiro,,,Wilson Keneshiro,VoucherEngine,,,,Paymentology,,,,,,,,,,,,2025-03-29T02:25:46.027Z,2025-03-29T02:25:46.027Z,,,,,2025-03-29T05:03:36.224Z,2025-03-29T01:35:00.000Z,2025-03-29T02:37:22.717Z,,,,2025-03-29T05:03:36.224Z,9470,2026-02-02T14:51:34.681Z,,public,90,90,0,0,29mar-p3-non-critical-101-missing-dsr-files-for-multiple-clients-due-to-sch,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08KFC86F55,Reported by Wilson Keneshiro,false
102,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P3 - Critical,Notification from MC regarding transactions being processed in STIP for BIN 544729 from 04:37 UTC to 04:50 UTC. No ongoing impact as transaction processing has normalised since 04:50 UTC. Further investigation ongoing.,Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,MOX,Issue self normalized,Intermittern time-outs causing transactions to be processed in STIP,Client,,,,,,,,,,,,2025-04-01T06:03:37.728Z,2025-04-01T06:03:37.728Z,,,,,2025-04-01T06:11:02.416Z,2025-04-01T04:37:00.000Z,2025-04-01T06:11:02.416Z,,2025-04-02T08:00:14.935Z,,,444,2026-02-02T14:52:42.716Z,,public,165.3,155.2,0,10.1,01apr-p3-critical-102-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08L4H2MKGD,Reported by Vicnesh Baskar,false
103,DSR Scheduled Task Missing Files for Multiple Clients,Service Incident,Closed,P3 - Non-Critical,"**Issue:** The DSR Scheduled Task is experiencing issues and missing files for multiple clients due to a crash.

**Impact:** Daily Settlement Reports for a mix of clients are missing.

**Causes:** The cause of the incident is an error executing database query, which led to the scheduled task for DSR crashing.

**Steps to resolve:** The DSR task has been rerun, a banner has been raised to inform VE clients, and a JIRA ticket has been raised to the DBA lead for a permanent fix.",Wilson Keneshiro,Wilson Keneshiro,,,Wilson Keneshiro,VoucherEngine,,,Delay in Daily Settlement Report for most of clients,Paymentology,,,,,,,,,,,,2025-04-02T03:31:29.410Z,2025-04-02T03:31:29.410Z,,,,,2025-04-02T06:29:28.056Z,,2025-04-02T03:43:30.950Z,2025-04-02T06:29:28.056Z,,,2025-04-02T06:29:28.056Z,10678,2026-02-02T14:53:05.181Z,,public,88.7,88.7,0,0,02apr-p3-non-critical-103-dsr-scheduled-task-missing-files-for-multiple-cli,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08LNMENNM7,Reported by Wilson Keneshiro,false
104,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P3 - Critical,"On 02 April 2025, at 02:24 AM UTC, the Support team was alerted that no transactions were being processed on 1 of 2 VISA PayRoute instances (VISA_PR01). Initial investigation showed that the Visa PR service encountered a disconnection and did not auto-reconnect. The service was manually restarted at 02:57:10 AM UTC, and transaction processing was normalised with no further issues.

Internal teams have been engaged in a detailed investigation. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,Wio,Visa PR service was manually restarted,,Third-Party - Client,,,,,,,,,,,,2025-04-02T06:48:21.986Z,2025-04-02T06:48:21.986Z,,,,,2025-04-02T06:57:45.249Z,2025-04-02T02:13:00.000Z,2025-04-02T06:57:45.249Z,,2025-04-03T03:02:22.112Z,,,563,2026-02-02T14:53:40.334Z,,public,248,207.5,0,40.5,02apr-p3-critical-104-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08LP9WRQ73,Reported by Vicnesh Baskar,false
105,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On 03 April 2025 at 02:34 AM UTC, the support team was alerted to a slow response on the PayPower database, causing some transactions to be processed in standin. Further investigation indicated blocking from a settlement matching job that lasted between 02:27-02:32 AM UTC, which caused delays in the rule engine. The issue was resolved by 02:34 AM UTC when the job ended, and transaction processing normalised thereafter.

An in-depth investigation is currently underway to identify the root cause for the blocking, and further updates, including any preventive actions, will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Dominic Wee,Banking.Live,MOX,Issue resolved once the settlement matching job completed its run,"Intermittent time-outs impacting transaction processing, causing transactions to be processed in standin by MC",Paymentology,,,,,,,,,,,,2025-04-03T02:48:28.624Z,2025-04-03T02:48:28.624Z,,,,,2025-04-03T04:00:02.337Z,2025-04-03T02:46:00.000Z,2025-04-03T04:00:02.337Z,,2025-04-04T00:05:55.226Z,2025-04-08T08:04:36.949Z,2025-04-08T08:04:36.949Z,4293,2025-04-08T08:04:37.042Z,,public,492.4,405.7,20,66.7,03apr-p3-critical-105-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08LGKG16UD,Reported by Vicnesh Baskar,false
106,Partial outage of Transction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On April 3, 2025, 04:20 UTC, Paymentology's support team was alerted on PIN format errors for some transactions on Node ALTC_3A. Initial investigation showed that vendor Altech exchanged a new key with 3A at 22:40 SAST and took 4B back online at 00:04 SAST, causing PIN format errors due to the new key not being shared with 3A. The Node 3A was restarted at 05:10 UTC, and transaction processing normalised after 05:11 UTC with no further errors. 

Altech has been engaged in a detailed investigation. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Wilson Keneshiro,VoucherEngine,SBSA PayCard,Restart of node ALTC_3A,Partial transaction failures,Third-Party - Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_NzdjYmVlNTUtMmQ3MS00MzMxLWFkMTAtMTE0OGZkNzk3YjY2%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%225bc8ffce-74fa-4e51-af76-275e9877517e%22%7d,Transaction Processing,"Altech did a failover just before 23:00 SAST last night exchanging a new key with 3A at 22:40. At roughly 00:04 SAST they took 4B back online for almost an hour during which time transactions on 3A were starting with PIN format errors 9120's. They exchanged a new key with 4B at 00:00, but this new key was not shared with 3A, hence the errors. Altech is currently using shared keys for prod, but we are configured for separate keys and Altech will switch back to separate keys again after UAT testing. To avoid the issue they should've exchanged the same key with both 3A and 4B",Yes - #13mar-p1-84-partial-outage-of-transacation-processing,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8644984955/Retrospective+-+INC-106+Partial+outage+of+Transction+Processing,2025-04-03T05:08:17.374Z,2025-04-03T04:28:00.000Z,,,,,2025-04-02T21:11:00.000Z,2025-04-03T04:20:00.000Z,2025-04-03T04:21:00.000Z,2025-04-03T05:11:00.000Z,2025-04-04T00:26:47.607Z,,,8053,2025-04-04T00:26:47.945Z,,public,307.4,289,0,18.4,03apr-p1-106-partial-outage-of-transction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08LH1XNF0D,Reported by Vicnesh Baskar,false
107,Transaction Processing Issue,Service Incident,Post-Incident Tasks & Documentation,P1,"Multiple Crimson alerts indicating failures, particularly high XML-RPC latency on transaction processing hosts. Initial alerts were resolved, but new ones continued to appear. Database Team is investigating the procedure causing the spikes",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,,Database team killed the long-running query. ,,Paymentology,https://paymentology.zoom.us/j/89802996453?pwd=3f5xKbIGcOGl6jp3wxwYw7uwPR0PoT.1,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8646328326/Retrospective+-+INC-107+Transaction+Processing+Issue,2025-04-03T12:23:14.164Z,2025-04-03T12:23:14.164Z,,,,,2025-04-03T12:37:33.376Z,2025-04-03T12:22:00.000Z,2025-04-03T12:37:33.376Z,,,,,859,2025-04-03T13:56:51.098Z,,public,295.4,295.4,0,0,03apr-p1-107-transaction-processing-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08LX5CAB09,Reported by Snothile Dlamini,false
108,Full Outage on Cortex Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P4,"**Issue:** There was a full outage on CORTEX transaction processing for BOJ, stopping all transactions.

**Impact:** Transactions were stopped, impacting the transaction processing.

**Causes:** The specific cause of the outage has not been identified, as application logs did not show any errors. Network team is currently investigating as well.

**Steps to resolve:** PR and PP were restarted, but it did not have any effect. Transactions resumed processing successfully without any specific action from Paymentology.",Daniel Velado,Daniel Velado,,,,Banking.Live,Bank of Jordan,,,Client,"https://teams.microsoft.com/l/meetup-join/19:meeting_MmUxNWQyOWEtNzY5Yi00Mzg0LWEwNzUtOGI0ODMzM2Q4YTk1@thread.v2/0?context=%7B%22Tid%22:%224f7797e2-a59f-4381-ad8e-b81223603c9d%22,%22Oid%22:%2277fd2081-e5b3-4187-a436-a13e4cc70a0f%22%7D",,,,,,,,,,,2025-04-03T17:10:03.550Z,2025-04-03T17:10:03.550Z,,,,,2025-04-03T17:28:46.060Z,,2025-04-03T17:28:46.060Z,,,,,1122,2026-02-02T14:55:29.466Z,,public,150.9,140.9,10,0,03apr-p4-108-full-outage-on-cortex-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08LFDE3PRU,Reported by Daniel Velado,false
109,Partial Outage on Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"On April 3rd at 19:52 UTC, Paymentology Monitoring team received an alert regarding db usage, which partially interrupted authorisations for 4 minutes.

The impact was from 19:47 to 19:51 UTC.",Daniel Velado,Daniel Velado,,,,Banking.Live,Bank Al Etihad,,,Paymentology,,,,,,,,,,,,2025-04-03T22:36:27.032Z,2025-04-03T22:36:27.032Z,,,,,,,,,,,,,2026-02-02T14:58:43.623Z,,public,15.3,15.3,0,0,03apr-p2-109-partial-outage-on-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08LS4PUTUL,Reported by Daniel Velado,false
110,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 04 April 2025 at 02:36 UTC, Paymentology's support team was alerted on processing issues in a PayPower instance. Initial investigation showed that the PayPower_01 server was non-responsive, impacting transactions processing via PayPower_01 from multiple schemes.

A reboot was initiated at 03:11 UTC and completed at 03:33 UTC. Transaction processing via PayPower_01 normalised thereafter, and no new issues were observed during monitoring.

The infrastructure team is currently performing a detailed investigation to identify the root cause and further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Dominic Wee,Banking.Live,Wio,PayPower_01 server was rebooted,Some transactions are being processed in standin by MC,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8647999493/Retrospective+-+INC-110+Partial+Outage+of+Transaction+Processing,2025-04-04T03:12:47.103Z,2025-04-04T03:12:47.103Z,,,,,2025-04-04T05:33:48.940Z,,2025-04-04T03:27:30.364Z,2025-04-04T03:40:54.946Z,2025-04-04T06:24:59.888Z,,,8461,2025-04-04T07:04:27.279Z,,public,650.2,542.9,74.3,33,04apr-p1-110-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08LAPG1N0P,Reported by Vicnesh Baskar,false
111,API Processing Timeouts,Service Incident,Closed,P2,"D360 experienced slowness, timeouts, and ""bad gateway"" errors across all APIs and PayControl in the production environment. The issue was traced to unresponsiveness from PayAPI node `10.12.2.4`, which was making API calls to the backend virtual IP `10.16.1.57`. From 07:23 UTC, no responses were received from the backend, causing threads to hang and the node to become unresponsive. Despite one node being affected, the load balancer did not redirect traffic to the healthy node, leading to a broader impact.

Both PayAPI services were restarted on nodes `10.12.2.4` and `10.12.2.5`, restoring normal API functionality. No errors were observed post 07:58 UTC.",Snothile Dlamini,Snothile Dlamini,,,Kamal Thapa,Banking.Live,D360,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8649900170/Retrospective+-+INC-111+API+Processing+Timeouts,2025-04-07T08:41:59.154Z,2025-04-07T08:41:59.154Z,,,,,2025-04-07T07:59:00.000Z,2025-04-07T07:23:00.000Z,,,,,2025-04-07T08:41:59.154Z,,2026-02-02T15:00:20.543Z,,public,143.7,133.3,0,10.4,07apr-p2-111-api-processing-timeouts,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08MC62F717,Reported by Snothile Dlamini,false
112,Partial Processing Outage on MADA Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"**Issue:** High CPU usage on D360 caused server MADA 4 to stop processing transactions and the Rule Engine to decline transactions.

**Impact:** Partial TBD

**Causes:** Long Query on DB

**Steps to resolve: The** Issue resolved itself without Paymentology's intervention.
Rule Engine error is a separate behaviour caused by DE22, and it is already addressed.",Daniel Velado,Daniel Velado,,,,Banking.Live,D360,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8657273566/Retrospective+-+INC-112+Partial+Processing+Outage+on+MADA+Transaction+Processing,2025-04-08T18:45:57.483Z,2025-04-08T18:45:57.483Z,,,,,2025-04-08T19:05:21.111Z,,2025-04-08T19:05:21.111Z,,,,,1163,2025-04-11T00:21:45.247Z,,public,352.4,301.8,50.6,0,08apr-p2-112-partial-processing-outage-on-mada-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08ME72HHCK,Reported by Daniel Velado,false
113,Partial Outage of Transction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P3 - Critical,"On 10th April 2025 at 00:54 UTC, the client reported that they were observing that 2 bin routes were down for MADA. Initial investigation showed that 1 out of 4 MADA PRs was down since 22:26 UTC, 9th April 2025. The service was restarted, and transaction processing normalised after 03:07 UTC with no further issues. The issue was resolved by restarting the affected MADA instance. Internal investigation is ongoing, and further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,D360,The issue was resolved by restarting the affected MADA instance,Loss of resiliency and partial outage as some transaction processing via 1 out of 4 MADA PRs impacted,Paymentology,,,,,,,,,,,,2025-04-10T04:29:35.451Z,2025-04-10T04:29:35.451Z,,,,,2025-04-10T04:33:34.581Z,2025-04-08T22:26:00.000Z,2025-04-10T04:33:34.581Z,,2025-04-10T07:23:45.241Z,,,239,2026-02-02T15:02:59.009Z,,public,313.4,294.8,0,18.6,10apr-p3-critical-113-partial-outage-of-transction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08MNJRCY3C,Reported by Vicnesh Baskar,false
114,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"On April 10th at 20:13 UTC our Monitoring Team received an alert regarding intermitent connection on PayPower. Paymentology Network team confirmed we experienced VPN disconnections for some clients, however only Coverflex had affected transactions

We are currently investigating with the ISP for root cause analysis.

Start time:10-Apr-2025 20:02
End time :10-Apr-2025 20:36",Daniel Velado,Daniel Velado,,,,Banking.Live,Coverflex,,,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8657273661/Retrospective+-+INC-114+Partial+Outage+of+Transaction+Processing,2025-04-10T22:20:42.240Z,2025-04-10T22:20:42.240Z,,,,,,,,,,,,,2026-02-02T15:02:08.738Z,,public,35.1,25.1,10,0,10apr-p2-114-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08MTCHTK4L,Reported by Daniel Velado,false
115,D360 GenerateSettlementReport failure - MADA,Service Incident,Closed,P3 - Non-Critical,"Max heap space for D360 PayScheduler was increased to 8 GB from 4 GB.
GenerateSettlementReport had successfully completed",Dominic Wee,Dominic Wee,,,,Banking.Live,D360,,,Paymentology,,,,,,,,,,,,2025-04-11T01:28:19.343Z,2025-04-11T01:28:19.343Z,,,,,2025-04-11T01:44:58.660Z,,2025-04-11T01:44:58.660Z,,,,2025-04-11T01:44:58.660Z,999,2026-02-02T15:04:33.647Z,,public,109.4,109.4,0,0,11apr-p3-non-critical-115-d360-generatesettlementreport-failure-mada,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08MU04HJKE,Reported by Dominic Wee,false
116,HighLatencyOnMeaWalletCallretrieveContactData,Service Incident,Closed,P4,"We received PD alert on the High Latency on /api/pws/predigitize-lite/v1/retrieveContactData on SharedEU. Per checking the API servers are up and processing fine. API calls are successful and only the API pws_mea_wallet_retrieve_contact_data encountered connection timed out. API Instance 01 was restarted to be sure all processing fine. Per further checking with Infra team, found that load balancer processing fine and other mea wallet calls are also processing fine. The pws_mea_wallet_retrieve_contact_data is not frequently called that is why we are not seeing any new calls as of now. No impact on AUTH and API.",Jenifer Cachero,Jenifer Cachero,,,Jenifer Cachero,Banking.Live,,,,,,,,,,,,,,,,2025-04-11T06:25:18.657Z,2025-04-11T06:25:18.657Z,,,,,2025-05-21T23:05:41.395Z,,2025-05-21T23:05:41.395Z,,,,2025-05-21T23:05:41.395Z,3516022,2025-05-21T23:05:41.495Z,,public,21.3,21.1,0,0.2,11apr-p4-116-highlatencyonmeawalletcallretrievecontactdata,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08MJCKH56J,Reported by Jenifer Cachero,false
117,MC clearing file processing failure,Service Incident,Root Cause Analysis and Corrective Actions,P1,"The issue affecting Mastercard T112 clearing files has been confirmed to stem from a server migration performed by Mastercard over the weekend. Paymentology was moved to a new MFE server, which introduced an encoding error in the clearing files, causing parsing failures.

Mastercard has since rolled back the change and moved Paymentology back to the legacy system. A test file was restaged by Mastercard and successfully parsed by Paymentology, confirming the resolution. All impacted files from April 13th onward have been resent and processed successfully. No further files are pending.

A formal RCA will be shared by Mastercard. Transaction processing remained unaffected throughout the incident.",Vicnesh Baskar,Vicnesh Baskar,,,Abdul Sahaptheen,Banking.Live,"Islandsbanki, Wio, MOX, Nomo Bank, Bank of Jordan, Mettle, Tweeq, Utility Warehouse, Constantinople, C24, Snappi, Xendit, Manigo","Paymentology was moved to a new MFE server, which introduced an encoding error in the clearing files, causing parsing failures.  Mastercard has since rolled back the change and moved Paymentology back to the legacy system.",3rd clearing file for processing is delayed for all MC clients.,Third-Party - Client,,,,,,,,,,,https://app.incident.io/paymentology/incidents/01JRQ9HGJWVY3J318X7TXS31XT?tab=postmortem_document,2025-04-13T10:01:25.372Z,2025-04-13T10:01:25.372Z,,,,,2025-04-15T03:22:52.464Z,,2025-04-15T03:22:52.464Z,,2025-04-17T05:53:13.604Z,,,148887,2026-02-02T15:05:16.036Z,,public,3461.8,2490.4,524.6,446.8,13apr-p1-117-mc-clearing-file-processing-failure,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08N2M2D80K,Reported by Vicnesh Baskar,false
118,Partial Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,"Partial Outage occurred from 12:46 till 13:01 UTC on 14th April. One instance of PayRoute was not processing, it resumed processing without any action from support. Root cause investigation in progress.",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,TymeBank SA,,Partial Outage on Transaction Processing. 226 transactions impacted. ,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8666382363/Retrospective+-+INC-118+Partial+Outage+of+Transaction+Processing,2025-04-14T13:27:23.579Z,2025-04-14T13:27:23.579Z,,,,,2025-04-14T16:46:40.000Z,2025-04-14T04:46:00.000Z,2025-04-14T16:46:40.000Z,,,,,11956,2025-04-15T07:18:56.994Z,,public,340.2,274.3,23.8,42.2,14apr-p1-118-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08MNA85ZHD,Reported by Kaisar Mahmood,false
119,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P3 - Critical,"On 15 April 2025 at 02:40 UTC, the Monitoring team was alerted on a disconnection for multiple VISA PayRoute (PR) instances. Initial investigation showed that some VISA PR instances experienced brief disconnections. While most automatically reconnected within 20 seconds, 1 VISA PR failed to reconnect and required a manual restart.

The instance was restarted at 03:02 UTC, after which transaction processing normalised. VISA has been engaged to confirm if there were any activities or disruptions. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,"Wio, D360",1 VISA PR was restarted,Partial Outage of Transaction Processing,Third-Party - Client,,,,,,,,,,,,2025-04-15T03:31:01.697Z,2025-04-15T03:31:01.697Z,,,,,2025-04-15T04:18:22.551Z,,2025-04-15T04:18:22.551Z,,2025-04-15T06:43:54.527Z,,,2840,2026-02-02T15:06:05.035Z,,public,293.4,283.4,10,0,15apr-p3-critical-119-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08MYJB82KG,Reported by Vicnesh Baskar,false
245,Duplicate Card Record PAN File,Service Incident,Post-Incident Tasks & Documentation,P1,"On April 15, perso file processing failures resulted in duplicate entries across three PAN files, causing 36,000 duplicate card records to be generated (20% of GoTyme’s April order). Impacted cards were scrapped, leading to operational, financial, and reputational impact.

Multiple clients and manufacturers were affected, including GoTyme, OneCard, Albo, RCS, Remessa, Kauri, P100, and Coverally. Root cause is suspected to be linked to perso file reprocessing without clearing temporary files and/or misconfiguration during AWS migration (pgp utilities folder missing). Further validation is ongoing.",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,"Albo, GoTyme, OneCard / OnePlan, RCS Group",L2 Support reprocessed 22 of 23 perso files; remaining file (Albo 44409) reprocessed on May 1 after escalation,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8970436617/2025-04-15+-+P1+-+Duplicate+Card+Record+PAN+File,2025-09-05T12:38:01.525Z,2025-04-15T13:01:00.000Z,,,,,2025-04-16T01:05:00.000Z,,,,,,,43440,2026-02-02T16:53:14.439Z,,public,369.1,358.8,10.3,0,05sep-p1-245-duplicate-card-record-pan-file,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09DV6WEGRJ,Reported by Snothile Dlamini,false
120,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 16 April 2025 at 08:07 HKT, the Application Support team was alerted about a partial outage impacting transaction processing. Initial investigation revealed that the PayKeyService (PKS) was unable to connect to its configured Hardware Security Modules (HSMs). HSM01 (primary) and HSM02 (secondary) were unavailable, resulting in all transactions that required the HSM’s to fail in processing.

HSM01 was not reachable and could not be remotely restarted. HSM02 was successfully restarted remotely, after this restart HSM02 started successfully processing transactions. All HSM traffic continued to successfully process via HSM02. To restore full resiliency, an ECR was implemented at 18:00 UTC on 16th April to add a HSM in Singapore as the Secondary HSM. HSM01 currently remains inaccessible. PYTG support teams are continuing to work with HSM vendor (Utimaco) to recovery and operationalise the appliance.

Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).  ",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,MOX,"Restart HSM02, reconfigure DR HSM as backup",Partial Outage of Transaction Processing,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8671887381/Retrospective+-+INC-120+Partial+Outage+of+Transaction+Processing,2025-04-16T00:33:42.628Z,2025-04-16T00:33:42.628Z,,,,,2025-04-16T02:18:00.000Z,2025-04-16T00:07:00.000Z,2025-04-16T02:30:05.741Z,,2025-05-22T07:10:38.322Z,,,6257,2026-02-02T15:07:24.177Z,,public,3789.5,3051.2,448.1,290.3,16apr-p1-120-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08NG7G8D26,Reported by Vicnesh Baskar,false
121,Full Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue:** ADQ is experiencing a partial outage of transaction processing due to database long running queries.  **Impact:** Transactions were not processing, impacting the handling of MC and VISA transactions.

**Causes:** The issue is caused by long running database queries, which resulted in CPU usage reaching 100%.

**Steps to resolve:** The long queries have been terminated ad applications were restarted to clear the remaining db connections, transaction processing for MC and VISA have resumed. A Jira ticket is being raised to address and solve the issue more efficiently in the future.",Daniel Velado,Daniel Velado,,,,Banking.Live,Wio,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8671297622/Retrospective+-+INC-121+Full+Outage+of+Transaction+Processing,2025-04-16T17:20:18.608Z,2025-04-16T17:20:18.608Z,,,,,2025-04-16T17:55:38.204Z,,2025-04-16T17:55:38.204Z,,,,,2119,2026-02-02T15:07:51.638Z,,public,521.9,461.8,40.1,20,16apr-p1-121-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08NDS50GH3,Reported by Daniel Velado,false
125,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On April 19th at 14:34 UTC our Monitoring team received an alert regarding a PayRoute instance not receiving MC transactions.
Advanced support restarted the affected PayRoute instance and transactions started to arrive at 14:49 UTC.

An internal ticket has been created for our Application team to investigate the reason behind the disconnection.

Started: 2025-04-19 14:27:16.764
Finished: 2025-04-19 14:49:58.210

Advice messages: 5720
Approved transactions: 5816
Impact: 0.49%",Daniel Velado,Daniel Velado,,,,Banking.Live,C24,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8675033237/Retrospective+-+INC-125+Partial+Outage+of+Transaction+Processing,2025-04-19T15:50:18.750Z,2025-04-19T15:50:18.750Z,,,,,,,,,,,,,2026-02-02T15:08:37.723Z,,public,278.1,261.1,17,0,19apr-p1-125-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08PJG6R81W,Reported by Daniel Velado,false
126,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 21 April 2025 at 00:36 UTC, the monitoring team was alerted on errors in the PayRoute instance connecting to AFS. Initial investigation showed repeated socket connection resets between AFS and PayRoute instance, resulting in advisement messages for some transactions since 22:51 UTC, 20th April. 

Despite these resets, the system was able to reconnect automatically on each reset and continued to process some transactions. The last advisement was observed at 00:36 UTC, 21st April. PayRoute-AFS instance was manually restarted at 00:51 UTC as a precautionary recovery step. Close monitoring was in place and no further advisements were received.

AFS and Paymentology’s Network team have been engaged for a detailed analysis. Further updates on the root cause and preventive actions will be included in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Dominic Wee,Banking.Live,Dopay LLC,The issue was resolved by restarting the PayRoute-AFS service and restoring connectivity to AFS,Partial Outage of Transaction Processing,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8675393563/Retrospective+-+INC-126+Partial+Outage+of+Transaction+Processing,2025-04-21T01:04:15.878Z,2025-04-21T01:04:15.878Z,,,,,2025-04-21T01:50:14.796Z,2025-04-20T22:51:00.000Z,2025-04-21T01:50:14.796Z,,2025-04-21T06:08:48.886Z,,,2758,2026-02-02T15:09:09.972Z,,public,402.2,332,30.2,40,21apr-p1-126-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08P1HBLLH1,Reported by Vicnesh Baskar,false
127,Partial Outage of Transaction Processing - MOX,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"Partial outage on transaction processing occurred from 11:13 UTC till 11:23 UTC on 21st April. 119 transactions were impacted. Currently transactions are processing as expected. Initial investigation shows signs of delayed response from Rules Engine, further investigation into the technical root cause is ongoing.",Kaisar Mahmood,Kaisar Mahmood,,,Kamal Thapa,Banking.Live,MOX,"Issue auto resolved, no action taken by support teams to resolve the issue.",Partial Outage on Transaction Processing (119 Txns Impacted / 2141 Successfully processed)  ,Paymentology,,,,,,,,,,,,2025-04-21T12:31:56.054Z,2025-04-21T12:31:56.054Z,,,,,2025-04-21T14:00:23.084Z,2025-04-21T03:13:00.000Z,2025-04-21T14:00:23.084Z,,,,,5307,2025-04-21T14:00:24.167Z,,public,691.7,447.2,186.6,58,21apr-p3-critical-127-partial-outage-of-transaction-processing-mox,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08P3N3RW1G,Reported by Kaisar Mahmood,false
128,Partial Outage of API Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"**Issue:** On April 22nd at 15:44 UTC, we received reports of intermittent API calls returning 404 errors for client Rain.

Advanced support restarted the server with high CPU usage and this stopped the impact. Additional restarts were also performed on the other API servers to prevent any future CPU spike

**Impact:** Approximately 15% of API calls are affected, accurate number couldn't be retrieved as those calls were not reaching our application.
15% was calculated using the 117 failed examples provided by the client against the 600+ successful API calls that were processed within the same timeframe.

**Causes:** Preliminary investigation shows a high CPU usage on papi06 server. Internal tickets were created for our team to investigate what caused the high CPU usage and to create an alert for 404 errors on Nginx servers.

 **Steps to resolve:**  Application Restart

 

 ",Daniel Velado,Daniel Velado,,,,Banking.Live,Rain,,,Paymentology,"https://teams.microsoft.com/l/meetup-join/19:meeting_ZjlhNWIyYTAtZDk1Ni00ODhjLWEzOGUtZjRlYWQ2YWFkNDll@thread.v2/0?context=%7B%22Tid%22:%224f7797e2-a59f-4381-ad8e-b81223603c9d%22,%22Oid%22:%2277fd2081-e5b3-4187-a436-a13e4cc70a0f%22%7D",,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8680210462/Retrospective+-+INC-128+Partial+Outage+of+API+Processing,2025-04-22T17:34:38.585Z,2025-04-22T17:34:38.585Z,,,,,2025-04-22T21:03:23.945Z,,2025-04-22T19:00:51.993Z,,,,,8924,2026-02-02T15:09:53.686Z,,public,442.9,359,30,53.9,22apr-p2-128-partial-outage-of-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08PAPMQ2HY,Reported by Daniel Velado,false
129,Full Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 23rd April 2025 at 00:14 UTC, the support team was informed of a Jetco transaction processing issue by the client, who was contacted by JETCO regarding transaction failures. Further investigation revealed a disconnection on Jetco PayRoute since 08:02 HKT, as the server hosting Jetco PayPower and Jetco PayRoute services had reached 100% disk utilisation due to log file growth. This caused the virtual machine to shut down, leading to a complete halt in Jetco transaction processing.

Disk cleanup actions were performed, and PayRoute services were restarted at 09:26 HKT. Jetco was then engaged for a key exchange, which was completed at 09:33 HKT, and transaction processing normalised thereafter.

Further root cause analysis is underway and will be detailed in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,MOX,"Disk cleanup actions were done, and services were restarted",Partial Outage of,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8681062406/Retrospective+-+INC-129+Full+Outage+of+Transaction+Processing,2025-04-23T00:34:51.267Z,2025-04-23T00:34:51.267Z,,,,,2025-04-23T02:35:13.159Z,2025-04-23T00:00:00.000Z,2025-04-23T02:35:13.159Z,,2025-04-23T05:47:12.516Z,,,7221,2025-04-23T07:20:55.059Z,,public,763.6,571.3,74.6,117.7,23apr-p1-129-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08PLPKBLTT,Reported by Vicnesh Baskar,false
130,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"On 24 April 2025 at 08:58 HKT, the support team was alerted by Mastercard regarding intermittent timeouts affecting BIN 544729. Initial investigation confirmed that the platform was stable and processing transactions as expected, apart from some transactions that timed out due to a delay with Mastercard.
The last advisement was received at 09:12 HKT, after which normal processing resumed. The platform continued to process valid transactions during the incident window. The issue appears to have originated externally, and Mastercard has been engaged for further investigation. Further updates on the root cause and preventive measures will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,MOX,N/A,Some transactions were intermitternly timing out between 00:28 UTC to 00:50 UTC,Third-Party - Paymentology,,,,,,,,,,,,2025-04-24T01:31:53.750Z,2025-04-24T01:31:53.750Z,,,,,2025-04-24T08:08:07.415Z,2025-04-24T00:28:00.000Z,2025-04-24T08:08:07.415Z,,,,,23773,2025-04-24T08:08:08.470Z,,public,773.3,698.2,10,65.2,24apr-p3-critical-130-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08P73WNV9C,Reported by Vicnesh Baskar,false
131,MDES Tokenization Failure ,Service Incident,Closed,P2,"Paymentology MeaWallet I-TSP service was not receiving any calls (from around 10:18 UTC) , this resulted in MDES tokenization failures. Investigation identified one certificate was missing on incoming, the missing certificate was added (at 14:17 UTC) and we subsequently started receiving incoming calls. Clients confirmed the issue is resolved.",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,"Salaam Bank - Somalia, Wio, Utility Warehouse, C24, Constantinople",Missing certificate was added,MDES Tokenization Failure ,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZTBkODc5NTUtZDg0My00ZDJiLWE1OWYtOGU2NWMzZjE4NzBj%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%22d48a7985-ef95-4c3b-8394-cfba4ea3afbb%22%7d,,,,,,,,,,,2025-04-24T13:16:26.504Z,2025-04-24T13:16:26.504Z,,,,,2025-04-24T14:59:56.520Z,2025-04-24T01:23:00.000Z,2025-04-24T14:59:56.520Z,,2025-04-25T06:56:42.686Z,2025-07-31T00:07:44.672Z,2025-07-31T00:07:44.672Z,6210,2025-07-31T00:07:44.802Z,,public,648.9,480.5,123.3,45.2,24apr-p2-131-mdes-tokenization-failure,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08PMDD3A58,Reported by Kaisar Mahmood,false
132,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P3 - Critical,"On 28 April 2025 at 03:36 UTC, the technical team was alerted by Mastercard that some transactions were being processed in standin. Investigation found the delays were caused by long-running requests on ColdFusion application servers (TRN-01, TRN-03, and TRN-05), which led to slowness in transaction responses and ultimately caused transaction timeouts. ColdFusion (CF) services were restarted and transactions normalised after 05:37 UTC. Further analysis is ongoing, and a detailed root cause and any corrective actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Ardiansyah Ar,VoucherEngine,Agoda,ColdFusion (CF) services were restarted ,Intermittern transaction timeouts,Paymentology,,,,,,,,,,,,2025-04-28T04:56:29.343Z,2025-04-28T04:56:29.343Z,,,,,2025-04-28T07:07:27.745Z,,2025-04-28T05:59:40.426Z,,2025-04-28T08:07:50.280Z,,,7858,2025-04-28T08:07:50.616Z,,public,411.6,371.6,20,20,28apr-p3-critical-132-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08PP92AW5U,Reported by Vicnesh Baskar,false
133,Full Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"Full outage on transaction processing, preliminary investigation shows that it could have been caused by a db failure.

Advices:
MC - 1478
Visa - 17
Do Not Honor (05) : 79**
**
Start time: 17:35 UTC
End time: 17:49 UTC",Daniel Velado,Daniel Velado,,,,Banking.Live,Wio,,,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8694038924/Retrospective+-+INC-133+Full+Outage+of+Transaction+Processing,2025-04-28T19:34:34.382Z,2025-04-28T19:34:34.382Z,,,,,,,,,,,,,2026-02-02T15:11:36.794Z,,public,912.9,761.2,141.5,10,28apr-p1-133-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08QQH1NUHE,Reported by Daniel Velado,false
134,Full Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"Disconnection on GIM nodes:
PROD_PANL_7C
PROD_GIMI_3C
PROD_FRMN_7C

Start time: 2025-04-28 22:11UTC
End time: 2025-04-28 22:42 UTC",Daniel Velado,Daniel Velado,,,,VoucherEngine,,,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8694006219/Retrospective+-+INC-134+Full+Outage+of+Transaction+Processing,2025-04-28T22:59:31.836Z,2025-04-28T22:59:31.836Z,,,,,,,,,,,,,2026-02-02T15:12:22.010Z,,public,191.2,137.6,12.1,41.5,28apr-p1-134-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08Q6N6MV44,Reported by Daniel Velado,false
135,Full Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On April 30th at 23:14 UTC PYTG monitoring systems triggered alerts indicating there was a transaction processing issue.

Monitoring Team confirmed no transactions were visible since 23:03 UTC and

Advanced Support restarted the affected applications.
Traffic visible after the restart, transactions started processing as expected with no issues from 23:20 UTC.


Advanced Support confirmed there were no connection errors on PYTG systems or applications. Mastercard has been engaged to assist in the investigation as no authorisations were received from their side at the time of the incident.",Daniel Velado,Daniel Velado,,,,Banking.Live,C24,,,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8698691643/Retrospective+-+INC-135+Full+Outage+of+Transaction+Processing,2025-04-30T23:32:07.596Z,2025-04-30T23:32:07.596Z,,,,,,,,,,,,,2026-02-02T15:13:14.419Z,,public,383.6,268.2,35.2,80.2,01may-p1-135-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08R49F2Y8Y,Reported by Daniel Velado,false
137,Full Outage of Transaction Processing - D360,Service Incident,Root Cause Analysis and Corrective Actions,P1,"Full Outage of transaction processing occurred from 10:43 -10:57 UTC on 3rd May. Currently transactions are processing as expected with no issues. Investigation into the technical root cause is on-going, thus far no issues identified within PYTG application/infra.",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,D360,"Auto resolved, no action was taken by PYTG support teams.",Full Outage of transaction processing occurred from 10:43 -10:57 UTC on 3rd May - (Transactions were processed in STIP),Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8703934833/Incident+Post+Mortem+INC-137+2025-05-03+-+P1+-+Full+Outage+of+Transaction+Processing+-+D360,2025-05-03T13:03:55.323Z,2025-05-03T13:03:55.323Z,,,,,2025-05-05T07:06:15.616Z,2025-05-03T02:43:00.000Z,2025-05-03T13:16:31.420Z,,2025-05-05T07:07:50.382Z,,,756,2026-02-02T15:14:41.879Z,,public,268.8,168.3,78.9,21.6,03may-p1-137-full-outage-of-transaction-processing-d360,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08R4N0QSLR,Reported by Kaisar Mahmood,false
138,Partial Outage of Transaction Processing,Service Incident,Closed,P1,"On 08 May 2025 at 05:02 UTC, the Support team was alerted about a partial outage impacting transaction processing in SH02. Initial investigation showed that tokenised FAST transactions were failing with HTTP 400 errors after a deployment, causing transactions to be routed to STIP. The issue was identified as a JSON formatting error in the FAST request specific to the deployment. A hotfix was prepared and deployed across all PayPowers, with deployment completed by 07:35 UTC. Transaction processing normalised after 07:48 UTC with no further failures observed. The issue was resolved by fixing the JSON formatting and deploying the corrected version. Internal technical teams have been engaged for a detailed investigation. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Abdul Sahaptheen,Banking.Live,Constantinople,,Tokenized transactions are being processed in Standin,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8715042847/2025-05-08+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-05-08T06:52:07.825Z,2025-05-08T06:52:07.825Z,,,,,2025-05-08T08:03:52.662Z,,2025-05-08T07:36:49.493Z,2025-05-08T08:03:52.662Z,2025-05-09T06:37:29.121Z,2025-05-13T22:50:02.303Z,2025-05-13T22:50:02.303Z,4304,2026-02-02T15:15:05.318Z,,public,423.6,374.2,0,49.4,08may-p1-138-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08R7UHP3JA,Reported by Vicnesh Baskar,false
139,Intermittent Transaction Processing Outage,Service Incident,Closed,P4,AS and Auth team checked the corresponding message on payroute logs for the few impacted auth from paypower and have concluded that there is no impact. It was a logging issue on the Paypower side but the correct message is sent to the Payroute and eventually UAEswitch. ,Snothile Dlamini,Snothile Dlamini,,,Kamal Thapa,Banking.Live,Wio,N/A,,,https://teams.microsoft.com/l/meetup-join/19%3ameeting_MzRiMzJmZGEtMWEwZC00YjQyLWI0ODA%5B%E2%80%A6%5D2c%22Oid%22%3a%22067d56db-044f-4b5b-b44c-8303c87625ab%22%7d,,,,,,,,,,,2025-05-09T12:12:02.151Z,2025-05-09T12:12:02.151Z,,,,,2025-05-09T13:13:17.549Z,,2025-05-09T13:13:17.549Z,,2025-05-09T13:13:29.548Z,,2025-05-09T13:13:29.548Z,3675,2025-05-09T13:17:18.775Z,,public,369.8,334.8,35.1,0,09may-p4-139-intermittent-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08SAAS9KT2,Reported by Snothile Dlamini,true
140,Full Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue:** There was a full outage of transaction processing for BOJ starting from 13-May-2025 17:02:15.483.

**Impact:** There was a complete interruption of transaction processing for clients.

**Causes:** The outage was related to a routing issue between BOJ and Azure

**Steps to resolve:** Transactions resumed by 13-May-2025 18:20:51.570, following a conference call with BOJ's infrastructure team to troubleshoot and ensure connections were restored.",Daniel Velado,Daniel Velado,,,,Banking.Live,Bank of Jordan,,,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8745910392/2025-05-13+-+P1+-+Full+Outage+of+Transaction+Processing,2025-05-13T17:41:29.068Z,2025-05-13T17:41:29.068Z,,,,,2025-05-13T18:36:14.364Z,,2025-05-13T18:22:32.339Z,2025-05-13T18:36:14.364Z,,,,3285,2026-02-02T15:17:03.980Z,,public,441.4,391.3,50.1,0,13may-p1-140-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08SV6HU4BS,Reported by Daniel Velado,false
141,D360 - Missing VISAIN files for 13 May,Service Incident,Closed,P4,"VISAIN files for 13 May were initially reported as missing which could affect settlements.

An early trigger of the ProcessSettlementVISA likely caused the alert due to a D360 deployment PS restart.

The incident was downgraded to a P4 as there was no impact, and confirmation was received that the necessary settlement report had been generated and sent.",Vicnesh Baskar,Dominic Wee,,,Dominic Wee,Banking.Live,,,,,,,,,,,,,,,,2025-05-14T00:01:56.283Z,2025-05-14T00:01:56.283Z,,,,,2025-05-14T00:47:23.512Z,,2025-05-14T00:47:23.512Z,,,,2025-05-14T00:47:23.512Z,2727,2025-05-14T00:47:47.237Z,,public,121.1,121.1,0,0,14may-p4-141-d360-missing-visain-files-for-13-may,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08S2RRNTR9,Reported by Dominic Wee,false
142,Partial Outage of Transaction Monitoring Capability,Service Incident,Closed,P3 - Non-Critical,"On 14 May 2025 at 04:19 UTC, the Engineering team was alerted about a potential full outage in transaction processing due to multiple Crimson instances (crimson3–8) showing down alerts in monitoring tools. Initial investigation showed that transaction processing appeared halted, and alerts were triggered by missing data in monitoring systems. Further analysis revealed that a timezone discrepancy in the database servers had shifted system timestamps by two hours into the future, causing monitoring tools (DataDog/SumoLogic) to misinterpret data availability.
The database timezone was found to have changed from SAST to UTC, affecting system timestamps. The database host was time-synced and confirmed by 06:45 UTC. Monitoring systems began reflecting correct data, confirming that transaction processing had been running correctly throughout the incident. The issue was resolved by syncing server times and validating system stability.
The Database, Infrastructure, and Application Support were engaged in a detailed investigation. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Ardiansyah Ar,VoucherEngine,"Agoda, Phillip Bank, Vodafone Fiji, Melkar",,Crimson nodes are down impacting transaction processing,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_YjliMTI5M2MtY2Y1ZC00Yzc2LWJhOWEtOWRiOTg1ODdkZDVj%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%225bc8ffce-74fa-4e51-af76-275e9877517e%22%7d,Transaction Processing,,,,,,,,,,2025-05-14T04:52:09.933Z,2025-05-14T04:52:09.933Z,,,,,2025-05-14T08:28:51.152Z,,2025-05-14T08:28:51.152Z,,2025-07-15T06:54:53.615Z,,2025-07-15T06:54:53.615Z,13001,2025-07-15T06:54:53.717Z,,public,1180.6,1056.4,53.8,70.4,14may-p3-non-critical-142-partial-outage-of-transaction-monitoring-capabili,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08SXQK6YLQ,Reported by Vicnesh Baskar,true
143,Partial Outage of ATM Transaction Processing - Flexpay,Service Incident,Post-Incident Tasks & Documentation,P1,"On May 17th at 09:00 UTC, we received cardholder reports regarding ATM transactions not working.

Our Dev team noticed reduced traffic for Acorn/Flexpay ATM transactions.
Application and network checks were performed, not returning any error from PYTG side.

Mastercard confirmed an infrastructure problem on their side, which prevented ATM transactions from all acquirers but one from going through.

Additional root cause information and preventive measures have been requested from Mastercard.",Daniel Velado,Daniel Velado,,,,Acorn / FlexPay,,,,Third-Party - Client,https://teams.microsoft.com/l/meetup-join/19%3ameeting_MjRjMGM2MmEtZTkwZS00NWRmLWI0NGQtYmUwOTUyYjYyMzQz%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%2277fd2081-e5b3-4187-a436-a13e4cc70a0f%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8745844980/2025-05-17+-+P1+-+Partial+Outage+of+ATM+Transaction+Processing+-+Flexpay,2025-05-17T09:56:24.723Z,2025-05-17T09:56:24.723Z,,,,,2025-05-17T16:30:49.916Z,,2025-05-17T16:30:49.916Z,,,,,23665,2026-02-02T15:17:30.285Z,,public,476.5,307.1,34.8,134.6,17may-p1-143-partial-outage-of-atm-transaction-processing-flexpay,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08T7CJ8XG9,Reported by Daniel Velado,false
144,Full Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 19 May 2025 at 03:42 UTC, Advanced Support team was alerted by a client about issues with transaction processing and API failures, including `sc_get_cvv2`, `sc_get_pan`, and `v2_sc_get_pin`. Initial investigation indicated the problem was due to the expiry of a PKS certificate, which affected transaction processing.
The PKS system's certificate validity was extended to the year 2030, and both PKS 01 and PKS 02 instances were restarted. By 04:34 UTC, successful API and transaction processing were confirmed, and normal service was restored. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Dominic Wee,Banking.Live,Zing,The issue was resolved by updating the certificate and restarting the affected components,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8740929585/2025-05-19+-+P1+-+Full+Outage+of+Transaction+Processing+1,2025-05-19T04:18:32.704Z,2025-05-19T04:18:32.704Z,,,,,2025-05-19T04:51:18.579Z,,2025-05-19T04:51:18.579Z,,2025-05-20T09:02:00.247Z,,,1965,2025-05-20T09:02:00.581Z,,public,342.9,278.6,20,44.3,19may-p1-144-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08SPBARLLT,Reported by Vicnesh Baskar,false
145,Partial Outage of Trasaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,"Currently investigating a high spike on 9120 for BIN 533892.

Error ratio: 33.36%",Daniel Velado,Daniel Velado,,,,VoucherEngine,Mukuru,,,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_NmUzMDBmY2UtMDA0MS00MzM2LTgwMTEtYzcxZWM0NWYzNjVm%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%2277fd2081-e5b3-4187-a436-a13e4cc70a0f%22%7d,,,,,,,,,,,2025-05-19T17:04:04.646Z,2025-05-19T17:04:04.646Z,,,,,2025-05-19T17:44:10.818Z,,2025-05-19T17:44:10.818Z,,,,,2406,2026-02-02T15:19:49.887Z,,public,160.3,145.8,14.5,0,19may-p1-145-partial-outage-of-trasaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08ST490FH9,Reported by Daniel Velado,false
146,Full Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 19 May 2025 at 21:17 UTC, the Paymentology team was alerted about a full outage of transaction processing due to a VPN tunnel disruption affecting connectivity with a client. Initial investigation indicated that the tunnel failed to reconnect after a disconnection, potentially due to an upstream ISP issue. Both Paymentology’s and the client’s network teams, along with their respective ISPs and Pulsant, were engaged to investigate the failure.
Prior to the incident, work had already been initiated to reroute traffic via an alternative Azure VPN tunnel. As both ISPs were unable to identify any fault within their networks, it was jointly decided during the incident to proceed with configuring the Azure VPN migration. However, while this migration was underway, the VPN connectivity on the original tunnel self-restored. Upon this restoration, the client opted to roll back any changes related to the Azure VPN and maintain the existing configuration. The Azure VPN migration will instead be completed under a planned change to ensure service stability.
Transaction processing resumed to normal following the restoration of connectivity at 01:25 UTC on 20 May 2025. Both internal and external teams were actively involved throughout the incident, and a detailed root cause analysis and follow-up actions will be documented in the RCAR.",Vicnesh Baskar,Daniel Velado,,,Jenifer Cachero,Banking.Live,Islandsbanki,Self Normalized,,Third-Party - Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZmE2MWFlZjItMDU0OC00MDA0LWJlNDktOTE0NzZlNWIzNmY5%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%2277fd2081-e5b3-4187-a436-a13e4cc70a0f%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8740864049/2025-05-19+-+P1+-+Full+Outage+of+Transaction+Processing,2025-05-19T22:18:23.938Z,2025-05-19T22:18:23.938Z,,,,,2025-05-20T02:00:35.399Z,,2025-05-20T00:56:26.844Z,2025-05-20T02:00:35.399Z,,,,13331,2026-02-02T15:20:58.298Z,,public,566.2,494.3,0,71.9,19may-p1-146-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08SWDL94KG,Reported by Daniel Velado,false
147,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P3 - Critical,"On 19 May 2025 at 22:39 UTC, the Advanced Support team was alerted to transaction timeouts from Mastercard. Initial investigation showed a delay in request processing between the PayRoute (PR) and PayKeyService (PKS) systems. The delay is suspected to be caused by a failover event triggered by Azure Maintenance around the same time.
Although the failover completed in a few seconds, subsequent analysis revealed that the PKS (located in the data centre) experienced delays receiving requests from PR for several transactions. This led to transaction timeouts and advisements being triggered.
Transaction processing normalised by 01:23 UTC with no further timeouts. The issue was resolved after the impact window passed and normal communication between PR and PKS resumed.
Internal infrastructure and engineering teams have been engaged to further investigate the communication delay between PR and PKS. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,,Banking.Live,MOX,Issue self normalized,Intermittern transaction timeouts,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8740601861/2025-05-20+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-05-20T05:49:41.349Z,2025-05-20T05:49:41.349Z,,,,,2025-05-20T05:50:25.313Z,2025-05-19T14:39:00.000Z,2025-05-20T05:50:25.313Z,,2025-05-20T06:32:41.456Z,,,43,2026-02-02T15:21:33.234Z,,public,430.8,327.8,31.6,71.4,20may-p3-critical-147-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08TH2X6WEM,Reported by Vicnesh Baskar,false
148,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On May 19th, PYTG Monitoring team received an alert regarding application latency.


Advanced Support and the Database team confirmed an issue on the database side, partially affecting transaction processing, and declining those with error code 9111.
A database high-resource-consumption task was stopped, resulting in transactions to processed again. 

An additional occurrence of this same behaviour was spotted and addressed on May 20th as well

Occurrences:
Start: 2025-05-19 15:13 - End: 2025-05-19 15:20 UTC
Start: 2025-05-20 18:30 - End: 2025-05-20 18:42 UTC 


",Daniel Velado,Daniel Velado,,,,VoucherEngine,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8745877786/2025-05-20+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-05-20T22:19:17.505Z,2025-05-20T22:19:17.505Z,,,,,,,,,,,,,2026-02-02T15:23:27.958Z,,public,400.6,270.7,32,97.7,20may-p1-148-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08TYSRG5LY,Reported by Daniel Velado,false
149,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"On 20th of May at 23:08 UTC, PYTG Support team received a notification regarding an increase in advice messages between 18:40 and 19:00.

Advanced support confirmed that transaction processing was partially affected between 18:10 and 18:23 following MOX BL Release 2.25.8-hf.3

An internal ticket has been created for further root cause investigation.",Daniel Velado,Daniel Velado,,,,Banking.Live,MOX,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8745877846/2025-05-20+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-05-20T23:16:26.227Z,2025-05-20T23:16:26.227Z,,,,,,,,,,,,,2026-02-02T15:24:02.903Z,,public,244.3,107.8,91.2,45.3,21may-p2-149-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08SW38219V,Reported by Daniel Velado,false
150,Partial Outage of API Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue:** There was a partial outage of API processing, starting at approximately 13:50 UTC, causing API calls to receive 404 system errors and fail. Impact stopped approximately at 17:30 UTC.

**Impact:** Customers experienced a partial outage of API processing.

**Causes:** API-05 Server disconnection.

**Steps to resolve:** A conference bridge was established with the respective teams to investigate the issue, a banner was uploaded to inform clients, and API-05 server was restarted to clear out the current errors.",Daniel Velado,Daniel Velado,,,David Aragon,VoucherEngine,,,,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_OTI1M2MyNzAtZjZmNy00NWUzLWE5NzktOWY2MzM3YzAzNTU0%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%2277fd2081-e5b3-4187-a436-a13e4cc70a0f%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8746106896/2025-05-21+-+P1+-+Partial+Outage+of+API+Processing,2025-05-21T16:57:46.314Z,2025-05-21T16:57:46.314Z,,,,,2025-05-21T17:57:05.147Z,,2025-05-21T17:57:05.147Z,,,,,3558,2025-05-22T05:04:28.927Z,,public,213.6,183.6,20,10,21may-p1-150-partial-outage-of-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08TG6SB4NN,Reported by Daniel Velado,false
151,Partial Outage of API Processing - FlexPay,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue:** There was a partial outage of API processing for FlexPay, which began around 16:30 UTC on 21 May 2025 and was resolved by 21:18 UTC.

**Impact:** This resulted in a partial outage of API processing.

**Causes:** TBD

**Steps to resolve:** A conference bridge was established to investigate the issue, on-site development teams were able to bring the database back up, and eventually confirmed that POS and ATM transactions, along with other services, were restored.",Daniel Velado,Daniel Velado,,,,Acorn / FlexPay,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8748597298/2025-05-21+-+P1+-+Partial+Outage+of+API+Processing+-+FlexPay,2025-05-21T18:29:27.536Z,2025-05-21T18:29:27.536Z,,,,,2025-05-21T21:27:24.328Z,,2025-05-21T21:27:24.328Z,,,,,10676,2025-05-22T22:10:36.679Z,,public,115.3,105.3,0,10,21may-p1-151-partial-outage-of-api-processing-flexpay,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08TTGLD7Q9,Reported by Daniel Velado,false
152,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P2,"On 23 May 2025 at 01:29 UTC, the Advanced Support team was alerted to a disconnection on a VISA Payroute instance, and around the same time, an increase in reversals from Mastercard.  Further investigation revealed 2 PR instances connecting to VISA and a PKS instance on the same data centre experienced a temporary network disconnection, causing transactions to be processed in stand-in by the schemes. A network issue in the connection to the data centre, likely on the VPN tunnel, is suspected to have caused the disconnection. By 01:37 UTC, all connections were back to normal, and transaction processing resumed without further issues.  Close monitoring post-resumption did not reveal further issues. Further investigation into the root cause is ongoing and will be shared in the Root Cause Analysis Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Dominic Wee,Banking.Live,"MOX, Bank Al Etihad",,Some transactions are processing in STIP,Third-Party - Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_MzAwMjA4YzQtMTI0Yi00NjQwLTkyNWEtNmZiZDRhY2Q2NGUy%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%225bc8ffce-74fa-4e51-af76-275e9877517e%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8749252614/2025-05-23+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-05-23T02:00:39.043Z,2025-05-23T02:00:39.043Z,,,,,2025-05-23T03:27:20.927Z,2025-05-23T01:42:00.000Z,2025-05-23T03:27:20.927Z,,2025-05-23T04:57:37.497Z,,,5201,2026-02-02T15:25:36.956Z,,public,483.4,431,22.4,30,23may-p2-152-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08TGG9NMHC,Reported by Vicnesh Baskar,false
154,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"On 25 May 2025 at 13:21 UTC, the technical team was alerted about a partial outage impacting transaction processing. Initial investigation showed that intermittent ATM/POS transaction timeouts were occurring due to insufficient hardware capacity, as the system was still operating in disaster recovery mode following a shutdown on 21 May. The system was placed into emergency maintenance from 21:00–22:00 UTC to allocate additional memory and improve performance. Transaction processing normalised after 21:56 UTC with no further issues. The issue was resolved by restoring hardware capacity via scheduled maintenance. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Tshegofatso Manakana,Acorn / FlexPay,,Restoring hardware capacity via scheduled maintenance,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8753283074/2025-05-25+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-05-25T18:15:49.528Z,2025-05-25T18:15:49.528Z,,,,,2025-05-25T22:28:11.932Z,2025-05-24T18:11:00.000Z,2025-05-25T18:40:59.040Z,2025-05-25T22:28:11.932Z,,,,15142,2025-05-26T06:15:11.190Z,,public,181.6,73.3,11.8,96.5,25may-p2-154-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08TJ5ENXRV,Reported by Vicnesh Baskar,false
155,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 26 May 2025 at 01:06 UTC, the Advanced Support team was alerted to the disconnection of several CRIMSON nodes (PANL, FRMN, and GIMI) from the GIM-UEMOA (local switching provider), impacting transaction processing via that route. The disconnection originated from the GIM-UEMOA side. The nodes were restored at 02:54 UTC with successful echo tests; however, transaction traffic did not resume immediately.

The switching team manually restarted the affected nodes between 13:33 and 13:38 UTC. Connectivity was confirmed as healthy, but transactions did not flow until GIM acknowledged the issue at 19:00 UTC and initiated their investigation. Transaction processing resumed at 06:09 UTC for PANL and 08:32 UTC for FRMN on 26 May. GIM were requested to provide a root cause, and further updates will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Ardiansyah Ar,VoucherEngine,"Panelys, GIM Uemoa",Automatically resolved after reconnectuon with GIM,Unable to receive any transactions from local switching provider (GIM UEMOA),Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8756002903/2025-05-26+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-05-26T01:43:59.991Z,2025-05-26T01:43:59.991Z,,,,,2025-05-27T00:39:31.984Z,2025-05-26T01:18:00.000Z,2025-05-26T04:09:29.648Z,,2025-05-26T05:45:09.465Z,,,82531,2026-02-02T15:26:22.200Z,,public,440.4,383.1,0,57.4,26may-p1-155-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08TZLASFJN,Reported by Vicnesh Baskar,false
156,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"On 27 May 2025 at 04:13 UTC, the Advanced Support team was informed of increased latency and transaction declines via multiple alerts. Initial investigation showed a spike in 9111 decline codes across several instances of the Crimson service. The system was observed to recover by 04:28 UTC, and transaction processing normalised with no further anomalies.

The issue was resolved without any manual intervention. Further investigation is ongoing, and updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Ardiansyah Ar,VoucherEngine,,Automatically resolved,Some transactions were declined with error 9111,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8758722681/2025-05-27+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-05-27T05:06:58.848Z,2025-05-27T05:06:58.848Z,,,,,2025-05-27T05:13:21.475Z,2025-05-27T05:06:00.000Z,2025-05-27T05:13:21.475Z,,,,,382,2026-02-02T15:27:04.763Z,,public,404.5,369,0,35.4,27may-p3-critical-156-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08TW5JFU2F,Reported by Vicnesh Baskar,false
158,Partial API Processing Issue,Service Incident,Post-Incident Tasks & Documentation,P2,"At 08:57, WIO reported various API failures due to timeout on the PayAPI service. The AS and L1 team reported issues with login, service unavailability, and multiple 502 errors. Following a restart of the PayAPI-02 server around 09:37 UTC everything was resolved. The team are still working to identify the cause, but host 10.66.2.21 has been identified as the source of multiple 502 errors. Network team confirmed that no network issues were observed during the timeframe.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,Wio,Restarted the payapi-02 server. API errors stopped post-restart.,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8756461804/2025-05-27+-+P2+-+Partial+API+Processing+Issue,2025-05-27T11:35:36.186Z,2025-05-27T06:57:00.000Z,,,,,2025-05-27T07:38:00.000Z,,,,,,,2460,2026-02-02T16:03:38.127Z,,public,140,119.9,20.1,0,27may-p2-158-partial-api-processing-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08UH6ABHGR,Reported by Snothile Dlamini,false
157,Partial Outage of Transaction and API Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"On 27 May 2025 at 06:32 UTC, the technical team was alerted via monitoring alerts to a connectivity issue between PayRoute and downstream systems. Initial investigation showed a loss of connection on PayRoute, with transactions failing and both FAST ACTIVE and PASSIVE paths receiving no responses. HTTP 499 errors were observed, indicating an issue with Azure connectivity.
Transaction processing resumed by 06:48 UTC without intervention. Network teams suspect the timeline is aligned with a maintenance event on the Azure platform; further investigation with Azure support is ongoing.
The issue was resolved by 06:48 UTC with services returning to normal. Close monitoring remains in place, and the root cause and preventive recommendations will follow in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Suganya Eswaran,Banking.Live,Wio,"Automatically resolved, no manual intervention",Partial outage of transaction processing and API calls,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8756625646/2025-05-27+-+P3+-+Critical+-+Partial+Outage+of+Transaction+and+API+Processing,2025-05-27T07:19:06.327Z,2025-05-27T07:19:06.327Z,,,,,2025-05-27T08:01:04.713Z,2025-05-27T06:32:00.000Z,2025-05-27T08:01:04.713Z,,,,,2518,2026-02-02T16:04:24.668Z,,public,264,254,10,0,27may-p3-critical-157-partial-outage-of-transaction-and-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08U6N3AH7E,Reported by Vicnesh Baskar,false
159,Partial Outage of Transacation Processing,Service Incident,Root Cause Analysis and Corrective Actions,P3 - Critical,"On 28 May 2025 at 01:41 UTC, the Technical Team was alerted about a disruption in transaction processing via the GIM-UEMOA route. Initial investigation showed that multiple CRIMSON nodes (PANL, FRMN, and GIMI) were disconnected from the GIM-UEMOA host, impacting transaction routing through that channel. The disconnection was confirmed to originate from the local switching provider, GIM-UEMOA. The system was reconnected at 02:38 UTC, and transaction processing normalised after 03:57 UTC. GIM-UEMOA have been engaged for a detailed investigation. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Heru Nugroho,,Ardiansyah Ar,VoucherEngine,,,Partial Outage of Transaction Processing,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8758853740/2025-05-28+-+P3+-+Critical+-+Partial+Outage+of+Transacation+Processing,2025-05-28T02:11:28.969Z,2025-05-28T02:11:28.969Z,,,,,2025-05-28T05:58:38.570Z,,2025-05-28T03:22:56.345Z,,2025-05-28T08:21:55.735Z,,,13629,2026-02-02T16:04:51.994Z,,public,271.5,271.5,0,0,28may-p3-critical-159-partial-outage-of-transacation-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08UA6PR0GJ,Reported by Vicnesh Baskar,false
160,Partial Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,FAST and API calls impacted due to an ISP issue (Pulsant),Daniel Velado,Daniel Velado,,,,Banking.Live,,,,Third-Party - Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_MmMxZjc4YWQtZTkyNi00MDQ4LWJiNGUtZDNhMGVkMDA4YWNh%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%2277fd2081-e5b3-4187-a436-a13e4cc70a0f%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8763343119/2025-05-29+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-05-29T19:38:50.419Z,2025-05-29T19:38:50.419Z,,,,,2025-05-29T19:58:46.464Z,,2025-05-29T19:48:39.038Z,,,,,1196,2026-02-02T16:05:13.735Z,,public,1016.5,852.9,117.2,46.4,29may-p1-160-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08UDDRS0RZ,Reported by Daniel Velado,false
161,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 29 May 2025 at 22:42 UTC, the monitoring team observed an issue with Seacom Link impacting transaction processing. Initial investigation showed that the link failed due to a problem on the service provider's end, and traffic had already been rerouted via a secondary link. Transaction processing normalised after 22:47 UTC, with no further timeouts",Vicnesh Baskar,Vicnesh Baskar,Maureen Figueroa,,Ardiansyah Ar,VoucherEngine,"DolarApp, Jeeves",Network link failover,Partial Outage of Transaction Processing,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8766685196/2025-05-30+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-05-30T00:54:30.056Z,2025-05-30T00:54:30.056Z,,,,,2025-05-30T01:00:27.846Z,,2025-05-30T01:00:27.846Z,,,,,357,2025-06-02T07:57:02.221Z,,public,424.4,412.8,0,11.6,30may-p1-161-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08UPNK13K6,Reported by Vicnesh Baskar,false
162,Full outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"Azure had an unplanned issue on the VPN host between 17:36:29 UTC and 17:37:44 UTC.
We were sending out traffic to ADQ but not receiving response from their side until 17:53 UTC.",Daniel Velado,Daniel Velado,,,,Banking.Live,Wio,,,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8771272952/2025-05-30+-+P1+-+Full+outage+of+Transaction+Processing,2025-05-30T18:02:00.744Z,2025-05-30T18:02:00.744Z,,,,,,,,,,,,,2026-02-02T16:07:41.936Z,,public,632,495.9,66,70.1,30may-p1-162-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08UKUX4F3Q,Reported by Daniel Velado,false
163,Full Outage of Transaction Processing - Acorn/FlexPay,Service Incident,Monitoring,P1,"Please note that the Acorn system is currently down. All ATM, POS and customer interfaces are affected. 
Team is currently looking into the issue and we will update on any progress made.",Daniel Velado,Daniel Velado,,,,Acorn / FlexPay,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8786575410/2025-05-30+-+P1+-+Full+Outage+of+Transaction+Processing+-+Acorn+FlexPay,2025-05-30T18:13:06.123Z,2025-05-30T18:13:06.123Z,,,,,,,2025-05-31T05:06:43.538Z,,,,,,2026-02-02T16:08:17.804Z,,public,2743.6,1645.7,503.7,594.1,30may-p1-163-full-outage-of-transaction-processing-acornflexpay,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08UU5BD4N8,Reported by Daniel Velado,false
164,Full Outageof API Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue:** There is a full outage of API Processing in the Voucher Engine starting at 20:00 UTC.

**Impact:** The impact is across all clients.

**Causes:** The outage was caused by database latency.

**Steps to resolve:** A conference bridge was established with the respective teams, and the API functionality has been restored.",Daniel Velado,Daniel Velado,,,,VoucherEngine,,,,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_OTMyZGIwYWEtOTMzYi00N2IzLTg5NDItZDdkZTNlY2JjZGQ4%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%2277fd2081-e5b3-4187-a436-a13e4cc70a0f%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8767143941/2025-05-30+-+P1+-+Full+Outageof+API+Processing,2025-05-30T20:29:04.939Z,2025-05-30T20:29:04.939Z,,,,,2025-05-30T20:41:26.354Z,,2025-05-30T20:41:26.354Z,,,,,741,2026-02-02T16:08:41.237Z,,public,116.7,94,22.7,0,30may-p1-164-full-outageof-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08V4L2HXUH,Reported by Daniel Velado,false
165,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"Rule Engine latency partially impacted transactions for ADQ, issue resolved by itself without PYTG intervention.
Additional root cause information will be shared soon.

Timeframe: 00:00:40 AM UTC - 00:02:50 AM UTC

MC: 38 transactions
Visa: 9 transactions",Daniel Velado,Daniel Velado,,,,Banking.Live,Wio,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8769700198/2025-06-01+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-06-01T01:30:53.531Z,2025-06-01T01:30:53.531Z,,,,,,,,,,,,,2026-02-02T16:10:31.699Z,,public,58.1,22,26.1,10,01jun-p1-165-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08USLT4K9B,Reported by Daniel Velado,false
166,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On June 1st, PYTG received a notification regarding an increase in reversals and timeouts for Old Mutual. Advanced Support confirmed that one PR instance wasn't processing any traffic since May 31st, 07:58 UTC. The PR instance was restarted, and transactions started to arrive. 

Issue start: 31-May-2025 08:11
Issue resolved: 01 May 2025 13:07
Impacted transactions count: 115
Successfully processed transactions: 83
",Daniel Velado,Daniel Velado,,,,Banking.Live,Old Mutual Bank / Olympus,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8767078403/2025-06-01+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-06-01T22:27:06.381Z,2025-06-01T22:27:06.381Z,,,,,,,,,,,,,2026-02-02T16:11:21.908Z,,public,46.2,46.2,0,0,01jun-p1-166-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08UY38DTAN,Reported by Daniel Velado,false
167,Partial Transaction Processing Outage,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"Mukuru cardholders reported that ATM withdrawals and POS cashback transactions were marked as successful but no cash was dispensed. Funds were debited from accounts.

A total of 251 confirmed transactions were affected across BIN **533892**, mainly at SBSA ATMs. 

Mastercard confirmed that the issue was due to a processing problem involving duplicate transaction records; one approved, one declined.

Mastercard has completed transaction validation and is preparing to execute reversals and adjustments. We're awaiting confirmation that they will process these reversals.",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,Mukuru,,ATM and POS Transactions ,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8774385715/2025-06-02+-+P3+-+Critical+-+Partial+Transaction+Processing+Outage,2025-06-02T13:49:34.366Z,2025-06-02T13:49:34.366Z,,,,,2025-06-04T14:42:33.782Z,2025-05-31T08:40:00.000Z,2025-06-02T13:49:50.574Z,2025-06-04T14:42:33.782Z,,,,175979,2026-02-02T16:13:29.816Z,,public,1590.2,1432.3,93.6,64.3,02jun-p3-critical-167-partial-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08VCBQN5R7,Reported by Snothile Dlamini,false
168,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On June 2nd at 15:02 UTC PYTG Monitoring team received alerts regarding FAST timeouts and proceeded to engage the Advanced Support team.

Upon further investigation, the team confirmed a partial impact on MADA and Visa transactions.

A troubleshooting call was started, and logs confirmed spikes on the database’s CPU utilisation; restarting the applications did not have any effect.
The team increased the CPU cores from 4 to 8, restarted the database service, and implemented additional measures to maintain CPU health, which has helped in mitigating the transaction impact, and no further errors were observed as of 18:51 UTC.",Daniel Velado,Daniel Velado,,,,Banking.Live,D360,,,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_N2M1YjIxOTktY2M4NS00OTgwLWJiYzE[…]2c%22Oid%22%3a%22a1233178-e123-44d4-9e33-8d2d3cdff58d%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8769044506/2025-06-02+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-06-02T15:51:01.189Z,2025-06-02T15:51:01.189Z,,,,,2025-06-02T20:03:05.094Z,,2025-06-02T20:03:05.094Z,,,,,15123,2026-02-02T16:14:02.423Z,,public,2059.1,1737.3,263.2,58.5,02jun-p1-168-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08V25ALAJE,Reported by Daniel Velado,false
169,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On June 4, 2025, at 00:00 UTC, the support team observed a brief, partial outage of transaction processing between 00:00 UTC and 00:02 UTC, causing transactions to be processed in STIP. Processing normalised thereafter without any manual intervention.
Initial investigation revealed delays in the Rule Engine response, and the relevant teams have been engaged for a root cause analysis. Further updates will be shared in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Sonia Seh,,Dominic Wee,Banking.Live,Wio,No manual actions takes. issue auto resolved,"Brief, partial outage of transaction processing",Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8772124683/2025-06-04+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-06-04T01:16:22.838Z,2025-06-04T01:16:22.838Z,,,,,2025-06-04T01:23:23.094Z,2025-06-04T00:00:00.000Z,2025-06-04T01:23:23.094Z,,2025-06-04T01:40:02.247Z,,,420,2025-06-04T01:40:02.684Z,,public,72,61.9,0,10,04jun-p1-169-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C08V3PHS6KG,Reported by Vicnesh Baskar,false
170,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue:** There was a partial outage of transaction processing impacting VISA transaction processing and API processing.

**Impact:** This outage partially impacted transactions and API

**Causes:** The outage was triggered by issues with the Paymentology API, including timeout issues on multiple endpoints.

**Steps to resolve:** We restarted the MC PP and PR 1, increased paylog db resources, restarted the PayKey service, and subsequently, API services were restored. All impacted services have been restored as of 18:45 UTC.",Daniel Velado,Daniel Velado,,,,Banking.Live,Wio,,,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_NGZkYzNmMmUtNzVkZi00ZWQxLThlOTUtNWE0NGFiOTBhZjQ1%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%2277fd2081-e5b3-4187-a436-a13e4cc70a0f%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8775073868/2025-06-04+-+P1+-+Partial+Outage+of+Transaction+Processing+1,2025-06-04T17:48:54.777Z,2025-06-04T17:48:54.777Z,,,,,2025-06-04T19:18:05.597Z,,2025-06-04T19:18:05.597Z,,,,,5350,2026-02-02T16:15:36.450Z,,public,582,495.1,64.4,22.5,04jun-p1-170-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0901F8F2KU,Reported by Daniel Velado,false
171,Full Outage on Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,"High CPU load on the ADQ Primary Paycore database server caused transaction failures, primarily impacting WIO. The issue was traced to resource exhaustion in the PayPower services interacting with the DB.",Snothile Dlamini,Snothile Dlamini,Zoja Mitric,,Abdul Sahaptheen,Banking.Live,Wio,"At 10:51 UTC, the AS team stopped all PayPower services and performed a restart at 10:56 UTC. This action successfully reduced CPU usage on the database server, bringing it back down to 20% utilization. Since then, transaction processing has stabilized, and no further issues have been observed. CPU utilization on Paycore DB01 remains within normal range (20–30%).",Transaction Processing ,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZWQyYWZhMzAtZmIyMi00MTRmLTk1MDItODc5YTc1NTVhMzg0%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%22067d56db-044f-4b5b-b44c-8303c87625ab%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8775991616/2025-06-05+-+P1+-+Full+Outage+on+Transaction+Processing,2025-06-05T10:56:36.521Z,2025-06-05T10:56:36.521Z,,,,,2025-06-05T11:50:17.293Z,2025-06-05T08:35:00.000Z,2025-06-05T11:50:17.293Z,,,,,3220,2026-02-02T16:15:51.488Z,,public,305.2,199.5,85.7,20,05jun-p1-171-full-outage-on-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C090GS27RFT,Reported by Snothile Dlamini,false
172,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 06 June 2025 at 00:02 UTC, the Support Team was alerted to a partial outage of transaction processing due to transaction timeouts. Processing recovered automatically within minutes. Additional short impacts occurred intermittently until 03:21 UTC. Active monitoring did not show any timeouts thereafter. Active root cause investigation is ongoing, and further updates will be provided in the Incident Report (IR).",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,Wio,Automatically resolved,Partial outage of transaction processing,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8781234329/2025-06-06+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-06-06T01:33:50.626Z,2025-06-06T01:33:50.626Z,,,,,2025-06-06T07:29:14.175Z,,2025-06-06T01:38:16.104Z,,2025-06-06T05:19:02.104Z,,,21323,2025-06-09T06:34:45.390Z,,public,696.3,650,20,26.3,06jun-p1-172-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C090YRN7HRN,Reported by Vicnesh Baskar,false
173,Transaction Processing Outage,Service Incident,Post-Incident Tasks & Documentation,P1,"The incident was caused by a missing SSL certificate in the Java 12 environment used by the VTP (Voucher Transaction Processor), which is responsible for handling companion wallet transactions. While the certificate was present in the ColdFusion environment, it had not been added to Java 12, resulting in repeated `-7` timeout responses from the RCS wallet and automatic throttling of all RCS campaigns. The certificate was added to the correct keystore, and VTP was restarted across all nodes in the cluster. Transaction processing has since resumed successfully, and the issue is now resolved.

Further details will be provided in the RCAR.",Snothile Dlamini,Snothile Dlamini,Megalatha Ramakrishnan,,Claudiu Stoica,VoucherEngine,RCS Group,,Transaction Processing,Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8780382280/2025-06-07+-+P1+-+Transaction+Processing+Outage,2025-06-07T09:52:55.768Z,2025-06-07T09:52:55.768Z,,,,,2025-06-07T11:18:35.583Z,2025-06-06T06:54:00.000Z,2025-06-07T10:50:46.233Z,,,,,5139,2026-02-02T16:16:27.616Z,,public,496.9,496.9,0,0,07jun-p1-173-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C090D75KSDB,Reported by Snothile Dlamini,false
174,Partial Transaction Processing Outage,Service Incident,Post-Incident Tasks & Documentation,P2,"Between 01:33 and 01:45 UTC on June 8, 2025, the ISB Paypower production environment experienced elevated database latency, causing advisement timeouts and impacting 138 Mastercard transactions. The rule engine and `handleauth` process also showed delays, suggesting a potential underlying DB issue. Disk latency was observed on the Paycore DB during this time.

A Jira has been raised to the Database Team for root cause investigation. Incident has been resolved with no ongoing impact.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8780349698/2025-06-08+-+P2+-+Partial+Transaction+Processing+Outage,2025-06-08T03:20:37.140Z,2025-06-08T00:57:00.000Z,,,,,2025-06-07T23:46:00.000Z,2025-06-07T23:33:00.000Z,,,,,,,2026-02-02T16:16:53.063Z,,public,327.7,271.9,0,55.8,08jun-p2-174-partial-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C090EG15K0V,Reported by Snothile Dlamini,false
175,ViaCarteSettlementProcessingIssue,Service Incident,Closed,P3 - Non-Critical,"We received ticket from client about unsettled transaction and Settlement report was not being received. Upon further investigation, have found that the EP files were received but encountered error when processed in the EP Utility. EP files starting from March 16, they're all encountering error till today and per checking no PD alert were triggered for the error in processing the EP files of ViaCarte.",Jenifer Cachero,Jenifer Cachero,,,,Banking.Live,ViaCarte,,,Paymentology,,,,,,,,,,,,2025-06-09T04:05:37.105Z,2025-06-09T04:05:37.105Z,,,,,2025-07-15T06:53:27.855Z,2025-03-16T06:00:00.000Z,2025-07-15T06:53:27.855Z,,,,2025-07-15T06:53:27.855Z,3120470,2026-02-02T16:17:40.371Z,,public,335.3,252.7,0,82.5,09jun-p3-non-critical-175-viacartesettlementprocessingissue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C090KQVJ57W,Reported by Jenifer Cachero,false
176,Partial Outage of API Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"API methods intermittently failing on payapi-02 / host:pt-adq-payapi03.paymentology.org

Affected calls:
_/api/pws/pws_tok_flag_edit
/api/sc/v2/sc_get_pan
/api/sc/v2/sc_get_cvv2

_Start:  13:51:12
End:    16:01:51",Daniel Velado,Daniel Velado,,,,Banking.Live,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8786870403/2025-06-11+-+P3+-+Critical+-+Partial+Outage+of+API+Processing,2025-06-11T03:36:46.640Z,2025-06-11T03:36:46.640Z,,,,,2025-06-10T22:01:00.000Z,2025-06-10T19:51:00.000Z,,,,,,,2026-02-02T16:19:50.333Z,,public,189,136.4,52.6,0,11jun-p3-critical-176-partial-outage-of-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C091L8BV064,Reported by Daniel Velado,false
177,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue:** There was a partial outage of transaction processing due to a spike in 9107 errors on the Voucher Engine. 
**Impact:** Partial outage of transaction processing.
**Causes:** TBD
**Steps to resolve:** A conference bridge was established with the respective teams to investigate the issue. The PYTG Dev team has restarted the five TRN servers, and transactions started to arrive.

**Start:** 16:20 UTC
**End:** 16:55 UTC",Daniel Velado,Daniel Velado,,,,VoucherEngine,,,,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_NGRlNmVmOTItMWJjZS00MGQyLWI3NWItNWVmMTk1OWFkNjJi%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%2277fd2081-e5b3-4187-a436-a13e4cc70a0f%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8790081565/2025-06-11+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-06-11T18:37:27.201Z,2025-06-11T18:37:27.201Z,,,,,2025-06-11T18:57:09.397Z,,2025-06-11T18:57:09.397Z,,,,,1182,2026-02-02T16:20:25.175Z,,public,467.2,441.4,25.8,0,11jun-p1-177-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09125A1VSQ,Reported by Daniel Velado,false
178,Partial Outage of create_card API,Service Incident,Post-Incident Tasks & Documentation,P2,"Tyme experienced intermittent error messages when calling the API for virtual card creation in the PROD environment, specifically receiving 'Error executing Request. Our technical teams have implemented a fix to address the intermittent failures affecting the `create_card` API. Since the update was applied at 11:10 UTC on 13 June 2025, there have been no further failures observed.  Post-Fix Monitoring completed",Vicnesh Baskar,Vicnesh Baskar,Huy Pham,,Dominic Wee,Banking.Live,TymeBank SA,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8794931325/2025-06-13+-+P2+-+Partial+Outage+of+create_card+API,2025-06-13T03:31:21.799Z,2025-06-13T03:31:21.799Z,,,,,2025-06-13T12:44:05.673Z,,2025-06-13T12:44:05.673Z,,,,,33163,2026-02-02T16:20:40.728Z,,public,774.8,731,0,43.8,13jun-p2-178-partial-outage-of-create_card-api,https://slack.com/app_redirect?team=T03SMJWS8&channel=C091N44QZC1,Reported by Vicnesh Baskar,false
179,Partial Outage of API Processing,Service Incident,Post-Incident Tasks & Documentation,P2,Partial impact on API processing between 15:37 UTC and 15:58 UTC.,Daniel Velado,Daniel Velado,,,,Banking.Live,Wio,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8796176390/2025-06-13+-+P2+-+Partial+Outage+of+API+Processing,2025-06-13T16:21:03.099Z,2025-06-13T16:21:03.099Z,,,,,,,,,,,,,2026-02-02T16:21:01.996Z,,public,872.3,775.6,96.7,0,13jun-p2-179-partial-outage-of-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C091DUCFZE0,Reported by Daniel Velado,false
180,Full Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 15 June 2025 at 01:31 UTC, the monitoring team detected a full outage of transaction processing due to a disconnection from AFS. No issues were identified on Paymentology's side. The AFS team was contacted but has not responded yet. Connectivity to AFS was restored at 02:44 UTC, and transaction processing resumed at 02:45 UTC. Additional monitoring is in place to ensure no further issues. AFS will be requested to provide their analysis on the root cause of the disconnection.",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Dominic Wee,Banking.Live,Dopay LLC,Auto restored,Full outage of transaction processing,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8797127127/2025-06-15+-+P1+-+Full+Outage+of+Transaction+Processing,2025-06-15T02:46:23.794Z,2025-06-15T02:46:23.794Z,,,,,2025-06-15T03:13:29.650Z,,2025-06-15T03:13:29.650Z,,2025-06-16T02:57:42.122Z,,,1625,2026-02-02T16:21:45.106Z,,public,180.5,150.5,10,20,15jun-p1-180-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C091ADBC2P5,Reported by Vicnesh Baskar,false
181,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P2,"On 15 June 2025 at 03:24 UTC, a client reported a reversal spike and on further analysis, the support team observed latency in receiving transactions from Mastercard to the PayRoute system, impacting authorisation flow. 

The issue was auto-resolved, with transaction flow returning to normal by 03:25 UTC. Transaction IDs reviewed confirmed delayed receipt from Mastercard. A ticket has been raised with Mastercard for further analysis, and the network team has been engaged to investigate any latency on the communication path between Mastercard, MIP, and PayRoute.",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Dominic Wee,Banking.Live,"Islandsbanki, MOX",Auto-resolved,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8798011403/2025-06-15+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-06-15T04:57:10.218Z,2025-06-15T04:57:10.218Z,,,,,,,,,2025-06-16T05:03:25.365Z,,,,2026-02-02T16:22:36.820Z,,public,262.3,211.1,10,41.2,15jun-p2-181-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C091AHPU2PM,Reported by Vicnesh Baskar,false
182,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 15 June 2025 at 21:44 UTC, the monitoring team was alerted to multiple issues with transaction processing. Initial investigation suggested a brief disconnection on the Visa and Mastercard networks. The incident was later traced to an Azure maintenance event. During this maintenance, Azure's infrastructure automatically rerouted traffic through a secondary link, resulting in a short disruption. 

No manual intervention was required, and transaction processing returned to normal by 21:46 UTC. Azure has been contacted for further details, and updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,"MOX, Islandsbanki, Bank Al Etihad, ONMO, Rain, Tweeq, Wio",Auto resolved,Partial Outage of transaction processing and API,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8798273541/2025-06-15+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-06-15T23:01:22.123Z,2025-06-15T23:01:22.123Z,,,,,2025-06-15T23:15:50.630Z,,2025-06-15T23:15:50.630Z,,,,,868,2025-06-16T06:59:56.626Z,,public,804.5,628.9,10,165.6,16jun-p1-182-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C091C2QC2QN,Reported by Vicnesh Baskar,false
183,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P3 - Critical,"On 15 June 2025 at 21:06 UTC, the monitoring team was alerted to latency issues affecting transaction processing. Initial checks showed no issues in the database or application layers. Network and DB teams were engaged, and further investigation revealed that multiple scheduled tasks—backup, archival, and maintenance—were running simultaneously, causing increased IO wait on the database.
These tasks slowed down internal transaction processing, with some operations taking up to 3 seconds instead of milliseconds. Once the tasks were completed, database performance returned to normal by 23:30 UTC, and no further issues were observed.
The issue was resolved without manual intervention once the scheduled tasks finished.",Vicnesh Baskar,Vicnesh Baskar,Heru Nugroho,,Wilson Keneshiro,VoucherEngine,,Resolved without manual intervention,Partial outage of transaction processing on multiple windows,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8798208046/2025-06-15+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-06-15T23:33:02.023Z,2025-06-15T23:33:02.023Z,,,,,2025-06-16T00:25:40.079Z,,2025-06-16T00:25:40.079Z,,2025-06-16T08:50:09.556Z,,,3158,2025-06-16T08:50:09.947Z,,public,342.1,210.4,0,131.6,16jun-p3-critical-183-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C091HBS8UKF,Reported by Vicnesh Baskar,false
186,Partial Outage of Settlement Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue:** On 19 June 2025, duplicate Mastercard settlement files for several South African issuers were identified, causing duplicated transactions.

**Impact:** Some customers are experiencing negative balances on their cards due to the duplicated settlements.

**Causes:** The duplication was first noticed through the Chargeback Queue, indicating over 200,000 unexpected settlement entries.

**Steps to resolve:** Two main workstreams are underway: one to compile and reverse the duplicate settlements, and another to investigate the root cause.",Vicnesh Baskar,Alaa Wehbe,Jhonatan Roldan Tavera,,Karthy Rajagopalan,VoucherEngine,,,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8808857674/2025-06-19+-+P1+-+Partial+Outage+of+Settlement+Processing,2025-06-19T06:31:02.797Z,2025-06-19T06:31:02.797Z,,,,,2025-06-20T16:04:10.292Z,,2025-06-19T14:53:11.904Z,2025-06-20T03:34:12.328Z,,,,120787,2026-02-02T16:23:18.010Z,,public,4197.7,3514.7,268,414.9,19jun-p1-186-partial-outage-of-settlement-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09255JU8TU,Reported by Alaa Wehbe,false
187,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"• Start time: 2025-06-22 21:06:00 UTC

• End time: 2025-06-22 21:08:00 UTC

• 553 system errors across all campaigns with an error ratio of 27% during those 2 minutes

Multiple VE clients affected",Daniel Velado,Daniel Velado,,,,VoucherEngine,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8809677239/2025-06-22+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-06-22T21:35:08.664Z,2025-06-22T21:35:08.664Z,,,,,,,,,,,,,2026-02-02T16:23:45.579Z,,public,249,219.5,0,29.5,22jun-p1-187-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C092XCWQAJV,Reported by Daniel Velado,false
188,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 25 June 2025 at 00:30 UTC, the support team was alerted to transaction timeouts due to database latency. Initial investigation revealed that a backup attempt from the primary database to support a rebuild of the secondary database caused a spike in disk IO and latency, resulting in multiple transaction timeouts. The backup activity was stopped immediately at 00:52 UTC, which normalised transaction processing. Monitoring confirmed no further impact post-resolution.",Vicnesh Baskar,Vicnesh Baskar,Huy Pham,,Jenifer Cachero,Banking.Live,MOX,Backup activity was stopped,Partial Outage of transaction processing,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_NTQzZTNkZTItM2NlMS00ZTFlLWFmYjAtYjIzZmY3MGQzODJh%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%225bc8ffce-74fa-4e51-af76-275e9877517e%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8816197901/2025-06-25+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-06-25T01:18:01.172Z,2025-06-25T01:18:01.172Z,,,,,2025-06-25T01:44:44.741Z,,2025-06-25T01:44:44.741Z,,2025-06-25T08:14:51.671Z,,,1603,2025-06-25T08:14:52.041Z,,public,540.2,500,0,40.2,25jun-p1-188-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C092X998A0J,Reported by Vicnesh Baskar,false
189,Partial Outage of Transaction Processing and API,Service Incident,Root Cause Analysis and Corrective Actions,P3 - Critical,"On 25th June 2025, at 01:39 UTC, the support team observed multiple alerts on Crimson, indicating transaction and API timeouts. Issue was narrowed down to long-running queries on the database, and the database team was mobilised. The long-running queries were killed, and transaction processing normalised from 02:29 UTC onwards.",Vicnesh Baskar,Vicnesh Baskar,Maureen Figueroa,,Ardiansyah Ar,VoucherEngine,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8816591051/2025-06-25+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing+and+API,2025-06-25T02:30:02.791Z,2025-06-25T02:30:02.791Z,,,,,2025-06-25T02:43:53.891Z,,2025-06-25T02:43:53.891Z,,2025-06-25T08:12:12.803Z,,,831,2025-06-25T08:12:13.124Z,,public,242.5,207.2,25.2,10,25jun-p3-critical-189-partial-outage-of-transaction-processing-and-api,https://slack.com/app_redirect?team=T03SMJWS8&channel=C092XFQ3T6W,Reported by Vicnesh Baskar,false
190,Transaction Timeout,Service Incident,Post-Incident Tasks & Documentation,P2,"On 25 June 2025, a spike in transaction declines was observed on SH02 due to Rule Engine timeouts and processing errors. The issue started around 04:38 UTC. Errors included slow response times and SQL failures in the `proc_ppwr_ruleengine_v10` procedure. 

The cause was identified as the recent configuration updates (BL 2.27 version) that set the Rule Engine queries (ruleEngineQueryTimeout) for 1 second. This value was too low, which caused the queries to time out and experience intermittent transaction failures. The SD team now has a higher timeout value, and transaction processing has returned to normal. No further declines have been observed since the query timeout change was made.",Snothile Dlamini,Snothile Dlamini,Zoja Mitric,,Kamal Thapa,Banking.Live,"Gnosis, Nomo Bank, ONMO, Utility Warehouse, SaltPay via PTY, Rain",,,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_Yzg1OGY5MTItODc5OS00ZjY3LThhOTUtYTAzOWI1YWQyZTMx%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%22067d56db-044f-4b5b-b44c-8303c87625ab%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8816198030/2025-06-25+-+P2+-+Transaction+Timeout,2025-06-25T09:48:42.734Z,2025-06-25T09:48:42.734Z,,,,,2025-06-25T10:29:08.785Z,2025-06-25T04:38:00.000Z,2025-06-25T10:29:08.785Z,,,,,2426,2025-06-25T10:29:20.457Z,,public,478.7,453.4,25.3,0,25jun-p2-190-transaction-timeout,https://slack.com/app_redirect?team=T03SMJWS8&channel=C092XMB1TGD,Reported by Snothile Dlamini,false
191,Full Outage of UAE Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P4,"Investigating UAE transaction drop on ADQ
Start: 14:59
End: 15:43",Daniel Velado,Daniel Velado,,,,,,,,,,,,,,,,,,,,2025-06-25T15:56:47.046Z,2025-06-25T15:56:47.046Z,,,,,,,,,,,,,2025-06-25T23:12:47.826Z,,public,202,180.1,21.9,0,25jun-p4-191-full-outage-of-uae-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C092M8J9EUF,Reported by Daniel Velado,false
193,Partial Outage of API Processing,Service Incident,Closed,P1,"On 29 June 2025, at 19:08 UTC, Advanced Support received a monitoring alert indicating API instability. Shortly after, at 19:14 UTC, the API02 instance was identified as unresponsive and restarted. Despite this, the client reported API timeouts starting at 19:04 UTC and escalated the issue at 20:30 UTC.

Additional alerts confirmed ongoing problems, and although some successful calls were seen at 21:02 UTC, client issues persisted. At 21:56 UTC, it was identified that the API02 instance was decommissioned and replaced with API03. API02 was stopped, and the replacement API03 instance was restarted, which resolved the issue and normalised API processing.",Vicnesh Baskar,Vicnesh Baskar,,,,Banking.Live,Wio,Restart of API03 instance,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8824979488/2025-06-30+-+P1+-+Partial+Outage+of+API+Processing,2025-06-30T05:40:38.914Z,2025-06-30T05:40:38.914Z,,,,,,,,,2025-07-30T23:56:02.293Z,2025-07-30T23:56:20.919Z,2025-07-30T23:56:20.919Z,,2025-07-30T23:56:21.039Z,,public,356.7,336.7,0,20,30jun-p1-193-partial-outage-of-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C093E4T4BUK,Reported by Vicnesh Baskar,false
194,Partial Outage of JONET Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"On 02 July 2025 at 06:00 UTC, the Advanced Support team was alerted to intermittent outages affecting JONET transaction processing. Initial investigations showed missing database files, which are auto-generated by the PostgreSQL engine, causing PayPower to become intermittently unresponsive.
During troubleshooting, restarting the PayPower instance was used as a temporary workaround whenever drops in transaction volume were observed, but the issue recurred after these restarts. The database team performed a stop/start restart at 07:07 UTC, which stabilised transaction processing, with no further errors observed.
Close monitoring will remain in place, and further updates will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Rajesh Subramanian,Banking.Live,Bank of Jordan,Stop/Start DB server,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8832679937/2025-07-02+-+P2+-+Partial+Outage+of+JONET+Transaction+Processing,2025-07-02T06:05:12.606Z,2025-07-02T06:05:12.606Z,,,,,2025-07-02T07:53:39.814Z,,2025-07-02T07:53:39.814Z,,,,,6507,2025-07-03T01:18:04.677Z,,public,554.2,554.2,0,0,02jul-p2-194-partial-outage-of-jonet-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0941CBUYUU,Reported by Vicnesh Baskar,false
195,Settlement Processing Outage,Service Incident,Closed,P3 - Critical,"The Mastercard `SettlementFastlite` job for WIO failed to complete successfully since June 1st due to application design limitations and infrastructure constraints. The job was sending HTTP requests to Mambu sequentially at a rate of about 1 request per second. Occasionally, Mambu would exhibit spikes in latency causing a backlog of over 700K settlement transactions due to the job's request rate being delayed several seconds.

No monitoring alerts were initially triggered because the job didn’t exit with errors but remained running indefinitely. Investigation confirmed that Payscheduler was under-provisioned 

To address the issue, the following were implemented:

• increased Payscheduler resources (CPU/RAM).

• deployed code change to support fanout processing (bulk sends in concurrent threads).

• coordinated with Mambu to validate throughput capacity.

• tested new fanout logic in UAT with 3 concurrent threads showing very positive results in processing time.

• production deployment and final backlog clearance is pending validation.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,Wio,,Mastercard settlements,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8852177054/2025-07-02+-+P3+-+Critical+-+Settlement+Processing+Outage,2025-07-02T15:07:34.009Z,2025-07-02T15:07:34.009Z,,,,,2026-02-02T16:27:18.070Z,2025-06-26T14:57:00.000Z,2025-07-03T04:57:12.620Z,2026-02-02T16:27:18.070Z,2026-02-02T16:27:51.569Z,,2026-02-02T16:27:51.569Z,18580784,2026-02-02T16:28:27.159Z,,public,1121.8,774.7,229.8,117.4,02jul-p3-critical-195-settlement-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C093XPWNLJJ,Reported by Snothile Dlamini,true
196,Partial ATM Non-Disbursement Outage,Service Incident,Post-Incident Tasks & Documentation,P1,"The first incident, which occurred on 3 July, was due to a switching issue within Mastercard’s South Africa transaction processing service. This impacted the delivery of ATM reversal messages; some were malformed and failed validation, while others may not have been sent at all. Mastercard confirmed the issue under Case INC000000490112 and all confirmed reversals have been processed.

The second incident, on 4 July, was caused by a Vodacom-related network outage that affected SBSA’s newly deployed ATMs. SBSA confirmed that 133 cardholders were impacted, totaling R210,450.00 in withheld funds. Paymentology processed all the required reversals and refunded any related ATM fees. The formal RCA from SBSA is still pending.",Snothile Dlamini,Snothile Dlamini,Megalatha Ramakrishnan,,Karthy Rajagopalan,VoucherEngine,Mukuru,,,Third-Party - Client,https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZjMwNTdhNjYtZmUyZS00MjNjLWExZTUtZDYzMjZjMDI4MDAw%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%225bc8ffce-74fa-4e51-af76-275e9877517e%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8847294495/2025-07-04+-+P1+-+Partial+ATM+Non-Disbursement+Outage,2025-07-04T08:03:54.207Z,2025-07-04T08:03:54.207Z,,,,,2025-07-10T23:36:32.393Z,2025-07-04T07:57:00.000Z,2025-07-10T23:36:32.393Z,,,,,574358,2026-02-02T16:29:34.250Z,,public,5713.6,5306.8,316.1,90.5,04jul-p1-196-partial-atm-non-disbursement-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C094B8RUXDK,Reported by Snothile Dlamini,false
197,Partial Outage of Transaction,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"WIO experienced 2 transactions partial outages after the BL Release 2.27 Upgrade on PROD.

Occurrences:
1-05:45 to 13:10: 91 transactions affected
2-14:02 to 14:20: 145 transactions affected",Daniel Velado,Daniel Velado,,,,Banking.Live,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8845689013/2025-07-09+-+P3+-+Critical+-+Partial+Outage+of+Transaction,2025-07-09T15:52:27.511Z,2025-07-09T15:52:27.511Z,,,,,,,,,,,,,2026-02-02T16:30:09.498Z,,public,362.5,249.5,82.8,30.2,09jul-p3-critical-197-partial-outage-of-transaction,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0950KATEG5,Reported by Daniel Velado,false
198,MOX – IPS Timeout Advisements ,Service Incident,Investigating,P3 - Critical,"Our monitoring detected IPS timeout advisements affecting the MOX BIN (client: dragon-sc) starting around 07:55 UTC on 10 July 2025. Despite responses being sent from our platform within 3–4 minutes, MOX continues to receive timeouts on these transactions.

Initial checks confirm that this is not a network issue on our side. A support ticket has been logged with Mastercard for further investigation. We’ve also asked the client to confirm if any issues are present on their side. Awaiting their feedback.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,MOX,,,Inconclusive,https://teams.microsoft.com/l/meetup-join/19%3ameeting_MjZjZjlhNWQtODA2Zi00ZTRjLWEzMDQtNDNlMmU1YzE3Y2Qw%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%22067d56db-044f-4b5b-b44c-8303c87625ab%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8847327309/2025-07-10+-+P3+-+Critical+-+MOX+IPS+Timeout+Advisements,2025-07-10T09:01:29.413Z,2025-07-10T09:01:29.413Z,,,,,2025-07-11T11:14:15.732Z,2025-07-10T07:55:00.000Z,2025-07-10T12:57:46.822Z,,2025-07-11T11:42:40.003Z,,2025-07-11T11:42:40.003Z,94366,2026-02-02T16:31:32.513Z,,public,891.8,784.2,80.2,27.3,10jul-p3-critical-198-mox-ips-timeout-advisements,https://slack.com/app_redirect?team=T03SMJWS8&channel=C094T0XRHJT,Reported by Snothile Dlamini,true
201,Partial Transaction Processing Outage,Service Incident,Closed,P3 - Critical,"A planned maintenance on July 10th at 18:00 UTC (CCM-1175) caused an unexpected 30-second network fluctuation. Although the fluctuation was brief, it triggered a known application-level issue that delayed session recovery between PayRoute and Mastercard systems for some clients.

This resulted in the following client impacts:

C24 - 20-minute transaction degradation (2,748 impacted txns), due to the delay in recovery.

Dopay - degraded ATM withdrawals resulted in 291 reversal advisements.

The impact was not immediately observed through monitoring, which delayed internal escalation to the recovery coordination.

The customer impacts have been resolved. The root cause was application session handling after the service experienced the network interruption.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,C24,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8851390483/2025-07-11+-+P3+-+Critical+-+Partial+Transaction+Processing+Outage,2025-07-11T15:27:38.089Z,2025-07-11T15:27:38.089Z,,,,,2025-07-10T16:25:00.000Z,2025-07-10T16:05:00.000Z,,,,,2025-07-11T15:27:38.089Z,,2026-02-02T16:32:15.654Z,,public,103.1,57.4,45.7,0,11jul-p3-critical-201-partial-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0962BS1HH6,Reported by Snothile Dlamini,false
203,Partial Outage of API Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"• At 06:58 UTC on 15th July, Mukuru reported intermittent failures of API calls.

• Conference bridge was initiated, investigation identified issues with web server (WEB-05), the server was taken out of the cluster at 08:44 UTC on 15th July, this action restored service.

• WEB-05 was added back to the cluster at 09:52 UTC on 15th July and is functioning as expected. 

• Investigation to determine the root cause of the failure with WEB-05 is ongoing.

 ",Kaisar Mahmood,Vicnesh Baskar,Pradeep Boopathe Rajeswari,,Claudiu Stoica,VoucherEngine,,web server (WEB-05) was taken out of the cluster at 08:44 UTC on 15th July,"Intermittern 404 errors on all API calls, retries were successfull ",Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8853258513/2025-07-15+-+P2+-+Partial+Outage+of+API+Processing,2025-07-15T08:04:45.639Z,2025-07-15T08:04:45.639Z,,,,,2025-07-15T11:29:05.520Z,,2025-07-15T09:17:01.525Z,,,,,4335,2025-07-15T12:11:53.893Z,,public,611.9,579.5,32.4,0,15jul-p2-203-partial-outage-of-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C095P1NBD35,Reported by Vicnesh Baskar,false
204,Partial Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,Conference bridge established. Investigation in progress. ,Kaisar Mahmood,Kaisar Mahmood,,,,VoucherEngine,,,Partial outage of transaction processing ,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZThlMGIwMDEtYzczZC00YjAwLWI4YzUtOWZjMGE2NmY1YzY5%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%22d48a7985-ef95-4c3b-8394-cfba4ea3afbb%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8853881010/2025-07-15+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-07-15T10:19:44.077Z,2025-07-15T10:19:44.077Z,,,,,2025-07-15T23:13:33.455Z,2025-07-15T01:43:00.000Z,2025-07-15T23:13:33.455Z,,,,,46429,2026-02-02T16:33:12.250Z,,public,499.5,459.2,30.3,10,15jul-p1-204-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C095WM67XC4,Reported by Kaisar Mahmood,false
206,No transaction on VISA PR,Service Incident,Closed,P4,"VISA PR02 was restarted and transaction processing normalised after 04:08 UTC. The impact assessment is ongoing; thus far, no advisories have been observed from VISA. The issue is suspected due to an activity on VISA, ticket logged for confirmation.",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Ardiansyah Ar,Banking.Live,,Restart of VISA PR02,Partial Outage of Transaction Processing,TBD,,,,,,,,,,,,2025-07-16T04:10:45.536Z,2025-07-16T04:10:45.536Z,,,,,2025-07-16T04:46:41.528Z,,2025-07-16T04:39:13.706Z,,2025-07-16T04:48:15.801Z,,2025-07-16T04:48:15.801Z,2155,2025-07-17T08:24:20.485Z,,public,130.3,130.3,0,0,16jul-p4-206-no-transaction-on-visa-pr,https://slack.com/app_redirect?team=T03SMJWS8&channel=C095Z0ER1T7,Reported by Vicnesh Baskar,true
207,Partial Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"Partial outage of transaction processing occurred from 09:41 UTC to 09:44 UTC. Currently transactions are processing as expected with no issues. Investigation into the root cause is ongoing, ticket has been raised to DB Team for further investigation as AS team identified Paycore DB utilization reached 100% between 09:35 to 09:40 UT.",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,Wio,,1 minute partial outage of transaction processing occurred from 09:41 UTC to 09:42 UTC. 88 transactions were impacted. ,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8868888579/2025-07-16+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-07-16T10:48:12.910Z,2025-07-16T10:48:12.910Z,,,,,2025-07-16T14:15:42.403Z,2025-07-16T01:41:00.000Z,2025-07-16T14:15:42.403Z,,,,,12449,2025-07-23T07:55:01.527Z,,public,235.5,163.7,71.8,0,16jul-p3-critical-207-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C095YML3TRU,Reported by Kaisar Mahmood,false
208,Partial Outage of Transaction Processing,Service Incident,Closed,P2,"On 17 July 2025 at 04:38 UTC, the Technical Team was alerted about service disruptions via multiple monitoring alerts. Initial investigation showed that some transactions between 04:32 UTC to 04:35 UTC were experiencing timeouts, caused by a blocking query in the database

The issue was resolved automatically after 04:35 UTC since the blocking query completed execution. Transaction processing normalised thereafter, with no further transaction failures observed. Internal teams are involved in a detailed investigation. Additional updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Heru Nugroho,,Ardiansyah Ar,VoucherEngine,,Blocking query completed execution,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8857780234/2025-07-17+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-07-17T05:14:03.926Z,2025-07-17T05:14:03.926Z,,,,,,,,,2025-07-17T07:06:07.943Z,2025-08-04T23:30:48.210Z,2025-08-04T23:30:48.210Z,,2026-02-02T16:33:44.260Z,,public,146.5,146.5,0,0,17jul-p2-208-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C096VP5MUKA,Reported by Vicnesh Baskar,false
209,Partial Outage of Transaction Processing,Service Incident,Closed,P4,"Investigating a partial impact on ISB transactions, started at 14:42 UTC.
PYTG Network and Application were working as intended.
ISB confirmed an issue on their end an impact stopped as of 16:38 UTC",Daniel Velado,Daniel Velado,,,,Banking.Live,Islandsbanki,,,,,,,,,,,,,,,2025-07-17T15:38:44.712Z,2025-07-17T15:38:44.712Z,,,,,2025-07-17T16:45:20.990Z,,2025-07-17T16:45:20.990Z,,,,2025-07-17T16:45:20.990Z,3996,2025-07-17T16:45:21.126Z,,public,309.5,275.1,0,34.4,17jul-p4-209-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0966U2NXEJ,Reported by Daniel Velado,false
210,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"Investigating advice messages on sh02.
Begin: 15:01:30
End: 15:44:00",Daniel Velado,Daniel Velado,,,,Banking.Live,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8861811170/2025-07-18+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-07-18T16:31:00.844Z,2025-07-18T16:31:00.844Z,,,,,,,,,,,,,2026-02-02T16:34:24.368Z,,public,839.1,595,224,20.2,18jul-p1-210-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C096DAKGGLE,Reported by Daniel Velado,false
211,Full Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 19 July 2025 at 01:13 UTC, the client reported that VISA transactions were being processed in STIP. Initial investigation revealed that while the systems were operational, transactions were being processed in stand-in mode between 00:07 UTC to 00:39 UTC. Both PayPower and PayRoute instances were proactively restarted, and on further investigation, it was observed that there was a connectivity error with VISA, which automatically resolved without any manual intervention.

Further investigation from the relevant teams is ongoing to determine the underlying cause. Updates on root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,D360,Automatically resolved without any manual intervention.,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8862073502/2025-07-21+-+P1+-+Full+Outage+of+Transaction+Processing,2025-07-21T01:25:56.936Z,2025-07-21T01:25:56.936Z,,,,,,,,,2025-07-21T06:50:15.199Z,,,,2025-07-21T06:53:19.102Z,,public,221.8,205.5,0,16.3,21jul-p1-211-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0967TXHDD5,Reported by Vicnesh Baskar,false
212,Full Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 20 July 2025 at 07:27 UTC, the monitoring team detected a disconnection on nodes prod_goty_6b and prod_goty_6c connecting to Bancnet, causing a full outage of transaction processing. Initial investigation showed port monitor failures. The systems were restored at 07:54 UTC, and transaction processing normalised thereafter with no further issues.

Bancnet later confirmed that the PLDT VPN tunnel, a local service provider, went down, causing the disconnection between Paymentology and Bancnet. PLDT has been contacted for further details, alongside an ongoing investigation by the internal Network team. Updates on root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Heru Nugroho,,Wilson Keneshiro,VoucherEngine,GoTyme,No manual intervention,,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8863645707/2025-07-21+-+P1+-+Full+Outage+of+Transaction+Processing+1,2025-07-21T02:05:12.566Z,2025-07-21T02:05:12.566Z,,,,,,,,,2025-07-21T07:05:32.046Z,,,,2026-02-02T16:35:30.186Z,,public,71.5,71.5,0,0,21jul-p1-212-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C096KKKMQPQ,Reported by Vicnesh Baskar,false
213,Outage of Transaction & API Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,"• Transaction and APIs are processing as expected from 14:04 UTC  21st July. Issue auto resolved, no action was taken by PYTG support team to resolve the issue.

• Investigation to determine the root cause is ongoing.",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,Bank Al Etihad,"Issue auto resolved, no action was taken by PYTG support team to resolve the issue.",Transaction and API processing ,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZDRkYzFjMTktOTNiNy00ZjhhLThiZTktMTE4ZWUxYjNmMzQ0%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%22d48a7985-ef95-4c3b-8394-cfba4ea3afbb%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8863744347/2025-07-21+-+P1+-+Outage+of+Transaction+API+Processing,2025-07-21T14:32:35.660Z,2025-07-21T14:32:35.660Z,,,,,2025-07-21T14:57:46.631Z,2025-07-21T05:55:00.000Z,2025-07-21T14:57:46.631Z,,,,,1510,2026-02-02T16:36:10.350Z,,public,543.4,368.9,138.2,36.4,21jul-p1-213-outage-of-transaction-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C096NFUV39U,Reported by Kaisar Mahmood,false
214,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 22 July 2025 at 06:46 UTC, the Application Support team was alerted via monitoring tools on latency issues impacting Crimson response time and transaction processing. Initial investigation revealed multiple long-running queries, causing the latency. 

The queries were killed, and web servers were restarted to clear any stuck queries. By 07:21 UTC, transaction processing had normalised. The Application and Database team is currently engaged in a root cause investigation, and further updates will be shared through the Root Cause Analysis Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,Alaa Wehbe,VoucherEngine,,Killed long running queries and web server restart,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8865349799/2025-07-22+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-07-22T07:04:38.595Z,2025-07-22T07:04:38.595Z,,,,,2025-07-22T07:54:09.638Z,,2025-07-22T07:54:09.638Z,,2025-07-30T23:57:35.408Z,,,2971,2025-07-30T23:57:35.823Z,,public,440.6,430.6,10,0,22jul-p1-214-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C096SNCD7U6,Reported by Vicnesh Baskar,false
215,Intermittent Transaction Processing Outage,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"Mastercard reported transaction processing issues on BIN 557377 (C24). Internal monitoring confirmed disconnections between `payroute-mc-sharedeu-c24-2bv-6500` and Mastercard, observed multiple times. PagerDuty alerts noted IPS timeout and socket disconnection in SH EU.

Internal bridge has been established for an investigation",Snothile Dlamini,Snothile Dlamini,Mauro Merino,,Kamal Thapa,Banking.Live,C24,Service restart,,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_NTRkMGQ3N2MtOWM2Ny00MGM1LThkYWMtM2QxNDczODJkYTE2%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%22067d56db-044f-4b5b-b44c-8303c87625ab%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8866758728/2025-07-22+-+P2+-+Intermittent+Transaction+Processing+Outage,2025-07-22T15:06:31.078Z,2025-07-22T15:06:31.078Z,,,,,2025-07-22T15:34:01.401Z,2025-07-22T13:26:00.000Z,2025-07-22T15:34:01.401Z,,,,,1650,2025-07-23T15:05:11.115Z,,public,524,420.8,103.2,0,22jul-p3-critical-215-intermittent-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C096V9Q3NMC,Reported by Snothile Dlamini,false
216,Full Outage of JONET Transaction Processing,Service Incident,Closed,P3 - Critical,"On 22 July 2025 at 23:08 UTC, the team was alerted about a disconnection issue related to Cortex on host `pt-boj-proute01.paymentology.org`. The initial investigation revealed that a socket disconnection occurred at 23:11:03 UTC, which was auto-reconnected by 23:11:08 UTC. However, despite reconnection, no transactions were processed until a manual restart of the Jonet PayRoute was done. The PayRoute Jonet system was restarted at 07:38 UTC on 23 July 2025, and transaction processing normalised after 23:37:53 UTC with no further issues.",Vicnesh Baskar,Vicnesh Baskar,,,Jenifer Cachero,Banking.Live,Bank of Jordan,Manual restart of the Jonet PayRoute,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8867971188/2025-07-23+-+P3+-+Critical+-+Full+Outage+of+JONET+Transaction+Processing,2025-07-23T00:41:17.443Z,2025-07-23T00:41:17.443Z,,,,,,,,,2025-07-23T00:43:40.403Z,,2025-07-23T00:43:40.403Z,,2026-02-02T16:37:48.912Z,,public,116.8,86.8,0,30,23jul-p3-critical-216-full-outage-of-jonet-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C097EAP281F,Reported by Vicnesh Baskar,true
218,Partial Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"• No Spike of memory utilization after ECR implementation (on 25th July) for C24, NOMO, UW, and Snappi.

• AS team to monitor closely and if any server shows critically low free memory, PayRoute will be proactively restarted to prevent any outage. MOX is high volume client that will be monitored extra closely.

• Additional memory (leak) alerting was implemented on 25th July.

• We will look to deploy the ECR to the remaining Environments / Clients this week. Service Delivery team will plan accordingly – Exact date/times to TBC.

 ",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,C24,ECR Implemented,Partial Outage of Transaction Processing ,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_MDc5MDc2MDEtYWQwMy00NTRhLWFkMDAtYTRhY2Q4NGE4Njdk%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%22d48a7985-ef95-4c3b-8394-cfba4ea3afbb%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8883863597/2025-07-25+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-07-25T13:02:05.052Z,2025-07-25T13:02:05.052Z,,,,,2025-07-28T07:44:29.208Z,2025-07-25T04:26:00.000Z,2025-07-28T07:44:29.208Z,,,,,240144,2025-07-31T08:05:11.833Z,,public,504.3,339.6,89.1,75.6,25jul-p3-critical-218-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C097BRCMDQT,Reported by Kaisar Mahmood,false
219,Full Outage Of Transaction and API Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 30 July 2025 at 12:17 UTC, we experienced a full outage affecting transaction processing and API services. All TRN clusters, as well as supporting Web and API components, went down simultaneously. This resulted in HTTP 503 errors and a complete halt to transaction activity across the platform.

Recovery required a coordinated restart of ColdFusion services across multiple servers. Web services came back first, followed by transaction and API clusters. By the end of the bridge, all components were back online, and transaction traffic had resumed. 

The exact root cause of the outage is still under investigation.",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8882159852/2025-07-30+-+P1+-+Transaction+Processing+Outage,2025-07-30T12:39:45.349Z,2025-07-30T12:39:45.349Z,,,,,2025-07-30T13:16:15.793Z,,2025-07-30T13:16:15.793Z,,,,,2190,2025-07-30T13:28:55.215Z,,public,549.4,549.4,0,0,30jul-p1-219-full-outage-of-transaction-and-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C097Q7C6QGP,Reported by Snothile Dlamini,false
220,Full Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"The AFS scheme is disconnected and is refusing reconnection. PR restarted, further investigation underway. A ticket has been logged to AFS for support as well.",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Dominic Wee,Banking.Live,Dopay LLC,Reconnection with scheme AFS,Full Outage of Transaction Processing,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8892645403/2025-08-05+-+P1+-+Full+Outage+of+Transaction+Processing,2025-08-05T01:50:37.138Z,2025-08-05T01:50:37.138Z,,,,,2025-08-05T02:07:41.678Z,,2025-08-05T02:07:41.678Z,,2025-08-05T04:10:10.590Z,,,1024,2026-02-02T16:41:44.589Z,,public,305.6,273.1,32.5,0,05aug-p1-220-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C098UC48S2F,Reported by Vicnesh Baskar,false
221,Transaction Processing Outage,Service Incident,Post-Incident Tasks & Documentation,P1,"On 5 August 2025, a partial transaction processing outage occurred, with multiple Crimson alerts and increased 9XXX declines observed. The issue was linked to one of the transaction routing nodes (TRN-02), which was removed from the cluster and restarted. This action successfully restored normal transaction flow, with the last errors observed at 10:20 AM UTC. Service has remained stable since, and root cause investigations are ongoing.",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8892874821/2025-08-05+-+P1+-+Transaction+Processing+Outage,2025-08-05T10:14:36.285Z,2025-08-05T10:14:36.285Z,,,,,2025-08-05T10:45:00.644Z,2025-08-05T10:00:00.000Z,2025-08-05T10:45:00.644Z,,,,,1824,2025-08-05T10:45:17.093Z,,public,449.1,377.6,52.2,19.4,05aug-p1-221-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C099FSKH02V,Reported by Snothile Dlamini,false
223,Incorrect Settlement Mapping,Service Incident,Fixing,P1,"MOX escalated a critical issue involving incorrect settlement postings caused by duplicate ECNO values assigned to different tokens. Investigation revealed that:

• Tokens were created via the `create_card` API at nearly the same timestamp.

• Both tokens share the same PAN and CVV, and originate from the same _perso_ file.

• The duplicate ECNOs caused the settlement system to misroute funds based on matching fields (DE38 and DE63), resulting in charges applied to the wrong customer.

The issue appears to originate from a race condition or logic flaw within the `stackfill` process or PayKeyServ ECNO generation, possibly tied to previous unresolved defects (e.g., DEFECT-415, DEFECT-147). Although a fix had been deployed earlier, older tokens or regression could be responsible.

Incident is now being proactively handled as a P1 based on MOX escalation.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,MOX,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8911585324/2025-08-07+-+P1+-+Incorrect+Settlement+Mapping,2025-08-07T10:23:35.994Z,2025-08-07T10:23:35.994Z,,,,,,2025-08-04T02:09:00.000Z,2025-08-07T14:04:42.645Z,,,,,,2026-02-02T16:43:35.515Z,,public,2613.4,2143.8,365.3,104.2,07aug-p1-223-incorrect-settlement-mapping,https://slack.com/app_redirect?team=T03SMJWS8&channel=C099G596THA,Reported by Snothile Dlamini,false
224,Acorn AWS Migration,Service Incident,Investigating,P3 - Critical,Channel regarding Acorn AWS migration deployment.,Daniel Velado,Daniel Velado,,,,Acorn / FlexPay,,,,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_NjE2YjIxYTItNmUwMi00Mjg5LWJjZTYtODQ4N2YyMzMyNDQ4%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%2277fd2081-e5b3-4187-a436-a13e4cc70a0f%22%7d,,,,,,,,,,,2025-08-11T20:10:54.495Z,2025-08-11T20:10:54.495Z,,,,,,,2025-08-13T08:00:26.550Z,,,,,,2026-02-02T16:45:29.291Z,,public,950.6,572.2,42.3,336,11aug-p3-critical-224-acorn-aws-migration,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09A9SH455F,Reported by Daniel Velado,false
225,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On August 13, 2025, at 05:29 UTC, the team was alerted to delays in authorisations and advisements from multiple clients. Initial investigation showed that responses from the Mastercard scheme were delayed before being delivered to the respective PayRoutes. The Mastercard cloud MIPS system appeared to delay transaction delivery, causing timeouts across multiple clients.

The issue was confirmed to be outside the platform, and a support case was raised with Mastercard for investigation. Transaction processing normalised post 05:32UTC with no further delays observed. The issue was escalated to Mastercard, and a deeper investigation is underway",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Abdul Sahaptheen,Banking.Live,"Mettle, MOX, Islandsbanki, Wio, Bank of Jordan",N/A,,Inconclusive,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8964276268/2025-08-13+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-08-13T08:04:13.458Z,2025-08-13T08:04:13.458Z,,,,,,,,,,,2025-08-13T08:04:13.458Z,,2026-02-02T16:50:03.731Z,,public,604.2,448.4,74.1,81.6,13aug-p3-critical-225-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C099ZL4FPR9,Reported by Vicnesh Baskar,false
227,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"On 16 August 2025 at 16:56 UTC, the support team was alerted to high latency on multiple Crimson instances. The team began investigating immediately and identified timeouts occurring during XML-RPC method calls, such as `deductauth` and `balance`. The issue was resolved at 16:58 UTC as latency levels dropped and transactions processing normalised.

A subsequent analysis showed that blocking queries on the database during the incident window were the likely cause of the latency and timeout errors. Further investigation into the root cause is currently underway and and further updates will be provided in the root cause analysis report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,William Perez,,David Aragon,VoucherEngine,,Auto-resolved,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8915550253/2025-08-17+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-08-17T02:49:01.498Z,2025-08-17T02:49:01.498Z,,,,,,,,,,,,,2026-02-02T16:46:04.027Z,,public,116.5,104.1,12.4,0,17aug-p2-227-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09BGP13V6U,Reported by Vicnesh Baskar,false
228,Full Outage of Transaction Processing,Service Incident,Closed,P4,"On 17 August 2025 at 10:33 UTC, the support team was alerted about a full outage of transaction processing, impacting VISA and MADA schemes. Initial investigation showed high CPU utilisation on the PaycoreDB node and complete failure of transaction processing on affected services. The PayPower services were restarted, but failed to restore transaction processing.

The client confirmed their core banking system was experiencing issues, and at 11:37 UTC, all PayRoutes (PRs) were stopped in response to a client request, disconnecting from the schemes. Recovery activities began at 12:20 UTC once the client confirmed their systems were restored.

At 13:00, both schemes were successfully reconnected, and no further response failures were observed. Resource utilisation normalised following the completion of advisement processing. Client has confirmed that transaction processing has normalised.",Vicnesh Baskar,Vicnesh Baskar,Rasheed Khan,,Suganya Eswaran,Banking.Live,D360,Fix of client side technical failure,Full outage of VISA transaction processing,Client,https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZjExZTUzZTgtMTU4Ny00NDc3LTgwYzMtYWMxOGY0MTAwNmZj%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%225bc8ffce-74fa-4e51-af76-275e9877517e%22%7d,,,,,,,,,,,2025-08-17T10:59:02.982Z,2025-08-17T10:59:02.982Z,,,,,2025-08-17T13:40:42.791Z,,2025-08-17T11:54:30.994Z,2025-08-17T13:40:42.791Z,2025-08-20T08:23:34.188Z,,2025-08-20T08:23:34.188Z,9699,2025-08-20T08:23:34.319Z,,public,909.9,526.5,304.2,79.2,17aug-p4-228-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09AUGTK60L,Reported by Vicnesh Baskar,true
229,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P4,"On 17 August 2025 at 15:00 UTC, the team was alerted about FAST authorisation transaction failures on the client instance. Initial investigation showed approximately 1500 transactions were impacted, with monitoring indicating high CPU usage and later high disk utilisation on the Paycore DB server.

Intermittent FAST API response failures caused elevated queuing, which impacted DB performance. Around 00:14 UTC, disk usage reached 95%, driven by excessive log file growth. Logging was observed to be triggered by transactions with longer query times—transactions that usually took milliseconds were now exceeding 1 second. No internal changes were confirmed to have caused this behaviour.

At 00:30 UTC, logging was reduced to capture only errors, resulting in decreased CPU usage. By 00:43 UTC, transaction processing had stabilised without timeouts. The tactical fix identified was to increase disk space on the Paycore DB server. Access has been requested from the client to proceed, and the infra team is on standby to execute the disk expansion.",Vicnesh Baskar,Vicnesh Baskar,Jhonatan Roldan Tavera,,Kevin Valle,,D360,Temporarily cleared space from the database server,,Paymentology,,,,,,,,,,,,2025-08-17T17:38:56.152Z,2025-08-17T17:38:56.152Z,,,,,,,,,,,,,2025-08-26T12:34:17.273Z,,public,362.2,256.7,29.6,75.9,17aug-p4-229-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09ARDA6T5Y,Reported by Vicnesh Baskar,false
230,Partial Transaction Processing Outage,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"Paymentology observed multiple client reports of transaction declines for BINs 533885 (Virtual Fuel Payout Cards), 519260 and 533892 (PayCentral FuelCentral) starting on 18 August 2025 at 08:58 UTC.
Obsered issues included elevated stand-in volumes, MTI 420 reversals, and parsing errors related to DE-106.


The root cause was identified as a mismatch in DE106 formatting: Mastercard sends DE106 as a 7-byte binary, while Paymentology’s system responded with a 14-byte ASCII format, leading to parsing failures and rejections.


A workaround was recommended for cardholders(retry payment inside the service station or use alternate POS machines).
The Crimson team implemented a fix (CCM-1396) on 19 August, which successfully resolved the issue. Post-deployment monitoring confirmed stability and normal transaction processing.",Vicnesh Baskar,Snothile Dlamini,Maureen Figueroa,,Ardiansyah Ar,VoucherEngine,"Pay Central (Prepay Central), Achievement Awards (AAG)",,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8924987403/2025-08-18+-+P3+-+Critical+-+Partial+Transaction+Processing+Outage,2025-08-18T09:26:38.522Z,2025-08-18T09:26:38.522Z,,,,,2025-08-19T16:20:46.807Z,2025-08-18T09:25:00.000Z,2025-08-19T16:20:46.807Z,,,,,111248,2026-02-02T16:46:34.228Z,,public,2983.5,2718.1,199.6,65.8,18aug-p3-critical-230-partial-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09ATTDBTDG,Reported by Snothile Dlamini,false
231,Intermittent Transaction Processing Outage,Service Incident,Closed,P4,"On 19 August 2025 at 04:00 UTC, the support team was alerted to intermittent transaction processing failures due to repeated FAST Active Response timeouts from the client system. Initial investigation showed that these timeouts caused CPU spikes on the PayRoute Paycore database nodes, which in turn led to wider, intermittent transaction failures.

The client cornfirmed that their corebanking system was facing an issue on high throughput on their corebanking system, leading to the FAST Active Response failures. The resulting timeouts induced CPU spikes on Paycore, leading to broader intermittent transaction failures. 

Transaction processing began stabilising around 21:08 UTC, and the client confirmed the issue on their end was fully resolved by 23:00 UTC. Transaction processing remained stable thereafter and under active monitoring. 

A thorough investigation is underway by the database team to identify potential improvements to handle such failures while awaiting additional details on the issue and the fix done on the client's system. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,D360,Fix on client's corebanking system,Fast Active Response Timeouts,Third-Party - Client,,,,,,,,,,,,2025-08-19T09:31:03.914Z,2025-08-19T09:31:03.914Z,,,,,2025-08-20T08:13:35.147Z,2025-08-19T02:00:00.000Z,2025-08-19T09:57:08.389Z,,2025-08-20T08:19:44.989Z,,2025-08-20T08:19:44.989Z,3303,2025-08-20T08:21:28.190Z,,public,531.3,454.6,66.8,10,19aug-p4-231-intermittent-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09BCSJHATB,Reported by Snothile Dlamini,true
232,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"On 08 July 2025 at 00:00 UTC, the Advanced Support team was engaged following a configuration issue involving a manual database update. Initial investigation showed that a script intended to update Mastercard campaign settings was erroneously applied to a Visa campaign (1851), impacting GoTyme’s Visa OCT/AFT settings. The configuration was reversed on 05 August 2025 at 23:59 UTC, and transaction processing normalized thereafter with no further client complaints. The issue was resolved by identifying and correcting the misapplied configuration in the database. Internal teams including Advanced Support, Account Management, and Monitoring have been involved in a detailed investigation. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,,,,VoucherEngine,GoTyme,Rollback changes,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8928788510/2025-08-21+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-08-21T08:11:37.620Z,2025-08-21T08:11:37.620Z,,,,,,,,,,,,,2025-08-21T08:16:05.414Z,,public,176.5,165.2,11.3,0,21aug-p2-232-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09BCNDQ8PL,Reported by Vicnesh Baskar,false
233,Challenges with 3D Secure (3DS) Post AWS Migration,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"**Issue:** Following its migration to AWS cloud, the Acorn application is experiencing an ongoing issue with 3D Secure (3DS) authentication.

**Impact:** Customers may encounter difficulties completing purchases that require 3DS verification.

**Causes:** The issue was caused by an incorrect configuration in our environment after the AWS migration, specifically directing transactions that require 3DS to a server not set up correctly for OTP generation.

**Steps to resolve:** The incorrect configuration has been corrected, restoring full 3DS functionality.",Jason Fairweather,Jason Fairweather,,,,Acorn / FlexPay,,,,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_YmUwNDA3OTAtOWJkYi00MzFkLThmM2Y[…]2c%22Oid%22%3a%22786507a8-a6b2-4d6a-a962-87e448d69fce%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8957887130/2025-08-21+-+P3+-+Critical+-+Challenges+with+3D+Secure+3DS+Post+AWS+Migration,2025-08-21T09:20:17.666Z,2025-08-21T09:20:17.666Z,,,,,2025-08-21T10:25:27.813Z,2025-08-21T04:30:00.000Z,2025-08-21T09:56:02.296Z,,,,,3910,2025-09-01T20:19:38.387Z,,public,141.9,136.7,5.2,0,21aug-p3-critical-233-challengeswith-3d-secure-3ds-post-aws-migration,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09BSAR0E57,Reported by Jason Fairweather,false
234,Transaction Processing Outage,Service Incident,Post-Incident Tasks & Documentation,P2,"On 21 August 2025 at 09:06 UTC, Mastercard informed Paymentology of problems with processing transactions for BIN 557377 (C24). Mastercard was seeing unusually high stand-in activity and timeouts, while our internal monitoring systems showed transactions were flowing normally through the expected MIPs (2BU and 2BV).

C24 raised a P1 incident due to a surge in POS declines. Mastercard pointed out that the affected MIP appeared to be 4IR, which is not used in our setup for C24. The declines had reason code 0030, and these issues were only visible from Mastercard’s side.

During the investigation, Paymentology conducted thorough checks, including looking at instance-level processing and reviewing PayRoute logs. Everything appeared normal from our end. However, Mastercard discovered that socket connections were closing on MIPs 00B and 4IR. Our service delivery team confirmed that all C24 traffic should be routed exclusively to the datacenter MIPs (2BU and 2BV).

Investigation revealed that Mastercard had started routing traffic simultaneously to both our datacenter MIPs and the cloud MIPs (4IR and 00B) without prior confirmation or approval from the PTY (CCM) team. This unexpected routing caused the stand-in activity and the “Network Not Dispatched” errors Mastercard observed.

To address the situation, PTY restarted the applications on 2BU and 2BV as a precaution and kept an open communication bridge with Mastercard. Mastercard then set up their own internal conference call and began rolling back the routing changes at 12:27 UTC, prioritizing traffic through our MIPs. The rollback lasted about an hour. During this time, starting from 13:12 UTC, we received around 3,7k advisements.

After the rollback was completed, normal transaction processing resumed on the 2BU and 2BV MIPs. By 13:21 UTC, we confirmed stability on our side, with Mastercard traffic flowing through our Payment Route instances as expected. We are still waiting for Mastercard to confirm that no traffic is being sent to the cloud MIPs, since those are not configured on our end and thus do not appear in our system logs. C24 was informed that their transactions are now processing normally following the rollback.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,C24,,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8950874224/2025-08-21+-+P2+-+Transaction+Processing+Outage,2025-08-21T09:34:14.562Z,2025-08-21T09:34:14.562Z,,,,,2025-08-21T13:35:14.634Z,2025-08-21T09:32:00.000Z,2025-08-21T13:35:14.634Z,,,,,14460,2026-02-02T16:47:36.232Z,,public,1213.6,1144.8,48.8,20,21aug-p2-234-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09BD4KCV18,Reported by Snothile Dlamini,false
235,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P2,"**Issue:** High latency on CRIMSON intermittently impacts transaction processing, including API response times.

**Impact:** Unknown

**Causes:** The issue was caused by long-running queries on the primary database (TT-AWS-P-SQL-09), exacerbated by a new query test for DE105 in chargeback query.

**Steps to resolve:** Engineering teams engaged and terminated long-running sessions to stabilize services. No new errors have been reported, and transaction processing has returned to normal.",Vicnesh Baskar,Vicnesh Baskar,Maureen Figueroa,,Ardiansyah Ar,VoucherEngine,,Killed long running session,Intermittern transaction processing failures,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZjI3ZWQyMDQtN2VlZS00Mzc5LWI3YmYtNzIzMTIyODdmNWU0%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%225bc8ffce-74fa-4e51-af76-275e9877517e%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8940027948/2025-08-26+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-08-26T02:49:52.168Z,2025-08-26T02:49:52.168Z,,,,,2025-08-26T03:08:06.908Z,,2025-08-26T03:08:06.908Z,,2025-08-26T03:08:35.126Z,,2025-08-26T03:08:35.126Z,1094,2025-08-26T08:49:46.825Z,,public,411.1,361.1,20,30,26aug-p2-235-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09BVMREYPM,Reported by Vicnesh Baskar,false
236,Full Outage of JONET Transaction Processing,Service Incident,Closed,P4,"**Issue:** JONET transactions ceased processing since 00:13 UTC, with connectivity including network and sign-on operational but transaction flow not restarting.

**Impact:** Bank of Jordan has been unable to process transactions via JONET since 00:13 UTC.

**Causes:** The issue was on the client's side.

**Steps to resolve:** Sequential restarts of Jonet PayRoute and PPWR were completed, and Bank of Jordan confirmed they have restarted their service, resuming Cortex transactions since 27-Aug-2025 04:46 UTC",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Huy Pham,Banking.Live,Bank of Jordan,,No transactions via Jonet since 00:13 UTC,Client,,,,,,,,,,,,2025-08-27T02:53:26.310Z,2025-08-27T02:53:26.310Z,,,,,2025-08-27T05:26:42.964Z,,2025-08-27T05:26:42.964Z,,,,2025-08-27T05:26:42.964Z,9196,2025-08-27T05:26:43.186Z,,public,306,306,0,0,27aug-p4-236-full-outage-of-jonet-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09BU49LFGX,Reported by Vicnesh Baskar,false
237,Partial Transaction Timeouts – Mastercard Delayed Requests,Service Incident,Monitoring,P3 - Critical,"On August 28, 2025, between 13:16 and 13:19 UTC, some clients experienced transaction delays because Mastercard’s requests to Paymentology’s PayRoute were slower than usual. This affected several clients including BOJ, ADQ, ISB, Mettle, UW, and BB2, with failure rates varying. Paymentology’s systems showed no issues, and firewalls indicated everything was stable, which suggests the delay came from Mastercard’s side. The issue was resolved without manual intervention from our support teams. Mastercard has been engaged for an investigation ",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,"Bank of Jordan, Islandsbanki, Wio, Mettle, Utility Warehouse, Nomo Bank",,,Inconclusive,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8957362784/-+P3+-+Critical+-+Partial+Transaction+Timeouts+Mastercard+Delayed+Requests,2025-08-28T15:09:25.361Z,,,,,,2025-08-28T11:19:00.000Z,2025-08-28T11:17:00.000Z,,,,,,,2026-02-02T16:51:38.243Z,,public,196.5,139.8,6.7,50,28aug-p3-critical-237-partial-transaction-timeouts-mastercard-delayed-reque,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09CWMGV23B,Reported by Snothile Dlamini,false
238,Full Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,Transactions are currently processing as expected. The outage was triggered by a physical server failure in the UKS Azure region affecting the UKS firewall.,Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,"Utility Warehouse, Rain, Snappi, Bank Al Etihad, Constantinople, Nomo Bank, ONMO, MOX, Xendit, Mettle, Wio, Bank of Jordan","Remedial actions included PayPower restarts for Visa clients, no restarts required for MasterCard clients as their issues auto-resolved.",Partial Outage of Transaction and API  processing ,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8958181398/2025-09-01+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-09-01T06:08:43.173Z,2025-09-01T06:08:43.173Z,,,,,2025-09-01T10:55:23.235Z,2025-08-31T13:14:00.000Z,2025-09-01T10:55:23.235Z,,,,,17200,2026-02-02T16:54:10.164Z,,public,2120.7,1930.5,77,113.2,01sep-p1-238-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09CGSM5TUP,Reported by Kaisar Mahmood,false
239,Partial Transaction Processing Outage,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"On August 31, between about 06:18 and 06:20 UTC, several urgent alerts flagged issues across the PayRoute Mastercard 4IR service nodes affecting multiple clients including MOX, WIO, BOJ, and OM Bank. Upon investigation, it was found that PayRoute 4IR had temporarily lost connection with Mastercard, causing interruptions in processing transactions and in transaction echo messages.

This disconnection led to transaction processing delays and the problem was resolved once PayRoute re-established connection with Mastercard, and all related alerts stopped with no manual intervention from Paymentology.

A case has been opened with Mastercard (06658656) to look into why the disconnection happened. Internally, it was noted that the 180-second timeout setting for socket reading played a role in how the issue unfolded. Investigation into the root cause is ongoing.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,"Wio, Bank of Jordan, Old Mutual Bank / Olympus, MOX",Reconnection to Mastercard restored normal PayRoute 4IR processing. Alerts cleared automatically once connections were re-established.,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8957395534/-+P3+-+Critical+-+Partial+Transaction+Processing+Outage,2025-09-01T19:54:37.327Z,,,,,,,2025-08-31T07:18:00.000Z,2025-08-31T07:18:00.000Z,2025-08-31T07:36:00.000Z,,,,,2026-02-02T16:54:49.142Z,,public,134.5,91.4,33.1,10,01sep-p3-critical-239-partial-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09CY9MG63F,Reported by Snothile Dlamini,false
240,Partial Outage of API Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,API calls are been processing correctly.,Daniel Velado,Daniel Velado,,,,Banking.Live,,,,Paymentology,https://teams.microsoft.com/meet/3199341100687?p=wJzkdDX2wO6b5rDwX5,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8960442375/2025-09-02+-+P3+-+Critical+-+Partial+Outage+of+API+Processing,2025-09-02T02:05:01.381Z,2025-09-02T02:05:01.381Z,,,,,2025-09-02T03:21:34.247Z,,2025-09-02T03:21:34.247Z,,,,,4592,2026-02-02T16:55:47.963Z,,public,349.1,228.3,100.8,20,02sep-p3-critical-240-partial-outage-of-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09D0MRNLSW,Reported by Daniel Velado,false
241,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P4,"**Issue**: Bank Al Etihad experienced a 6-minute period where transaction processing failed.

**Impact**: Bank Al Etihad transactions on the Banking.Live platform could not be processed for 6 minutes, from 16:25 to 16:31 UTC.

**Causes**: Transaction failures were due to slow responses from the client's systems.

**Steps to resolve**: Transactions resumed processing normally after 16:31 UTC. The client has been notified.",Daniel Velado,Daniel Velado,,,,Banking.Live,Bank Al Etihad,,Bank Al Etihad,Client,,,,,,,,,,,,2025-09-03T17:06:00.481Z,2025-09-03T17:06:00.481Z,,,,,2025-09-03T17:07:36.029Z,,2025-09-03T17:07:36.029Z,,,,,95,2026-02-02T16:56:10.767Z,,public,107.6,97.6,10,0,03sep-p4-241-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09E5MPRCSC,Reported by Daniel Velado,false
242,TEST INCIDENT - PLEASE IGNORE,Service Incident,Closed,P4,"**Issue:** TEST INCIDENT - PLEASE IGNORE.

**Impact:** This was a test

**Causes: This was a test**

**Steps to resolve: This was a test**",Stephen Fleming,Stephen Fleming,,,,Banking.Live,C24,,,,,,,,,,,,,,,2025-09-04T11:41:02.970Z,2025-09-04T11:41:02.970Z,,,,,2025-09-04T12:01:08.227Z,2025-09-04T11:40:00.000Z,2025-09-04T12:01:08.227Z,,,,2025-09-04T12:01:08.227Z,1205,2025-09-04T12:01:08.374Z,,public,30,30,0,0,04sep-p4-242-test-incident-please-ignore,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09D39S8K5M,Reported by Stephen Fleming,false
243,TEST INCIDENT - PLEASE IGNORE,Service Incident,Closed,P4,PLEASE IGNORE,Stephen Fleming,Stephen Fleming,,,,Banking.Live,,,,,,,,,,,,,,,,2025-09-04T15:09:33.172Z,2025-09-04T15:09:33.172Z,,,,,2025-09-04T15:53:37.269Z,,2025-09-04T15:53:37.269Z,,,,2025-09-04T15:53:37.269Z,2644,2025-09-04T15:53:37.449Z,,public,40.4,40.4,0,0,04sep-p4-243-test-incident-please-ignore,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09EC9TK000,Reported by Stephen Fleming,false
244,Duplicate Mastercard settlement file processed,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"**Issue:** A Mastercard settlement file was incorrectly reprocessed, resulting in duplicate settlements for some transactions.

**Impact:** Duplicate settlement records were created for two clients, Agoda and Phillip Bank, affecting transactions including Authorization Number 6492 from December 2024.

**Causes:** The incident was caused by the Mastercard settlement file being processed twice on the VoucherEngine platform due to having two different filenames.

**Steps to resolve:** We have reversed the duplicated settlements for both Phillip Bank and Agoda, completing all necessary reversals.",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,"Agoda, Phillip Bank",,Duplicate Settlement Processing,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8970731616/2025-09-05+-+P3+-+Critical+-+Duplicate+Mastercard+settlement+file+processed,2025-09-05T10:50:06.636Z,2025-09-05T10:50:06.636Z,,,,,2025-09-08T07:02:48.068Z,2025-09-04T10:47:00.000Z,2025-09-05T16:26:11.546Z,2025-09-08T07:02:48.068Z,,,,245561,2026-02-02T16:56:37.358Z,,public,353.5,353.5,0,0,05sep-p3-critical-244-duplicate-mastercard-settlement-file-processed,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09EHP36U3S,Reported by Snothile Dlamini,false
246,Full Outage of MADA transactions,Service Incident,Post-Incident Tasks & Documentation,P1,"On 07 Sep 2025 at 22:35 UTC, the support team was alerted about a full outage of MADA transactions, where all transactions were being declined with response code 916 (Invalid MAC). Initial investigation suspected an issue with the MADA MAC key, and the client was requested to coordinate with MADA to reinitiate the MAC and PIN key exchange.

It was later confirmed that the root cause was the expiry of the previously active MAC key. The system was configured to expect the key to remain valid for one year; it expired on 07 Sep 2025 without any new key being proactively issued by MADA. A new key was requested and received,  but it was then inserted into the system with a new index value, which caused the MAC validation to continue to fail with `macKey is null` errors.

Despite multiple restarts of the PayRoute and PKS systems, the issue persisted until manual intervention was performed. The database team manually updated the key index in the PKS production database, after which MADA transaction processing was successfully restored.

The outage only affected MADA transactions; VISA and Mastercard transactions were unaffected. MADA transaction processing was restored at 03:44 UTC on 08 Sep. The team has since enabled heightened monitoring and is working on permanent fixes to both the key indexing logic and key expiry handling. Further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Dominic Wee,Banking.Live,"D360, Tweeq",Manual update of key index in the PKS production database,Transactions are being declined with response code 916 (Invalid MAC),Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8973680643/2025-09-08+-+P1+-+Full+Outage+of+MADA+transactions,2025-09-08T00:21:45.102Z,2025-09-08T00:21:45.102Z,,,,,2025-09-08T04:18:59.838Z,,2025-09-08T04:18:59.838Z,,,,,14234,2025-09-08T06:59:23.985Z,,public,1828.3,1474.9,80,273.2,08sep-p1-246-full-outage-of-mada-transactions,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09DXFYKF45,Reported by Vicnesh Baskar,false
247,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 11 September 2025 at 04:22 UTC, the support team was alerted to an increase in transaction declines with error 96 following the BL 2.31 release. Initial investigation showed that the new release introduced masking for data elements DE48_33_2 and DE48_33_3. Client's systems performs validation on these fields, and the unexpected masking caused validation failures, resulting in error 96 system declines. The Payroute component was rolled back across all relevant environments at 07:29 UTC, and transaction processing normalised after that time with no further issues observed. The issue was resolved by unmasking the impacted fields through rollback. Internal teams have been engaged for further RCA and preventive actions. Further updates will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Rajesh Subramanian,Banking.Live,Constantinople,Rollback Payroute version,Some transactions are declining with the error code 96,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_OWI2MjlkZDktM2M4ZS00ZWZmLWJjNDUtNTA2NzQ0OTIzZjU2%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%225bc8ffce-74fa-4e51-af76-275e9877517e%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8984101119/2025-09-11+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-09-11T07:15:56.831Z,2025-09-11T07:15:56.831Z,,,,,2025-09-11T07:59:07.278Z,,2025-09-11T07:59:07.278Z,,,,,2590,2025-09-11T09:50:14.308Z,,public,1036.2,915.8,110.4,10,11sep-p1-247-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09EK761R99,Reported by Vicnesh Baskar,false
248,Delayed Processing of VISA EP Files,Service Incident,Post-Incident Tasks & Documentation,P3 - Non-Critical,"The issue began at 23:31 UTC when several VISA EP reports were found missing. Investigation revealed that AWS APP-01’s D drive ran out of free space, causing incomplete VISA incoming files for CIB 420164 and preventing the generation of EP reports.

To resolve this, space was cleared on APP-01, the affected files were downloaded from S3, and manually reprocessed using the EP Processor application. All missing EP reports were successfully generated and delivered to clients, with individual communications sent via Zendesk tickets.

Following this, the related ITF files were reprocessed to generate the CTF settlement file (INCTF01.EPD.Orange.20250913.221501.CTF), which contained approximately 384,000 settlement records and completed processing at 09:24 UTC. The daily settlement report (DSR) task for VISA has been restarted to ensure downstream reconciliation reflects these corrected settlement records.

**Resolution:**

• EP reports have been restored and shared with all impacted clients.

• CTF settlement file processing is complete.

• DSR regeneration is underway to ensure full alignment.",Snothile Dlamini,Snothile Dlamini,,,Wilson Keneshiro,VoucherEngine,"Orange Botswana, GoTyme, Boya Kenya, Panelys, Paycard Guinea, Wave Senegal, Vodacom DRC, Wave Ivory Coast",,Potential downstream impact on reconciliation and reporting for the above clients.,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8992653374/2025-09-14+-+P3+-+Non-Critical+-+Delayed+Processing+of+VISA+EP+Files,2025-09-14T02:05:34.940Z,2025-09-14T02:05:34.940Z,,,,,2025-09-14T09:51:18.686Z,,2025-09-14T02:23:01.351Z,2025-09-14T09:51:18.686Z,,,2025-09-14T09:51:18.686Z,27943,2026-02-02T16:57:14.052Z,,public,480.8,405.6,14.3,60.9,14sep-p3-non-critical-248-delayed-processing-of-visa-ep-files,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09EPLVLE3Z,Reported by Snothile Dlamini,false
249,Full Outage of MADA transations,Service Incident,Post-Incident Tasks & Documentation,P1,"The incident began at 16:28 UTC on 14 September 2025, affecting Derayah MADA PayRoute transactions with a full outage. At first, only switch01 experienced issues, and traffic was successfully routed through switch02 without any impact for clients. Later, both switch01 and switch02 disconnected, which caused a temporary outage of MADA transactions. Application Support restarted both switches, and by 19:38 UTC, transaction processing was confirmed to be back to normal on both. Visa transactions on PayControl were not affected at any point. We are continuing to monitor the situation closely, and validating logs.",Snothile Dlamini,Snothile Dlamini,William Perez,,Kevin Valle,Banking.Live,D360,,Mada outage,Paymentology,,,,,,,,,D360,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8992653339/2025-09-14+-+P1+-+Full+Outage+of+MADA+transations,2025-09-14T17:13:59.473Z,2025-09-14T17:13:59.473Z,,,,,2025-09-14T17:56:44.843Z,,2025-09-14T17:56:44.843Z,,,,,2565,2025-09-14T17:57:19.703Z,,public,782.5,584.5,171.6,26.3,14sep-p1-249-full-outage-of-mada-transations,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09F5DG63K4,Reported by Snothile Dlamini,false
250,Full Outage of MC Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue**: Full outage of Mastercard transaction processing for Tweeq began at 16:06 UTC.

**Impact**: Tweeq customers could not process any Mastercard transactions from 16:06 UTC until service was restored.

**Causes**: TBD

**Steps to resolve**: We restarted PayRoute and PayPower servers. After the restarts, traffic resumed, and connections with Mastercard were restored.",Daniel Velado,Daniel Velado,,,,Banking.Live,Tweeq,,,Paymentology,https://teams.microsoft.com/meet/3323359745219?p=gibEQLhmdJumoxODlC,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8998551589/2025-09-16+-+P1+-+Full+Outage+of+MC+Transaction+Processing,2025-09-16T18:13:20.316Z,2025-09-16T18:13:20.316Z,,,,,2025-09-16T18:35:44.537Z,2925-09-16T22:06:00.000Z,2025-09-16T18:35:44.537Z,,,,,1344,2026-02-02T16:57:43.898Z,,public,525.2,495.2,20,10,16sep-p1-250-full-outage-of-mc-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09FYF8RSCR,Reported by Daniel Velado,false
251,Full Outage of MC Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"**Issue**: Full outage of Mastercard transaction processing for Xendit.

**Impact**: Xendit experienced a full outage of Mastercard transaction processing from 15:40 to 22:16 UTC. No impact was confirmed for other clients.

**Causes**: Unknown

**Steps to resolve**: We manually restarted PayPower application, but we did not see any change. The Advanced Support team proceeded to restart PayRoute 2bu, which restored transaction processing for Xendit. PayRoute 2bv was restarted as a preventive measure.",Daniel Velado,Daniel Velado,,,,Banking.Live,Xendit,,,Paymentology,https://teams.microsoft.com/meet/3693464556298?p=bRlh7oIGnjjkJy0BFm,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/8998748174/2025-09-16+-+P1+-+Full+Outage+of+MC+Transaction+Processing+1,2025-09-16T22:19:29.504Z,2025-09-16T22:19:29.504Z,,,,,2025-09-16T22:44:45.821Z,,2025-09-16T22:44:45.821Z,,2025-09-17T00:37:04.741Z,,,1516,2026-02-02T16:58:16.998Z,,public,468.7,389.3,59.4,20,16sep-p1-251-full-outage-of-mc-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09FHCCAQVB,Reported by Daniel Velado,false
252,Jetco - Partial Transaction Processing Outage,Service Incident,Post-Incident Tasks & Documentation,P1,"At 11:04 UTC, all four VPN tunnels between Paymentology and Jetco went down simultaneously, causing MOX JETCO transactions to stop processing. Other clients and Mastercard connections were unaffected. Initial restart of the Payroute service did not restore traffic. Manual intervention was required from Jetco to re-establish connectivity via a key exchange. 

No monitoring alerts were triggered during the outage; thresholds are being reviewed by the Monitoring team. Follow-up with Jetco is in progress to confirm root cause and prevent recurrence.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,MOX,,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9005302047/2025-09-19+-+P1+-+Jetco+-+Partial+Transaction+Processing+Outage,2025-09-19T11:50:05.301Z,2025-09-19T11:50:05.301Z,,,,,2025-09-19T12:20:45.170Z,,2025-09-19T11:58:49.204Z,,,,,1839,2025-09-19T12:21:12.681Z,,public,827.1,639.3,126.1,61.8,19sep-p1-252-jetco-partial-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09G1RT4M1B,Reported by Snothile Dlamini,false
253,Full Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,"• Currently transactions are processing as expected from  10:44 UTC on 20th Sep.

• We observed 2 brief disconnections (PayRoutes -MIPS) to Mastercard.  No actions were taken by Advanced Support, the connections auto resumed, and transactions started processing as expected.

• No issues identified on Paymentology application / infrastructure. Case is being logged with Mastercard to investigate the cause of the disconnection.",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,Wio,Auto resumed,Full outage of Mastercard transaction processing from 10:31 to 10:35 and 10:41 to 10:44 on 20th September. 2681 transactions impacted. ,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9009004551/2025-09-20+-+P1+-+Full+Outage+of+Transaction+Processing,2025-09-20T12:04:42.740Z,2025-09-20T12:04:42.740Z,,,,,2025-09-20T12:21:39.577Z,2025-09-20T02:31:00.000Z,2025-09-20T12:21:39.577Z,,,,,1016,2026-02-02T16:58:51.628Z,,public,883.3,666.7,176.7,39.9,20sep-p1-253-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09G8L7D6J2,Reported by Kaisar Mahmood,false
254,Full Outage of API Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 23 September 2025 at 01:30 UTC, the deployment team initiated a deployment of version BL 2.30 to the Shared OCI Environment (Tweeq). Shortly after the deployment, both PayAPI servers became inaccessible, leading to a full outage of API processing. However, transaction processing remained unaffected throughout the incident.

Initial investigation showed that the Docker container IP configuration conflicted with the jumpbox IP, causing the Docker instance to crash on startup. SD team, with inputs from Infra team, updated the Docker configuration and sequentially brought up both API servers. PayAPI 01 was restored at approximately 02:46 UTC and began processing requests. PayAPI 02 was then successfully restarted and confirmed to be handling live traffic by 02:59 UTC.

Further updates on root cause and corrective actions will be shared in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Sonia Seh,,Jenifer Cachero,Banking.Live,Tweeq,Docker configuration,Full outage of API Processing,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9011855547/2025-09-23+-+P1+-+Full+Outage+of+API+Processing,2025-09-23T02:11:33.837Z,2025-09-23T02:11:33.837Z,,,,,2025-09-23T03:19:55.844Z,,2025-09-23T03:19:55.844Z,,,,,4102,2025-09-23T05:32:04.643Z,,public,478.7,405.8,42,30.9,23sep-p1-254-full-outage-of-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09GKRJTUDQ,Reported by Vicnesh Baskar,false
255,Full Outage of Jetco Transaction Processing,Service Incident,Closed,P4,"**Issue**: No Jetco transactions processed for MOX since 21:26 UTC. Connectivity and key exchange were confirmed as working between JETCO and Paymentology.

**Impact**: All Jetco transactions for MOX were unavailable from 21:26 UTC until at least 05:24 HKT (21:24 UTC). The issue was not due to a system fault but rather very low transaction volume caused by Typhoon signal number 10 in Hong Kong.

**Causes**: Unknown

**Steps to resolve**: We confirmed successful connectivity and key exchange with JETCO. A test transaction from MOX was successfully processed, confirming the Jetco network is working as expected.",Daniel Velado,Daniel Velado,,,,Banking.Live,MOX,,,,https://teams.microsoft.com/meet/328920811553?p=wAqOSoFTcWMfYfNdsN,,,,,,,,,,,2025-09-23T22:03:00.890Z,2025-09-23T22:03:00.890Z,,,,,2025-09-23T22:29:05.949Z,,2025-09-23T22:29:05.949Z,,2025-09-23T23:34:36.213Z,,2025-09-23T23:34:36.213Z,1565,2025-09-23T23:34:36.344Z,,public,132.7,76.9,12.8,43,23sep-p4-255-full-outage-of-jetco-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09HL5275PS,Reported by Daniel Velado,true
256,ASDA 3DS Transaction Failures,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"Since August 2025, NOMO has reported thousands of declined zero-amount ASI transactions with SCA exemption (SLI 210). A feature flag originally intended to support this flow was enabled on July 30 but then disabled on August 5th due to recurring transaction issues. Despite turning off this feature, the zero-amount ASI declines still persist.

A permanent fix will be included in the upcoming BL 3.0 release scheduled for October 23. However, because this issue is affecting clients and has been escalated, a quick fix is being prepared. This hotfix is a simple one-line code change that will skip these zero-amount ASI transactions by default, which follows Mastercard’s rules and won’t impact recurring transactions.

Hotfix has been applied in **SH02-PROD -** [here](https://paymentology.slack.com/archives/C02CA1QPG5Q/p1758858655655939), and the activity completion mail has been sent to the client.
So far, we are not seeing any declined (Response code DE39=65.) for **ASI transactions** with **SLI indicator 210**",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,Nomo Bank,,"Declined zero-amount ASI transactions with SCA exemption (SLI 210), which should have been authorised under Mastercard rules.",Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9024798749/2025-09-25+-+P3+-+Critical+-+ASDA+3DS+Transaction+Failures,2025-09-25T13:55:55.218Z,2025-09-25T13:55:55.218Z,,,,,2025-09-26T13:14:44.211Z,,2025-09-25T13:56:57.905Z,2025-09-26T13:14:44.211Z,,,,83928,2026-02-02T16:59:45.825Z,,public,79.4,79.4,0,0,25sep-p3-critical-256-asda-3ds-transaction-failures,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09HHQQHG1F,Reported by Snothile Dlamini,false
257,Partial Outage of Visa Token Service (VTS) transactions,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"**Issue**: Expiry date and CVV validation were not applied to e-commerce Visa Token Service (VTS) transactions in the VE environment, allowing transactions with invalid CVVs to be approved. After restoring the validation flag, missing CVV2 result codes from Visa continued to cause errors.

**Impact**: Multiple GoTyme transactions with invalid CVVs were approved in the VoucherEngine platform. This exposed a security gap where transactions with incorrect security data were processed successfully, increasing the risk of fraud.

**Causes**: The feature (flag) toggle controlling expiry date and CVV validation was disabled, causing these checks to be bypassed for e-commerce Visa Token Service (VTS) transactions. After re-enabling the flag, an additional issue was found: ECOM VTS transactions were missing the CVV2 result code from Visa, which led to further validation errors.

**Steps to resolve**: We re-enabled the feature flag to restore expiry date and CVV validation. When a new issue was discovered with missing CVV2 result codes, we developed and deployed a hotfix so that validation proceeds.",Vicnesh Baskar,Vicnesh Baskar,,,,VoucherEngine,GoTyme,,Multiple transactions with invalid CVVs are getting approved,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9046458432/2025-09-26+-+P3+-+Critical+-+Partial+Outage+of+Visa+Token+Service+VTS+transactions,2025-09-26T05:34:49.409Z,2025-09-26T05:34:49.409Z,,,,,2025-09-26T20:44:56.161Z,,2025-09-26T15:47:52.023Z,2025-09-26T20:44:56.161Z,,,,54606,2026-02-02T17:00:06.701Z,,public,668.2,657.9,0.3,10,26sep-p3-critical-257-partial-outage-of-visa-token-service-vts-transactions,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09HE4AQFEY,Reported by Vicnesh Baskar,false
258,Full Outage of MC Transactions,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue**: Two PR socket disconnections for WIO/ADQ each lasted about 3 minutes and stopped all MC transaction processing during those times.

**Impact**: No Mastercard transactions for WIO/ADQ were processed during two disconnection periods (14:27:00–14:30:30 and 14:36:00–14:39:30), resulting in a temporary outage for MC transactions.

**Causes**: TBD

**Steps to resolve**: Issue auto-resolved without PTGY intervention",Daniel Velado,Daniel Velado,,,,Banking.Live,Wio,,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9039118384/2025-10-01+-+P1+-+Full+Outage+of+MC+Transactions,2025-10-01T15:45:00.475Z,2025-10-01T15:45:00.475Z,,,,,,,,,,,,,2026-02-02T17:01:08.471Z,,public,850.9,795.4,45.5,10,01oct-p1-258-full-outage-of-mc-transactions,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09J35G16SH,Reported by Daniel Velado,false
259,Partial Outage of API Processing ,Service Incident,Root Cause Analysis and Corrective Actions,P2,"Issue: Card creation API calls for Tyme were failing due to an expired key.

Impact: All card creation requests for Tyme failed while the key was expired. No cards could be created during this period.

Causes: An expired encryption key (cp_key) caused all card creation API requests to fail.

Steps to resolve: We updated the expired key, restarted related applications and the database, and confirmed card creation requests are processing successfully again.

Total Impacted card creation API calls:  500

Total Incident duration: 2 hours and 20 minutes.

Start time: 04 Oct 2025 08:43

End time: 04 Oct 2025 11:03",Daniel Velado,Daniel Velado,,,,Banking.Live,GoTyme,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9049768025/2025-10-04+-+P2+-+Partial+Outage+of+API+Processing,2025-10-04T11:00:01.198Z,2025-10-04T11:00:01.198Z,,,,,2025-10-04T11:17:28.614Z,,2025-10-04T11:17:28.614Z,,2025-10-05T23:52:19.123Z,,,1047,2026-02-02T17:01:24.659Z,,public,666,482.1,35.4,148.5,04oct-p2-259-partial-outage-of-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09KKDBJ9PA,Reported by Daniel Velado,false
260,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"**Issue**: Transaction processing on the Banking.Live platform was partially unavailable for D360 due to database performance issues.

**Impact**: D360 experienced a partial outage where VISA and MADA transactions were intermittently failing or delayed.

**Causes**: High CPU utilisation and resource contention on the Paycore database led to application-level errors and disrupted transaction processing.

**Steps to resolve**: We restarted the database, doubled CPU resources for Paycore and Paytok, and restarted application services. D360 re-enabled API services, and VISA and MADA transactions are now being approved with healthy CPU usage. We continue to monitor the situation.",Daniel Velado,Daniel Velado,,,,Banking.Live,D360,,,Paymentology,https://teams.microsoft.com/meet/3271413761068?p=HhQNpUNHTioDf2KCMv,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9049702586/2025-10-04+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-10-04T17:51:45.600Z,2025-10-04T17:51:45.600Z,,,,,2025-10-04T20:00:12.515Z,,2025-10-04T20:00:12.515Z,,2025-10-05T23:52:44.096Z,,,7706,2026-02-02T17:02:12.443Z,,public,1062.1,770.5,239.6,52,04oct-p1-260-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09JPTK0S4W,Reported by Daniel Velado,false
261,Full Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"A planned network change (**CCM-1281**) to migrate Azure ExpressRoute connectivity from Cisco routers to Palo Alto firewalls commenced at **23:00 UTC** on 07 October 2025. The initial stages of the change were successful — traffic failed over seamlessly to the backup VPN links while the primary ExpressRoute circuits were being reconfigured.

During reconfiguration, an issue was identified where new BGP connections could not be established. Despite a **P1 support case raised with Palo Alto**, no immediate resolution was identified. The troubleshooting extended beyond the planned window, and the **risk period was formally extended**, although at that stage there was **no outage or client impact**.

A decision was then made to roll back the change while keeping all production traffic running over the existing backup VPNs. The intention was to later complete the migration under a new change control, without any service disruption.

However, when the ExpressRoute routers were reconnected during the rollback, the **route advertisement mapping** (which restricts which routes each router is responsible for) was lost. As a result, the routers began advertising routes that should only have been handled by the firewalls. This misconfiguration led to **asymmetric routing and a loss of connectivity**, causing a **temporary full outage of transaction processing**.

Further investigation revealed that some clients, including **BoJ and ISB**, were also impacted despite not typically using the Pulsant Data Centre. This indicates an **unintended dependency** on the Pulsant routing path that had not been previously identified.

The issue was resolved after the **primary ExpressRoute links were taken down**, restoring stable routing over the secondary VPN. All services recovered successfully by **02:00 UTC**, with resiliency maintained through the **dual-link backup VPN**, which continues to provide redundancy.",Vicnesh Baskar,Vicnesh Baskar,,,Dominic Wee,Banking.Live,"Bank Al Etihad, Wio, Coverflex, Gnosis, Utility Warehouse, Vaultspay, Rain, Nomo Bank, Bank of Jordan, Islandsbanki, MOX, Mettle",,No impact to transaction processing at present,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9057501194/2025-10-08+-+P1+-+Full+Outage+of+Transaction+Processing,2025-10-08T00:47:26.098Z,2025-10-08T00:47:26.098Z,,,,,2025-10-08T03:33:26.029Z,,2025-10-08T03:33:26.029Z,,2025-10-08T07:35:01.101Z,,,9959,2025-10-08T07:35:01.592Z,,public,1531.7,1362.4,89.5,79.8,08oct-p1-261-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09K703KWMT,Reported by Vicnesh Baskar,false
262,Partial Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,"Transaction processing has remained stable (no spikes on the PayCore Database) since 09:57 UTC Thursday, 9th October. Draft RCA is under review by senior management, once approved it will be published to the client.",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,D360,,Partial outage of transaction processing ,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9073066088/2025-10-08+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-10-08T10:25:14.003Z,2025-10-08T10:25:14.003Z,,,,,2025-10-13T16:46:57.793Z,2025-10-08T08:28:00.000Z,2025-10-13T16:46:57.793Z,,,,,454903,2026-02-02T17:02:36.341Z,,public,1719.6,1590.6,97,32,08oct-p1-262-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09JXFH5JLF,Reported by Kaisar Mahmood,false
263,Partial outage of JETCO transaction processing ,Service Incident,Post-Incident Tasks & Documentation,P2,Jetco connection was re-established (key exchange performed / AS called JETCO) at 13:49 UTC 8th October. We do not see any time outs.,Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,MOX,Jetco connection was re-established (key exchange performed / AS called JETCO) at 13:49 UTC 8th October. We do not see any time outs.,"Since October 8th at 2am UTC we observe large amount of incoming Jetco cash withdrawal correction (CWC) messages -> out of ~950 cash withdrawals, ~350 of them were cancelled with CWC.",Paymentology,https://teams.microsoft.com/meet/3652134467017?p=aQx7mpOXWwQKfgi6pg,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9059139676/2025-10-08+-+P2+-+Partial+outage+of+JETCO+transaction+processing,2025-10-08T13:42:51.402Z,2025-10-08T13:42:51.402Z,,,,,2025-10-09T06:36:27.217Z,2025-10-08T09:00:00.000Z,2025-10-09T06:36:27.217Z,,,,,60815,2025-10-09T06:38:47.411Z,,public,341.2,323.7,0,17.5,08oct-p2-263-partial-outage-of-jetco-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09L8CWSXDW,Reported by Kaisar Mahmood,false
264,Partial Outage of Transactions Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"**Issue**: A partial outage caused some transactions to be declined intermittently due to a malfunctioning server in the processing cluster.

**Impact**: Some transactions for clients were intermittently declined between 2025-10-09 20:20 and 2025-10-10 04:08 UTC due to routing issues within one server in the processing cluster. Normal processing resumed after the affected server was removed from the cluster.

**Causes**: A server in the processing cluster (TRN-01) failed to load its configuration after being restarted during a deployment window. This caused the VTP service on that server to stop sending transaction traffic.

**Steps to resolve**: We removed the faulty TRN-01 server from the cluster at 04:08 UTC, which restored normal processing. The team is investigating the server's configuration and will apply a fix before safely returning it to service.",Vicnesh Baskar,Vicnesh Baskar,Pradeep Boopathe Rajeswari,,Ardiansyah Ar,VoucherEngine,Octopus,Removed the faulty TRN-01 server from the cluster,Some transactions are getting declined intermitternly,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9062612998/2025-10-10+-+P3+-+Critical+-+Partial+Outage+of+Transactions+Processing,2025-10-10T03:24:02.780Z,2025-10-10T03:24:02.780Z,,,,,2025-10-10T04:40:10.108Z,,2025-10-10T04:40:10.108Z,,,,,4567,2025-10-10T08:50:43.379Z,,public,2589.4,2401.8,80.7,106.9,10oct-p2-264-partial-outage-of-transactions-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09KNV7V4BF,Reported by Vicnesh Baskar,false
265,ATM Transaction Failures ,Service Incident,Closed,P3 - Non-Critical,Standard Bank deployed a fix at 21:00 SAST on 10th October to resolve the issue. Pending for formal RCA from SBSA.,Kaisar Mahmood,Kaisar Mahmood,,,,VoucherEngine,,,Standard Bank ATM’s requesting for 4 digit PIN. This should be 5 digits. Standard Bank recently implemented a 4-digit PIN requirement on some ATMs. This is resulting in increased declines. ,Third-Party - Client,https://teams.microsoft.com/meet/3539482844240?p=eHwj3QjR4jKDcw5pCl,,,,,,,,,,,2025-10-10T09:38:50.826Z,2025-10-10T09:38:50.826Z,,,,,2025-10-14T11:08:31.141Z,2025-10-08T09:35:00.000Z,2025-10-14T11:08:31.141Z,,,,2025-10-14T11:08:31.141Z,350980,2025-10-14T11:08:31.415Z,,public,315.9,257.4,58.5,0,10oct-p3-non-critical-265-atm-transaction-failures,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09L72BLX33,Reported by Kaisar Mahmood,false
266,Partial Outage of ATM Withdrawals,Service Incident,Closed,P4,"MC Case – 06931557. The declines were due to stand in issues on Mastercard Side,  as communicated in INC000000516496. Pending RCA from Mastercard.",Vicnesh Baskar,Vicnesh Baskar,,,,VoucherEngine,Mukuru,,,Third-Party - Paymentology,,,,,,,,,,,,2025-10-12T10:09:02.239Z,2025-10-12T10:09:02.239Z,,,,,2025-10-13T16:38:43.059Z,,2025-10-12T13:43:58.439Z,,,,2025-10-13T16:38:43.059Z,66580,2025-10-13T16:38:43.225Z,,public,821.4,707.7,102.6,11.1,12oct-p4-266-partial-outage-of-atm-withdrawals,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09L62U346Q,Reported by Vicnesh Baskar,false
267,Partial Outage of create_card API,Service Incident,Post-Incident Tasks & Documentation,P1,"On 11 Oct 2025 at 23:39 UTC, the support team was alerted about an issue where the client's card stack was empty since 22:47 UTC, 11th October, preventing the successful execution of create card API requests. Initial investigation showed that the card stack had depleted and was not automatically replenished as expected. The stack was manually increased at 09:50 UTC on 12 Oct 2025, and the stack fill job was configured to run hourly to prevent recurrence. Transaction processing normalised after 09:50 UTC with no further failures observed. The issue was resolved by manually replenishing the card stack and implementing automated stack replenishment scheduling. Investigation is ongoing on the root cause of the issue, and further details will be shared via the Root Cause Analysis Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Pradeep Boopathe Rajeswari,,Kamal Thapa,Banking.Live,D360,Manually replenishing the card stack and implementing automated stack replenishment scheduling,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9070247993/2025-10-12+-+P1+-+Full+Outage+of+create_card+API,2025-10-12T12:59:28.833Z,2025-10-12T12:59:28.833Z,,,,,,,,,,,,,2026-02-02T17:04:17.490Z,,public,437.5,384.4,43.1,10,12oct-p1-267-partial-outage-of-create_card-api,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09LFNC003T,Reported by Vicnesh Baskar,false
268,Partial Outage of Transactions Processing,Service Incident,Closed,P3 - Critical,"At 22:20 UTC on October 14, 2025, the support team was alerted to a spike in active connections on a Crimson node, which resulted in temporary transaction timeouts. The alerts auto-resolved shortly after being raised. Upon investigation, a blocking query in the databases was identified as the likely cause of the disruption.

The system has since stabilised, and transaction processing has returned to normal since 22:22 UTC. Heightened monitoring remains in place to ensure continued stability.",Vicnesh Baskar,Vicnesh Baskar,,,,VoucherEngine,"Albo, Ascend Group (True Money), GoTyme, Fondeadora/Mibo, Nelo, Ecocash",Auto resolved,Partial Outage of Transaction Processing,Paymentology,,Transaction Processing,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9081913405/2025-10-14+-+P3+-+Critical+-+Partial+Outage+of+Transactions+Processing,2025-10-14T23:36:27.743Z,2025-10-14T23:36:27.743Z,,,,,,,,,2025-10-14T23:39:32.745Z,,2025-10-14T23:39:32.745Z,,2025-10-15T13:46:44.751Z,,public,78.2,61.7,0,16.5,15oct-p3-critical-268-partial-outage-of-transactions-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09LJCUK1H8,Reported by Vicnesh Baskar,true
269,Service disruption on Mastercard File Express,Service Incident,Closed,P3 - Non-Critical,"On 14 October 2025 at 23:10 UTC, Mastercard issued a notification regarding a service disruption on the Mastercard File Express (MFE) platform, indicating customer access issues. This impacted both the Banking.Live (BL) and Voucher Engine (VE) platforms by delaying settlement file delivery and downstream reporting. Transaction processing was unaffected throughout the incident.
On BL, delayed receipt of T140 and T112 settlement files was observed across multiple endpoints. VE experienced similar delays, halting generation of daily settlement reports. All systems remained online and capable of processing files once received.
Following incident declaration, teams initiated impact analysis, monitored file flow, and validated internal readiness to resume processing. Continuous engagement was maintained with Mastercard via MC Connect and Mission Control, with repeated escalations to ensure prioritization of file recovery.
File delivery resumed progressively on 15 October. Once received, all settlement files were validated and processed. Faulty or incomplete files were manually retrieved and reloaded where required. Reporting processes were re-triggered on both platforms to ensure completeness. Duplicate handling logic and audit controls were closely monitored during recovery to prevent downstream inconsistencies.
The incident was formally resolved following Mastercard’s confirmation at 18:17 UTC on 15 October 2025. All delayed files were accounted for and processed successfully, with no residual impact to reporting or settlement systems.",Vicnesh Baskar,Vicnesh Baskar,,,Claudiu Stoica,Banking.Live,,Recovery of Mastercard MFE,Settlement processing and reporting will be delayed for all MC clients,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9084502038/2025-10-15+-+P3+-+Non-Critical+-+Service+disruption+on+Mastercard+File+Express,2025-10-15T04:28:15.303Z,2025-10-15T04:28:15.303Z,,,,,2025-10-16T03:39:00.449Z,,2025-10-16T03:39:00.449Z,,,,2025-10-16T03:39:00.449Z,83445,2025-10-16T07:44:47.651Z,,public,1733,1326.9,114.7,291.7,15oct-p3-non-critical-269-service-disruption-on-mastercard-file-express,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09LD2NEM6X,Reported by Vicnesh Baskar,false
270,Expired pre-authorisations not reversing after expiry date,Service Incident,Closed,P3 - Non-Critical,"Automated reversals for expired pre-authorisations were not processed for certain campaigns, leaving funds held for cardholders.

Cardholders in affected campaigns had funds held beyond the expected expiry period. The backlog reached over 30,000 missed reversals but has now been cleared, with the reversal queue reduced to 9 and all affected transactions processing.

A recent change required a non-NULL expiry date in the reversal script, which excluded over 30,000 valid expired pre-authorisations from being processed. Additionally, a static configuration list was missing entries, causing false settlement file alerts and blocking some reversals.

VE Team updated the reversal script to handle transactions without expiry dates and deployed a configuration fix for false settlement file alerts. Both fixes have been deployed, resulting in successful processing of all impacted reversals and resolution of false alerts.",Vicnesh Baskar,Vicnesh Baskar,,,Dominic Wee,VoucherEngine,,,,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9112813592/2025-10-16+-+P3+-+Non-Critical+-+Expired+pre-authorisations+not+reversing+after+expiry+date,2025-10-16T01:36:36.284Z,2025-10-16T01:36:36.284Z,,,,,2025-10-17T07:21:32.053Z,,2025-10-17T07:21:32.053Z,,,,2025-10-17T07:21:32.053Z,107095,2025-10-24T07:29:37.100Z,,public,1779.1,1513.9,191.5,73.6,16oct-p3-non-critical-270-expired-pre-authorisations-not-reversing-after-ex,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09M5QE9AMP,Reported by Vicnesh Baskar,false
271,Partial Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"Partial Outage of Transaction Processing was observed on PROD_VISA_3A and PROD_VISA_4B. Service was restored after removing transaction server (TT-AWS-P-TRN-01) from the cluster, restarting it, and adding it back. Since 18:47 UTC, transactions have been processing as expected.",Kaisar Mahmood,Kaisar Mahmood,,,,VoucherEngine,,,,Paymentology,   ________________________________________________________________________________         Microsoft Teams     Need help?            Join the meeting now            Meeting ID:      347 597 653 444            Passcode:      bA7P6Uw7                                                      ________________________________________________________________________________,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9109012484/2025-10-16+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-10-16T18:41:38.433Z,2025-10-16T18:41:38.433Z,,,,,2025-10-16T20:40:45.833Z,2025-10-16T17:00:00.000Z,2025-10-16T20:40:45.833Z,,,,,7147,2025-10-23T08:50:38.174Z,,public,174.8,95.7,79.1,0,16oct-p3-critical-271-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09MT6CEQNL,Reported by Kaisar Mahmood,false
272,Dynamic Block Functionality Failure,Service Incident,Post-Incident Tasks & Documentation,P1,"Following the BL 2.31 release, the dynamic block functionality failed due to a regression in the f_act_BlockCard function. This prevented tokens triggered by fraud rules from being inserted into the t_blocked_entities_toks table. As a result, fraud rules activated correctly, but the corresponding blocking actions were not applied, potentially allowing fraudulent transactions to proceed.

The regression introduced in BL 2.31 altered the f_act_BlockCard function, breaking the token insertion process for dynamic blocking. This issue impacted all environments running BL version 2.31 and above.

The fraud and SD team identified the faulty logic and implemented a hotfix. This fix was tested and deployed to all affected environments, including:

• ISB (194 cases)

• C24 Bank (9,548 cases)

• Union54 (522 cases)

Shared02, although not actively using dynamic blocking, received the hotfix proactively due to client readiness. All other environments were confirmed unaffected.

Post-deployment monitoring is ongoing to ensure the fix is effective and that normal dynamic blocking functionality is restored across all environments.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,"Islandsbanki, C24, Union54 / Zazu Zambia",,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9089450158/2025-10-17+-+P1+-+Dynamic+Block+Functionality+Failure,2025-10-17T12:47:52.716Z,2025-10-17T12:47:52.716Z,,,,,2025-10-17T15:31:24.636Z,2025-10-14T01:20:00.000Z,2025-10-17T12:49:44.398Z,2025-10-17T13:44:11.782Z,,,,9811,2025-10-17T15:32:17.275Z,,public,796.2,766.2,30,0,17oct-p1-272-dynamic-block-functionality-failure,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09M21G343Y,Reported by Snothile Dlamini,false
273,Partial Outage of Transaction Processing ,Service Incident,Closed,P3 - Critical,"All MOX settlements have processed successfully, with confirmation from both log analytics and database monitoring. No high database load or errors were observed during processing, though there were brief CPU and disk latency spikes that did not impact successful completion.
Client MOX has been updated via the [Zendesk ticket](https://paymentology.zendesk.com/agent/tickets/1044828 ""Zendesk ticket"").",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,MOX,,72 advisements from Mastercard with 7598 successfully processed Mastercard transaction from 00:53 - 1:36 AM – 20th Oct 2025,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9107406920/2025-10-20+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-10-20T02:49:41.500Z,2025-10-20T02:49:41.500Z,,,,,2025-10-23T07:29:16.890Z,2025-10-19T23:49:00.000Z,2025-10-23T07:29:16.890Z,,2025-10-23T07:30:18.000Z,,2025-10-23T07:30:18.000Z,275975,2026-02-02T10:09:15.350Z,,public,1858,1420.9,204.8,232.1,20oct-p3-critical-273-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09M7AJ6VEX,Reported by Kaisar Mahmood,true
274,Full Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue**: Transaction processing was completely unavailable for all clients listed below on the Banking.Live platform.

**Impact**: Clients were unable to process transactions from 05:00 to 05:28 UTC.
We received advice messages after 05:28 as they belonged to the transactions that were attempted before the rollback.
All transactions after 05:28 were processed as expected.

**Causes**: A deployment related to CCM-1618 went over the risk time, causing transaction processing to fail across all clients.

**Steps to resolve**: We rolled back the CCM-1618 deployment at 05:28 UTC, which restored transaction processing for all clients. All services are now fully operational.",Daniel Velado,Daniel Velado,,,,Banking.Live,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9098788868/2025-10-21+-+P1+-+Full+Outage+of+Transaction+Processing,2025-10-21T05:26:56.914Z,2025-10-21T05:26:56.914Z,,,,,2025-10-21T06:03:41.926Z,,2025-10-21T06:03:41.926Z,,,,,2205,2026-02-02T17:05:03.699Z,,public,963.8,710.6,88.7,164.5,21oct-p1-274-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09N0H2HEKB,Reported by Daniel Velado,false
275,TEST INCIDENT - PLEASE IGNORE - IGNORE,Service Incident,Closed,P4,IGNORE IGNORE IGNORE,Stephen Fleming,Stephen Fleming,,,,Banking.Live,,Test incident - closing,,,,,,,,,,,,,,2025-10-22T08:15:32.758Z,2025-10-22T08:15:32.758Z,,,,,2025-10-23T20:17:33.933Z,2025-10-22T08:15:00.000Z,2025-10-23T20:17:33.933Z,,,,2025-10-23T20:17:33.933Z,129721,2025-10-23T20:17:34.114Z,,public,58.3,48.3,10,0,22oct-p4-275-test-incident-please-ignore-ignore,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09MQ0W8QSF,Reported by Stephen Fleming,false
276,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"B.L. client BAE experienced a problem with Rule Engine between 18:13:18 and 18:17:41.
Declines - none
Approvals - 522
Advices - 77
Impact 12.85%",Daniel Velado,Daniel Velado,,,,Banking.Live,Bank of Jordan,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9110978580/2025-10-23+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-10-23T19:25:58.928Z,2025-10-23T19:25:58.928Z,,,,,,,,,,,,,2026-02-02T17:05:52.676Z,,public,173.5,172.7,0.8,0,23oct-p3-critical-276-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09P4CJRVGQ,Reported by Daniel Velado,false
277,Visa Certificate Renewal Issue  ,Service Incident,Closed,P4,"The Visa offline settlement certificate renewal has been completed successfully. There was no disruption to clearing, settlement, or refunds, and all client communications have been sent to confirm continued service stability. Settlement processing over the weekend was completed without any issues. Process improvements are underway, including the creation of a consolidated certificate register.",Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,,certificate renewal has been completed successfully,"Potential impact to end-of-day clearing, settlement, or refunds for a period until the certificate is renewed",Paymentology,,,,,,,,,,,,2025-10-24T10:29:30.686Z,2025-10-24T10:29:30.686Z,,,,,2025-10-27T09:49:07.523Z,2025-10-23T23:00:00.000Z,2025-10-27T09:49:07.523Z,,,,2025-10-27T09:49:07.523Z,256776,2025-10-27T09:49:07.742Z,,public,518.1,485.9,32.2,0,24oct-p4-277-visa-certificate-renewal-issue,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09P8QHLDR6,Reported by Kaisar Mahmood,false
278,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"At 11:03 UTC, connectivity between ACS and the host at 196.37.41.81:50045 was interrupted due to a network outage with NTT. This caused intermittent authorization declines for BINs 526722 and 533822.

The issue was initially reported through customer support calls and later confirmed by ACS, who saw connectivity beginning to restore by 11:14 UTC and fully restoring by 11:22 UTC.

After the connection was re-established, transaction approvals returned to normal with no ongoing impact. The root cause has been traced to the upstream outage from the NTT carrier. Further investigation into the root cause will be provided in the RCAR. ",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,,,,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9115238401/2025-10-24+-+P1+-+Transaction+Processing+Outage,2025-10-24T12:17:59.969Z,2025-10-24T12:17:59.969Z,,,,,2025-10-24T09:23:00.000Z,2025-10-24T09:03:00.000Z,,,,,,,2026-02-02T17:06:24.316Z,,public,197.9,192.1,5.8,0,24oct-p1-278-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09NF4CU4RG,Reported by Snothile Dlamini,false
280,Full Outage of Transaction and API Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"At 20:43 UTC on October 26, Paymentology's monitoring reported a disruption affecting transaction processing for WIO Bank. The root cause was a connectivity problem between PayRoute and the UAE switch due to an Azure tunnel disruption. Even though the tunnel later appeared connected, data wasn’t flowing back, which blocked authorizations and API calls from completing.

The system stayed on a backup network route, but an automatic switch back to the primary route didn’t happen as it should have. At 22:37 UTC, normal transaction processing and API operations resumed after WIO manually reset the tunnel from their end. Monitoring confirmed that transactions with Visa, Mastercard, and the UAE switch were back to normal and stable.

The incident was triggered by a partial tunnel failover and a delayed reconnection at the application level. Logs are now being gathered to analyze the event further. Next steps include updating recovery procedures and reviewing Azure tunnel settings with the client to prevent similar issues.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,Wio,,Transaction and API Processing ,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9120153606/2025-10-26+-+P1+-+Transaction+Processing+Outage,2025-10-26T22:16:21.610Z,2025-10-26T22:16:21.610Z,,,,,2025-10-26T22:54:35.612Z,,2025-10-26T22:54:35.612Z,,,,,2294,2026-02-02T17:06:44.921Z,,public,385.2,271.3,10.3,103.6,26oct-p1-280-full-outage-of-transaction-and-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09NVT14XJ8,Reported by Snothile Dlamini,false
281,ATM Non-Disbursement Outage,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"Mukuru reported that multiple cardholders are attempting ATM withdrawals, but funds are not being disbursed despite transactions showing as pending on the Voucher Engine (VE). The escalation originated from child ticket #1050966. Early indications suggest that transactions appear successful on VE, similar to a previous behavior observed in earlier incidents.",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,Mukuru,,,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9156853792/2025-10-27+-+P3+-+Critical+-+ATM+Non-Disbursement+Outage,2025-10-27T08:20:49.706Z,2025-10-27T08:20:49.706Z,,,,,2025-10-31T17:20:07.676Z,,2025-10-31T17:01:59.046Z,,,,,376869,2026-02-02T10:11:03.528Z,,public,1988.8,1811.7,124.6,52.5,27oct-p3-critical-281-atm-non-disbursement-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09NNAW1DC3,Reported by Snothile Dlamini,false
282,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Non-Critical,"On 28 October 2025 at 04:04 UTC, Monitoring team was alerted to intermittent disconnections affecting transaction processing with both VISA and Mastercard schemes. These disruptions caused a partial outage impacting authorisations.

The VISA PayRoute disconnection was traced to a BL 3.0 deployment on SHARED02, resulting in failed connections to PKS endpoints (10.22.2.25 and 10.22.2.26). Mastercard disconnections occurred separately and are still under investigation, with a support ticket raised with Mastercard for root cause analysis.

Transaction processing normalised by 04:26 UTC after automatic reconnections were established. Several shared and dedicated clients experienced an impact on multiple windows during this period. A detailed impact assessment is ongoing.",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Dominic Wee,Banking.Live,,,Partial Outage of Transaction Processing,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9128443907/2025-10-28+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-10-28T04:38:04.738Z,2025-10-28T04:38:04.738Z,,,,,2025-10-28T05:43:21.359Z,,2025-10-28T05:43:21.359Z,,,,,3916,2025-10-28T14:38:57.971Z,,public,715.2,672.9,32.4,10,28oct-p3-non-critical-282-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09PFAJVCKB,Reported by Vicnesh Baskar,false
283,MC settlement files not processed,Service Incident,Closed,P3 - Non-Critical,"MC settlement files for a client were not processed from October 27, 2025, onwards due to a change in the MFE file download directory. Files were being placed in the MFE root directory instead of the expected subdirectory. No FAST settlement messages were generated during this period, impacting downstream reconciliation processes.

The client has now acknowledged that

1. Backdated file processing (27–29 Oct 2025) can proceed. 

2. All impacted settlement records will be consolidated into a single presentment report. 

3. Upon processing, delayed FAST settlement messages will be generated and sent.

 The download paths for the T112 and T114 files have been updated in the MFE configuration to point to the correct subdirectories.

Service delivery has confirmed no unexpected access to the MFE server during the impacted period, and directory changes are now aligned with PS Task requirements. The client previously confirmed that most pending authorizations have been cleared once settlement records were sent.",Vicnesh Baskar,Vicnesh Baskar,,,Dominic Wee,Banking.Live,Xendit,,,,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9166389254/2025-10-29+-+P3+-+Non-Critical+-+MC+settlement+files+not+processed,2025-10-29T03:46:36.482Z,2025-10-29T03:46:36.482Z,,,,,2025-10-29T10:21:57.966Z,,2025-10-29T05:13:16.756Z,,,,2025-10-29T10:21:57.966Z,12920,2025-11-04T07:26:49.792Z,,public,1470.5,1115.8,161.6,193.2,29oct-p3-non-critical-283-mc-settlement-files-not-processed,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09P4UGD2BV,Reported by Vicnesh Baskar,false
284,Partial Outage of 3DS transactions,Service Incident,Closed,P3 - Critical,"On 22 October 2025 at 21:00 UTC, the technical teams completed a scheduled migration of a client to the Mastercard production environment using Bankserv’s ACS for 3D Secure (3DS) authentication. Initial testing confirmed successful 3DS flows.
However, by 26 October, the client reported a high rate of failed 3DS transactions. Investigation revealed a significant spike in Action Code 1022 errors, caused by IAV (Integrity Authentication Value) validation failures. The root cause was traced to a different ACS key being used in the Bankserv environment post-migration.
The key was corrected during a joint troubleshooting session with Bankserv on 29 October 2025, and the final 3DS failure with error 1022 occurred at 08:31 UTC. Close monitoring remains in place, and a detailed RCA and future corrective actions will follow in the RCAR.",Vicnesh Baskar,Vicnesh Baskar,,,Wafa Ben Nacef,VoucherEngine,Ecocash,,Partial Outage of 3DS transactions,Third-Party - Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_YTgwNWU5ODItOTRmZi00MDIzLTg3MTgtODQzZjc5NjI1MGYz%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%225bc8ffce-74fa-4e51-af76-275e9877517e%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9134637125/2025-10-29+-+P3+-+Critical+-+Partial+Outage+of+3DS+transactions,2025-10-29T07:48:47.126Z,2025-10-29T07:48:47.126Z,,,,,2025-10-29T09:13:41.234Z,,2025-10-29T09:13:41.234Z,,2025-10-29T09:36:04.400Z,,2025-10-29T09:36:04.400Z,5094,2025-10-29T09:39:03.346Z,,public,373.3,363.3,0,10,29oct-p3-critical-284-partial-outage-of-3ds-transactions,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09PADCQ4CE,Reported by Vicnesh Baskar,true
285,RTC - Transaction Processing Outage ,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"RTC transaction processing experienced a partial outage beginning at 20:30 UTC on 28 October 2025. The issue affected RTC transactions only; ATM and POS transactions continued to process normally.

We identified that connectivity was failing for the MIP environment connected via Vodacom MPLS, while the Teraco Fiberlink environment remained accessible. The networking issue was traced to a source IP configuration change on Access Bank’s side. After corrective action from Access Bank's networking team, connectivity was restored to both MIP instances.

RTC processing is now fully operational over the MPLS link. Access Bank will conduct an internal investigation and provide a root cause analysis. A detailed impact assessment will follow.",Snothile Dlamini,Snothile Dlamini,,,Tshegofatso Manakana,Acorn / FlexPay,,,,Third-Party - Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9134866517/2025-10-29+-+P2+-+RTC+-+Transaction+Processing+Outage,2025-10-29T08:03:51.676Z,2025-10-29T08:03:51.676Z,,,,,2025-10-29T08:31:04.152Z,,2025-10-29T08:31:04.152Z,,,,,1632,2025-10-30T07:31:02.986Z,,public,121.6,121.6,0,0,29oct-p3-critical-285-rtc-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09P65R2SAX,Reported by Snothile Dlamini,false
286,OCI VPN Connectivity Failure,Service Incident,Post-Incident Tasks & Documentation,P1,"At 07:29 UTC, Paymentology identified a partial outage caused by a connectivity problem within OCI’s Saudi region. This issue led to a loss of VPN access, which temporarily affected the visibility and authorization of transactions for clients Derayah and Tweeq. Fortunately, Derayah’s MADA transactions continued without interruption. However, Tweeq’s Mastercard transactions experienced a brief disruption.

By approximately 08:17 UTC, OCI had restored connectivity, and all systems, including Visa and Mastercard processing, quickly returned to normal. The database and service delivery teams confirmed that full recovery and data integrity were maintained. The incident has now been resolved, and ongoing monitoring shows stable operations across all environments. OCI’s confirmatory root cause analysis is still pending and will be included in the final RCA report.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,"D360, Tweeq",,,Third-Party - Paymentology,,,,,,,,,"D360, TWEEQ",,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9141583897/2025-10-30+-+P1+-+OCI+VPN+Connectivity+Failure,2025-10-30T07:55:25.936Z,2025-10-30T07:55:25.936Z,,,,,2025-10-30T08:47:08.377Z,,2025-10-30T08:47:08.377Z,,,,,3102,2025-10-30T08:47:26.892Z,,public,505.4,505.4,0,0,30oct-p1-286-oci-vpn-connectivity-failure,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09PKH5H6T0,Reported by Snothile Dlamini,false
287,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On 30 October 2025 at 22:44 UTC, an alert was triggered indicating a disconnection on one of the PayRoutes. Further investigation identified multiple disconnection and reconnection windows across PayRoutes connected to VISA. These events occurred between 22:44 UTC and 22:51 UTC and were reconnected automatically without any manual intervention.

Internal teams are continuing to investigate the root cause of the disconnections, and further details will be shared once available.",Vicnesh Baskar,Vicnesh Baskar,Sonia Seh,,Jenifer Cachero,Banking.Live,"Bank Al Etihad, ONMO, Wio",,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9147121682/2025-10-31+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-10-31T03:07:50.961Z,2025-10-31T03:07:50.961Z,,,,,,,,,,,2025-10-31T03:07:50.961Z,,2026-02-02T17:08:02.589Z,,public,188.1,172.9,1.3,13.9,31oct-p3-critical-287-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09PVDX2398,Reported by Vicnesh Baskar,false
288,Partial Outage of MC Transaction Processing,Service Incident,Closed,P4,"MC B.L. clients were not processing transactions on MIP OOB due to MC maintenance.
Traffic was routed through 4IR MIP so no transactions were lost.",Daniel Velado,Daniel Velado,,,,Banking.Live,,,,,https://teams.microsoft.com/meet/32028047045415?p=KURsocUkJXPtaeNwDk,,,,,,,,,,,2025-11-01T06:48:26.374Z,2025-11-01T06:48:26.374Z,,,,,2025-11-01T08:11:22.642Z,,2025-11-01T08:11:22.642Z,,,,2025-11-01T08:11:22.642Z,4976,2025-11-01T08:11:22.796Z,,public,315.7,230.9,10.1,74.7,01nov-p4-288-partial-outage-of-mc-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09QGUWNMBK,Reported by Daniel Velado,false
289,Partial Outage of MC Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"One of the PR server was not available due to the memory issue for client WIO/ADQ between 06:01 to 06:13 UTC 

Affected Transaction count : 0784.
Succesfully processed transactions  : 1315.
Root cause :",Daniel Velado,Daniel Velado,,,,Banking.Live,Wio,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9154691079/2025-11-02+-+P1+-+Partial+Outage+of+MC+Transaction+Processing,2025-11-02T07:48:18.982Z,2025-11-02T07:48:18.982Z,,,,,,,,,,,,,2026-02-02T17:09:05.604Z,,public,536.4,412.1,23.5,100.7,02nov-p1-289-partial-outage-of-mc-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09Q6001M2S,Reported by Daniel Velado,false
290,Full Outage of Bancnet Transactions,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue**: PIN format errors at the PROD_GOTY_6B and 6C endpoints caused all Bancnet transactions to  decline after Bancnet performed system maintenance.

**Impact**: GoTyme customers experienced intermittent declines on Bancnet transactions from approximately 16:15 UTC until the issue was resolved. Only Bancnet transactions were affected; other transaction types were not impacted.

**Causes**: A key management issue on the Bancnet side after their maintenance introduced PIN format errors, which caused transaction declines.

**Steps to resolve**: We worked with Bancnet to perform a new key exchange, which resolved the PIN errors. Bancnet transactions are now processing normally and the declines for GoTyme have stopped.",Daniel Velado,Daniel Velado,,,,VoucherEngine,GoTyme,,,Third-Party - Client,https://teams.microsoft.com/meet/3173126286061?p=6IKwB4Qes1SqZVaus7,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9161441297/2025-11-03+-+P2+-+Partial+Outage+of+Bancnet+Transactions,2025-11-03T18:39:07.864Z,2025-11-03T18:39:07.864Z,,,,,2025-11-03T19:44:14.410Z,,2025-11-03T18:52:15.709Z,2025-11-03T19:44:14.410Z,,,,3906,2026-02-01T19:46:25.222Z,,public,332.1,320.3,1.8,10,03nov-p1-290-full-outage-of-bancnet-transactions,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09QH021Y6N,Reported by Daniel Velado,false
291,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P2,"On 04 Nov 2025 at 23:12 UTC, the support team was alerted about multiple Crimson alerts indicating increased error rates and elevated latencies across several nodes. Initial investigation revealed that active connections and overall transaction volumes appeared normal; however, some specific nodes exhibited elevated error or latency levels.

By 23:16 UTC, transaction processing had stabilised across all affected nodes without any manual intervention. An impact analysis and a deep dive into the potential root cause are currently underway, alongside close monitoring of the impacted nodes.",Vicnesh Baskar,Vicnesh Baskar,Heru Nugroho,,Ardiansyah Ar,VoucherEngine,"Agoda, Albo, DolarApp, Fondeadora/Mibo, Vodafone Fiji, Punto Pago, Quicko, Nelo",,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9174548495/2025-11-05+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-11-05T00:14:07.730Z,2025-11-05T00:14:07.730Z,,,,,,,,,2025-11-05T05:30:41.775Z,,,,2026-02-02T17:09:52.164Z,,public,335.5,255,6.7,73.7,05nov-p2-291-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09QXRD2QQ4,Reported by Vicnesh Baskar,false
292,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 05 November 2025 at 00:36 UTC, the support team was alerted about declines due to rule engine delays. Transaction processing normalised by 00:38 UTC. No anomalies were found in the infra or application layer during this window, and relevant teams have been engaged for further investigation. Further updates will be shared in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Huy Pham,Banking.Live,Mettle,"No manual intervention, self normalized",,Paymentology,,,,,,,,,METTLE,Dedicated,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9174253601/2025-11-05+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-11-05T04:01:02.445Z,2025-11-05T04:01:02.445Z,,,,,,,,,2025-11-05T05:31:00.784Z,,,,2025-11-05T05:31:01.204Z,,public,606.1,557.7,28.4,20,05nov-p1-292-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09QRRZAEJZ,Reported by Vicnesh Baskar,false
293,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On 05 November 2025 at 22:20 UTC, the support team was alerted about high latency on multiple hosts, impacting transaction processing. Initial investigation showed a spike in active connections and a database blocker query during the incident window. The issue was auto-resolved at 22:22 UTC, and transaction processing normalised thereafter with no further latency alerts observed. 

The issue was resolved automatically without any manual intervention. Further investigation has been initiated, and updates on the root cause will be provided once available.",Vicnesh Baskar,Vicnesh Baskar,Frendi Muhamad,,Ardiansyah Ar,VoucherEngine,"ASAP, Agoda, Albo, Ascend Group (True Money), Punto Pago, Mixx by Yas, Salaam Bank - Somalia, DolarApp, Fondeadora/Mibo, GoTyme, Inswitch, Jeeves, Nelo, Panelys, Paycard Guinea, Octopus, Orange CIV, Quicko, Safaricom, Vodacom DRC, Wave Ivory Coast",The issue was resolved automatically without any manual intervention,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9183461377/2025-11-06+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-11-06T06:36:13.684Z,2025-11-06T06:36:13.684Z,,,,,,,,,2025-11-06T07:09:22.439Z,,2025-11-06T07:09:22.439Z,,2025-11-06T07:15:03.307Z,,public,166.2,166.2,0,0,06nov-p3-critical-293-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09QYJRQ69H,Reported by Vicnesh Baskar,true
294,SharedEU – MC Settlement Matching Performance Degradation,Service Incident,Closed,P3 - Non-Critical,"The SharedEU Mastercard Settlement Matching process slowed down significantly, matching 500 settlements took over 4 minutes instead of the usual 10 seconds. 

Initial checks showed that no recent deployments or application changes caused the problem. Monitoring the databases revealed a big delay in replication between the master and replica databases. AS team restarted payscheduler, and the matching performance returned to normal.

No transaction processing was affected , the slow down only impacted the settlement matching job. Root cause investigation is still ongoing",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,C24,Restarting Payschedular,"Settlement matching process is running but significantly slower than normal, causing delays in transaction reconciliation.",TBD,https://teams.microsoft.com/meet/39647758533064?p=719P23Dl82g9QPrLIt,,,,,,,,,,,2025-11-06T13:40:15.017Z,2025-11-06T13:40:15.017Z,,,,,2025-11-06T14:40:45.589Z,2025-11-06T13:15:00.000Z,2025-11-06T14:40:45.589Z,,,,2025-11-06T14:40:45.589Z,3630,2025-11-06T14:40:45.814Z,,public,166.8,166.8,0,0,06nov-p3-non-critical-294-sharedeu-mc-settlement-matching-performance-degra,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09QS5FBXMM,Reported by Snothile Dlamini,false
295,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 08-Nov-2025 at 02:45 UTC, the support team was alerted about intermittent declines for MADA transactions due to a failure in the rules engine processing on NGINX. Initial investigation showed that NGINX encountered errors related to JDBC connection failures and rules engine timeouts. The PayPower component was restarted, but issues persisted. The team observed database-side query deadlocks and multiple connections in a waiting state. The long-running and blocking queries were terminated at 06:38 UTC, after which no further errors were observed, and transaction processing normalised. The issue was resolved by identifying and terminating problematic database queries that caused deadlocks. Root cause investigation is ongoing, and further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Pradeep Boopathe Rajeswari,,Kamal Thapa,Banking.Live,D360,Long-running and blocking queries were terminated,,Paymentology,,,,,,,,,D360,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9198567472/2025-11-08+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-11-08T06:17:40.904Z,2025-11-08T06:17:40.904Z,,,,,2025-11-08T06:55:21.449Z,2025-11-08T06:16:00.000Z,2025-11-08T06:55:21.449Z,,,,,2260,2025-11-10T08:45:54.352Z,,public,504.8,450.2,25.8,28.8,08nov-p1-295-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09RMQYT2RY,Reported by Vicnesh Baskar,false
296,Full Outage of JONET Transaction Processing,Service Incident,Closed,P4,"A full outage affecting JONET transaction processing for Bank of Jordan occurred between 15:38 UTC and 16:58 UTC on 08-Nov-2025, during which no JONET transactions were received. Mastercard and FAST transactions remained unaffected.

Multiple restarts of PayRoute and PayPower were carried out with no resolution. API connectivity to the client and FAST remained stable, and network checks revealed no internal issues. The client was notified and later confirmed that the issue was on their side. Transaction processing resumed at 16:58 UTC following their intervention.",Vicnesh Baskar,Vicnesh Baskar,Mauro Merino,,Kevin Valle,Banking.Live,Bank of Jordan,Client resolution,,Client,,,,,,,,,BOJ,,,2025-11-08T16:35:16.753Z,2025-11-08T16:35:16.753Z,,,,,2025-11-08T17:10:56.544Z,,2025-11-08T17:10:56.544Z,,,,2025-11-08T17:10:56.544Z,2139,2025-11-08T17:10:56.787Z,,public,169.5,94.1,30,45.4,08nov-p4-296-full-outage-of-jonet-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09RPCVA4RY,Reported by Vicnesh Baskar,false
297,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On 08 November 2025 at 23:30 UTC, the monitoring team was alerted about high XML-RPC latency across multiple nodes, causing latency and intermittent error 9111 responses on several nodes.

The latency was reduced by 23:36 UTC without any manual intervention, and transaction processing normalised thereafter. Further investigation showed a spike in the database, and the DB team has been engaged for a root cause analysis.",Vicnesh Baskar,Vicnesh Baskar,Heru Nugroho,,Jenifer Cachero,VoucherEngine,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9199255553/2025-11-09+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-11-09T02:22:36.276Z,2025-11-09T02:22:36.276Z,,,,,,,,,,,2025-11-09T02:22:36.276Z,,2025-11-10T06:15:33.612Z,,public,91.2,91.2,0,0,09nov-p3-critical-297-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09RUFAULE8,Reported by Vicnesh Baskar,false
298,Full Outage Of Bancnet Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 09 November 2025 at 21:24 UTC, the support team was alerted about a connectivity issue with Bancnet. Initial investigation showed that both nodes to Bancnet (PROD_GOTY_6B and PROD_GOTY_6CB) had lost connectivity, resulting in a full outage of Bancnet transaction processing.

The VPN tunnels were confirmed down while DirectConnect remained operational. Bancnet was engaged and later advised to failover to the backup VPN (IPSec tunnel). The failover was completed by Paymentology at 23:14 UTC, but traffic did not resume until new endpoint IPs were received and configuration changes were applied by Bancnet.

At 00:04 UTC on 10 November, transaction processing normalised with no further disconnections.  The disconnection is suspected due to a local network provider outage. Root cause investigation is underway by both Bancnet & Paymentology, and further updates on the root cause and preventive actions will be provided in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Siglinda Martinez,,Wilson Keneshiro,VoucherEngine,GoTyme,,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9199124509/2025-11-09+-+P1+-+Full+Outage+Of+Bancnet+Transaction+Processing,2025-11-09T22:26:31.642Z,2025-11-09T22:26:31.642Z,,,,,2025-11-10T00:40:27.378Z,,2025-11-10T00:40:27.378Z,,2025-11-10T06:25:32.062Z,,,8035,2026-02-01T19:38:43.948Z,,public,1364.8,948,119.3,297.6,09nov-p1-298-full-outage-of-bancnet-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09RXGJPU0L,Reported by Vicnesh Baskar,false
299,VISA Transactions Not Processing - Shared UAE,Service Incident,Post-Incident Tasks & Documentation,P1,"Starting at 03:00 UTC on 12 November 2025, VISA transactions stopped processing in the Shared UAE environment. This outage impacted Huru and Ziina. Datadog showed repeated “no response from PP” errors in the PayRoute logs (specifically payroute-visa-sharedeu-multiple-6502). PagerDuty alerts reported socket timeouts and disconnects on servers payroute-sh-uae-switch-01 and pt-shareduae-proute01.paymentology.org. AS team confirmed that the PayRoute system was not getting responses from the PayPower (PP) service. After restarting the PayPower service, VISA transaction processing returned to normal.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,"Ziina, Huru",,No VISA transactions were processed on the Shared UAE environment from 03:00 UTC until the issue was resolved.,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9214885891/2025-11-12+-+P1+-+VISA+Transactions+Not+Processing+-+Shared+UAE,2025-11-12T08:35:36.448Z,2025-11-12T08:35:36.448Z,,,,,2025-11-12T08:36:34.072Z,2025-11-12T01:00:00.000Z,2025-11-12T08:36:34.072Z,,,,,57,2025-11-12T08:48:31.620Z,,public,861.1,785,66.1,10,12nov-p1-299-visa-transactions-not-processing-shared-uae,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09SS8X3X25,Reported by Snothile Dlamini,false
300,Full Outage of MADA Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,"MADA transaction processing experienced a full outage from 04:40 to 05:10 UTC, impacting all transactions for the client. Visa transactions were not affected.

The outage was caused by database deadlocks, leading to 'failed to obtain JDBC Connection' errors. The DBA team resolved the issue by terminating the problematic database sessions, and no further errors have been observed since 05:10 UTC.

MADA transaction processing has remained stable, with enhanced monitoring now in place. Manual log monitoring is ongoing using Datadog queries for both deadlock detection and JDBC connection failures. Additional alerting improvements are being implemented.

Root cause analysis is currently underway.",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Huy Pham,Banking.Live,D360,,Full Outage of Mada Transactions – Visa transactions are not impacted and continue to process as expected.,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9213903062/2025-11-13+-+P1+-+Full+Outage+of+MADA+Transaction+Processing,2025-11-13T05:12:05.272Z,2025-11-13T05:12:05.272Z,,,,,2025-11-13T05:46:34.577Z,,2025-11-13T05:46:34.577Z,,,,,2069,2026-02-01T19:36:31.557Z,,public,948.6,778.6,92.1,77.9,13nov-p1-300-full-outage-of-mada-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09SHRSSBS9,Reported by Vicnesh Baskar,false
301,Full Outage of Transactions Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On **13 November 2025** at **05:25 UTC**, the engineering team was alerted via monitoring that ISB transaction processing had completely stopped. Investigation showed that an `OutOfMemoryError` occurred on the PayPower application following the deployment of ISB BL Release 3.0.20-hf.2 at 04:00 UTC.

A restart of all PayPower instances was completed by **05:51 UTC**, restoring processing. At **06:10 UTC**, additional memory errors were seen; however, proactive restarts prevented further transaction loss. A validated hotfix (3.0.20-hf.4) was deployed sequentially across all nodes without disrupting transaction flow. Post-deployment monitoring confirmed stable operations.",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Abdul Sahaptheen,Banking.Live,Islandsbanki,Hot fix deployment,Full Outage of Transactions Processing,Paymentology,,,,,,,,,ISB,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9219309580/2025-11-13+-+P1+-+Full+Outage+of+Transactions+Processing,2025-11-13T05:52:15.250Z,2025-11-13T05:52:15.250Z,,,,,2025-11-13T07:00:44.153Z,,2025-11-13T06:05:39.095Z,,,,,4108,2025-11-13T08:04:52.521Z,,public,380.4,350.4,0,30,13nov-p1-301-full-outage-of-transactions-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09SKDGFR62,Reported by Vicnesh Baskar,false
303,Daily Net settlement VSS file and Daily settlement file variances. ,Service Incident,Investigating,P3 - Non-Critical,"Safaricom settlement variance issue - Daily settlement file missing transactions that appear in VSS summary file, blocking bank settlement with ABSA threatening midday cutoff",Kaisar Mahmood,Kaisar Mahmood,,,,VoucherEngine,Safaricom,,"Safaricom settlement variance issue - Daily settlement file missing transactions that appear in VSS summary file, blocking bank settlement with ABSA threatening midday cutoff",Third-Party - Client,,,,,,,,,,,,2025-11-13T09:58:53.963Z,2025-11-13T09:58:53.963Z,,,,,,,,,,,,,2026-02-02T17:14:56.751Z,,public,1192.8,1128.4,22.6,41.8,13nov-p3-non-critical-303-daily-net-settlement-vss-file-and-daily-settlemen,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09SSDC9XQU,Reported by Kaisar Mahmood,false
304,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On 17-Nov-2025 at 21:45 UTC, the support team was alerted on a high database latency on the Paycore DB. Initial investigation showed transaction declines caused by delays in the rule engine’s response time.

The disk latency subsided by 21:48 UTC, and transaction processing normalised shortly after. No further transaction declines were observed post-incident window. The issue was resolved without requiring any manual intervention. The DBA team has been engaged for further root cause analysis, and further updates on the root cause and any preventive actions will be provided once available.",Vicnesh Baskar,Vicnesh Baskar,Sonia Seh,,Jenifer Cachero,Banking.Live,Bank Al Etihad,The issue was resolved without requiring any manual intervention,Partial Outage of Transaction Processing,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9243295859/2025-11-17+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-11-17T23:58:12.123Z,2025-11-17T23:58:12.123Z,,,,,,,,,,,2025-11-17T23:58:12.123Z,,2025-11-18T00:08:58.192Z,,public,85.5,74,0,11.5,17nov-p3-critical-304-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09TFLUSP9T,Reported by Vicnesh Baskar,false
305,Intermittent Rule Engine Query Timeouts,Service Incident,Fixing,P3 - Critical,"Intermittent long-running Rule Engine queries are occurring on the database side, leading to recurring transaction timeouts. The DB team has confirmed no DB-level remediation options aside from increasing resources. This behaviour matches a previously reported defect (DEFECT-4199) linked to card-on-file and zero-amount transactions. Development has confirmed the fix (DEC-1022) is in progress. The ongoing timeouts continue to cause isolated transaction failures.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,MOX,,,Paymentology,,,,,,,,,,,,2025-11-18T09:28:40.358Z,2025-11-18T09:28:40.358Z,,,,,,,2025-11-18T09:29:09.283Z,,,,,,2026-02-01T19:34:16.251Z,,public,442.9,401.8,31.1,10,18nov-p3-critical-305-intermittent-rule-engine-query-timeouts,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09TKTBMJGN,Reported by Snothile Dlamini,false
306,Transaction Processing Outage,Service Incident,Post-Incident Tasks & Documentation,P1,"We've confirmed that the Amsterdam MIP was unavailable between 13:35 and 14:05, during which not all transactions were successfully rerouted to the London MIP. ISB and MOX clients were affected, with 557 and 732 impacted transactions respectively, according to Datadog logs.

London MIP continued processing transactions during this period, and Paymentology's systems and network were not the source of the issue. The matter has been escalated to Mastercard under case 07111522 for further investigation.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,"MOX, Islandsbanki",,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9249390623/2025-11-18+-+P1+-+Transaction+Processing+Outage,2025-11-18T13:54:45.762Z,2025-11-18T13:54:45.762Z,,,,,2025-11-18T14:55:46.092Z,,2025-11-18T14:55:46.092Z,,,,,3660,2026-02-01T19:33:16.420Z,,public,679,498.5,170.5,10,18nov-p1-306-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09TPLN9JAE,Reported by Snothile Dlamini,false
307,Full Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 20 November 2025 at 01:29 UTC, the support team was alerted about a socket disconnection on PayRoute-AFS. Initial investigation showed that the PayRoute system experienced an AFS Remote socket closed error, followed by repeated connection refused errors.

PayRoute attempted auto-reconnection starting at 01:29:47 UTC but was continuously refused until 01:37:22 UTC. Transaction processing normalised after automatic reconnection at 01:37 UTC, with no further disconnection observed.

A support ticket has been raised with AFS for further investigation. Additional findings and any preventive measures will be detailed in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Pradeep Boopathe Rajeswari,,Dominic Wee,Banking.Live,Dopay LLC,No manual intervention,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9260007463/2025-11-20+-+P1+-+Full+Outage+of+Transaction+Processing,2025-11-20T03:47:05.170Z,2025-11-20T03:47:05.170Z,,,,,,,,,2025-11-20T05:31:27.930Z,,,,2025-11-20T05:31:28.311Z,,public,230.9,160.7,20,50.2,20nov-p1-307-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09U2EECWCW,Reported by Vicnesh Baskar,false
308,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P4,Currently investigating partial declines on D360 due to High CPU usage ,Daniel Velado,Daniel Velado,,,,Banking.Live,D360,,,,https://teams.microsoft.com/meet/38474760699836?p=gAnnngr16QBg9o7Eie,,,,,,,,,,,2025-11-20T17:16:15.270Z,2025-11-20T17:16:15.270Z,,,,,2025-11-20T18:17:27.715Z,,2025-11-20T18:17:27.715Z,,,,,3672,2025-11-21T02:03:56.128Z,,public,321.1,254.3,66.8,0,20nov-p4-308-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09U66BC4LD,Reported by Daniel Velado,false
309,Full Outage of ATM Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 21 November 2025 at 00:25 UTC, the support team was alerted by Bancnet on ATM port disconnections and timeouts. Initial investigation showed that the connection from AWS to Bancnet was down, consistent with a previous issue involving PLDT circuits. The system was switched over to a secondary VPN connection at 01:14 UTC, and transaction processing normalised after 01:15 UTC with no further issues.

A PLDT network outage was later confirmed, and traffic will remain on VPN and PLDT confirms resolution. Close monitoring is in place, and bot PLDT and Bancnet remain engaged for the recovery of the primary link.",Vicnesh Baskar,Vicnesh Baskar,Maureen Figueroa,,Dominic Wee,VoucherEngine,GoTyme,Failover to secondary link via VPN,Full Outage of ATM Transaction Processing,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9270591489/2025-11-21+-+P1+-+Full+Outage+of+ATM+Transaction+Processing,2025-11-21T00:50:51.016Z,2025-11-21T00:50:51.016Z,,,,,2025-11-21T02:29:42.615Z,,2025-11-21T01:29:52.018Z,,,,,5931,2025-11-21T07:49:04.808Z,,public,600.1,504.9,30.2,65,21nov-p1-309-full-outage-of-atm-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09UG5V5ERJ,Reported by Vicnesh Baskar,false
310,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On 21 November 2025 at 05:33 UTC, the Support team was alerted about delays in transaction processing due to an increase in zero-value TVN (Token Validation Notification) messages.
Initial investigation showed that legacy products, whose TVNs were not previously disabled, were generating zero-value messages that delayed the Rule Engine and partially degraded transaction throughput.
Transaction processing normalised at 05:34 UTC after the zero-value TVNs subsided. A backend update was completed to bypass Rule Engine validation for TVN messages, ensuring the issue does not reoccur.",Vicnesh Baskar,Vicnesh Baskar,Pradeep Boopathe Rajeswari,,Rajesh Subramanian,Banking.Live,MOX,Bypassed Rule Engine validation for TVN messages,Some transactions failed due to delay in the Rule Engine caused bu TVN transactions,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9269346323/2025-11-21+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-11-21T06:41:52.992Z,2025-11-21T06:41:52.992Z,,,,,2025-11-21T07:38:36.500Z,,2025-11-21T07:38:36.500Z,,2025-11-21T07:38:54.123Z,,2025-11-21T07:38:54.123Z,3403,2025-11-21T07:40:41.018Z,,public,255.5,235.5,10,10,21nov-p3-critical-310-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09UC3VUHB8,Reported by Vicnesh Baskar,true
311,Transaction Processing Outage,Service Incident,Post-Incident Tasks & Documentation,P1,"On 21 November 2025, a partial outage affected MADA ecommerce transactions. The root cause was traced to a single MADA PayPower instance running in a non-dockerised environment. This instance was down and unable to start due to a port conflict, leading to intermittent transaction declines routed through it. All other PayPower instances and Visa processing paths remained healthy throughout.

The team carried out multiple restart attempts, but the service persistently bound to the wrong port. To resolve this, the affected instance was fully dockerised, which allowed it to come back online and process transactions normally. Additionally, another PayPower service was discovered running but not processing transactions, this was restarted successfully as well.

Following these actions, no further errors were observed from 11:11 UTC, and all MADA and Visa transaction flows stabilised. Monitoring continues, and a follow-up JIRA ticket will be opened with the Authorisation team to investigate the unusual port configuration issue seen on the non-dockerised instance.

The service is now fully restored and stable, with no additional impact reported.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,D360,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9270951960/2025-11-21+-+P1+-+Transaction+Processing+Outage,2025-11-21T10:20:41.555Z,2025-11-21T10:20:41.555Z,,,,,2025-11-21T11:57:33.742Z,,2025-11-21T11:57:33.742Z,,,,,5812,2025-11-21T11:58:09.342Z,,public,967.1,895.3,71.8,0,21nov-p1-311-transaction-processing-outage,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09U8VC6CJX,Reported by Snothile Dlamini,false
312,Full Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"The issue affecting transaction processing has been resolved. 4 of the 5 TRN servers were restarted during the recovery, and error rates began dropping as soon as the first server came back online. All 4 active servers are stable, and the remaining server is being reviewed separately.
The investigation confirmed that the underlying cause was database slowness, which led to transaction processing delays. As transactions exceeded the 8-second timeout, this created a domino effect where more threads began waiting, increasing contention and triggering multiple alerts across the platform. Restarting the TRN servers cleared the accumulated threads and allowed processing to normalise.
Voucher Engine and Fusion logs on the isolated node show no further issues. The platform is now processing transactions as expected. Further details will be shared in the RCAR.",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9281667283/2025-11-24+-+P1+-+Full+Outage+of+Transaction+Processing,2025-11-24T14:02:31.462Z,2025-11-24T14:02:31.462Z,,,,,2025-11-24T14:56:59.581Z,,2025-11-24T14:56:59.581Z,,,,,3268,2026-02-01T19:24:41.039Z,,public,854.5,768,76.5,10,24nov-p1-312-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09UUTZHLB0,Reported by Snothile Dlamini,false
313,Partial Outage of Transaction Processing,Service Incident,Closed,P4,"On 27 Nov 2025 at 18:05 UTC, the Infrastructure and Application Support teams were alerted about intermittent transaction processing issues. Initial investigation showed Rule Engine errors and high CPU usage on PayCore DB. Further investigation showed that the high CPU usage on the Paycore database was triggered by the combined load from D360's transaction processing and FAST Passive feed, which led to performance degradation.

The PayPower and PayRoute applications were restarted to mitigate CPU usage. FAST Passive feeds were disabled at 20:09 UTC with client confirmation, which stabilised transaction processing and allowed the existing queue to clear. At 00:00 UTC on 28 Nov, FAST Passive was re-enabled. PayPower restarts were completed by 00:13 UTC, and monitoring confirmed stability.",Daniel Velado,Daniel Velado,,,,Banking.Live,D360,,,Client,https://teams.microsoft.com/meet/37800034153083?p=hV036nALwaUSSl0jNw,,,,,,,,,,,2025-11-27T18:21:59.443Z,2025-11-27T18:21:59.443Z,,,,,2025-11-28T02:07:43.463Z,,2025-11-27T21:01:34.770Z,,,,2025-11-28T02:07:43.463Z,19783,2025-11-28T02:07:43.619Z,,public,1812.9,1204.9,294.6,313.4,27nov-p4-313-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C09V7DJBDE3,Reported by Daniel Velado,false
314,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue**: A full outage stopped all transaction processing for VoucherEngine clients. Transactions were consistently declined with a system error code.

**Impact**: All VoucherEngine clients were unable to process any transactions between 14:42 and 16:03 UTC. Transactions were declining with a system error code, and customers were unable to complete payments during this period.

**Causes**: Preliminary investigation suggests an error within MetaDataLogger components

**Steps to resolve**: We restarted the transaction servers and applied multiple emergency code changes to restore transaction processing. Service was fully restored by 16:03 UTC.",Daniel Velado,Daniel Velado,,,,VoucherEngine,,,Currently investigating a full outage affecting all VE clients,Paymentology,https://teams.microsoft.com/meet/35657598402646?p=a4hS4a7a3MwDsHZw7Y,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9312567343/2025-11-30+-+P1+-+Full+Outage+of+Transaction+Processing,2025-11-30T15:10:22.979Z,2025-11-30T15:10:22.979Z,,,,,2025-11-30T16:14:54.641Z,,2025-11-30T16:14:54.641Z,,,,,3871,2025-11-30T22:15:53.472Z,,public,783.6,753.3,30.3,0,30nov-p1-314-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A104WT3B3,Reported by Daniel Velado,false
315,Partial Outage of Transaction Processing,Service Incident,Investigating,P3 - Critical,"Incident to investigate individual occurrences regarding Rule Engine error on UW, BB2, and BAE",Daniel Velado,Daniel Velado,,,,Banking.Live,Utility Warehouse,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9469198348/2025-12-01+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-12-01T16:41:26.187Z,2025-12-01T16:41:26.187Z,,,,,,,,,,,,,2026-02-01T19:21:59.275Z,,public,1820.3,1576.9,204.6,38.9,01dec-p3-critical-315-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A1NC6NWN4,Reported by Daniel Velado,false
316,Partial Outage of Settlement Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"On 28 November 2025 at 16:11 UTC, the client reported an issue with incorrect wallet balances across multiple customer accounts. Customers were reporting sudden negative balances or balance reductions.

Initial investigation confirmed that impacted transactions were marked as ""Settlement-Partial match to spend – Now Completed"", but the system erroneously calculated inflated settlement amounts, leading to incorrect blocked and available balances.

[DEFECT-4454](https://paymentology.atlassian.net/browse/DEFECT-4454 ""DEFECT-4454"") was raised for the issue and the development team determined that the affected transaction logic failed to calculate settlement amounts based on the unique `rid` reference ID, resulting in excessive amounts being blocked.

 ",Bartosz Grenda,Vicnesh Baskar,Rasheed Khan,,,Banking.Live,Utility Warehouse,Apply a fix correcting the broken logic,Negative balance issue for some accounts due to incorrect settlement calculations,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9357000768/2025-12-04+-+P3+-+Critical+-+Partial+Outage+of+Settlement+Processing,2025-12-04T08:57:35.313Z,2025-12-04T08:57:35.313Z,,,,,2025-12-10T13:30:15.416Z,,2025-12-10T13:30:15.416Z,,,,,534760,2025-12-10T13:30:16.682Z,,public,1250.8,1101.4,137.6,11.8,04dec-p3-critical-316-partial-outage-of-settlement-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A1ANCQ199,Reported by Vicnesh Baskar,false
317,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 09 December 2025 at 05:02 UTC, the support team was alerted to socket timeouts and disconnections between multiple Payroute instances to Mastercard, which caused a partial outage of transaction processing.

The first disconnection occurred between 05:06:24 and 05:06:30 UTC, and the second between 05:12:31 and 05:12:36 UTC. Connectivity was fully restored by 05:14:14 UTC. Although transaction flow resumed promptly, transactions continued to normalise until 05:28 UTC.

The Network Team has been engaged for a detailed root cause analysis. Further updates and preventive actions will be detailed in the RCA Report (RCAR).",Vicnesh Baskar,Vicnesh Baskar,Pradeep Boopathe Rajeswari,,Abdul Sahaptheen,Banking.Live,Wio,,Partial outage of transaction processing,Third-Party - Client,,,,,,,,,WIO,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9356541953/2025-12-09+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-12-09T05:55:41.373Z,2025-12-09T05:55:41.373Z,,,,,2025-12-09T06:53:19.844Z,,2025-12-09T06:12:00.137Z,,,,,3458,2026-02-01T19:20:40.144Z,,public,574.2,481.6,34.2,58.4,09dec-p1-317-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A2DGRRYJE,Reported by Vicnesh Baskar,false
318,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"Between 07:53 and 08:05 UTC, several Mastercard endpoints experienced a connection drop on MIP 4DC. Although connectivity was quickly restored, starting around 08:34 UTC, multiple endpoints stopped receiving authorization traffic from MIP 4DB.

The two affected Jeeves Colombia endpoints PROD_CLMC_3C and PROD_SIGM_1C remained online throughout the incident, but Mastercard was not routing transactions to them. While multiple restart attempts successfully refreshed the sign-on, transaction traffic did not immediately resume.

No internal infrastructure changes, deployments, or configuration updates were made during this period. The issue was escalated to Mastercard under Case #07194694, and their teams have confirmed an internal investigation is in progress.

At 15:09:55 UTC, the first stand-in transactions reached the affected Jeeves Colombia endpoints, confirming that transaction traffic had resumed. Mastercard also corrected the logical routing for PROD_OCTO_3B, which restored traffic to that node.

Currently, both impacted Jeeves Colombia endpoints are receiving traffic normally, with no further degradation observed.",Snothile Dlamini,Snothile Dlamini,,,Wafa Ben Nacef,VoucherEngine,Jeeves,,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9379020918/2025-12-09+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-12-09T12:15:09.283Z,2025-12-09T12:15:09.283Z,,,,,2025-12-09T15:47:44.388Z,,2025-12-09T15:47:44.388Z,,,,,12755,2026-02-02T10:15:22.555Z,,public,432.6,422.6,10,0,09dec-p3-critical-318-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A2FPQF4M8,Reported by Snothile Dlamini,false
319,Partial Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P2,"On 10th December at 12:24 UTC, all our database clusters experienced outages due to replication lag caused by an open transaction that was left running during the disabling of a cleanup job related to transaction data processing (iso8583 CDC). This led to timeouts affecting transaction processing. The issue lasted about 20 minutes, with full recovery by 12:44 UTC. The root cause was the open transaction during the job disablement, which was not detected or monitored. The team intervened promptly, stabilized the system, and all clusters returned to normal. Increased reversal activity afterward was expected as the system processed failed transactions. The incident is now resolved, further details will be provided in the RCAR.",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9361850442/2025-12-10+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-12-10T13:19:42.245Z,2025-12-10T13:19:42.245Z,,,,,2025-12-10T12:36:00.000Z,2025-12-10T12:24:00.000Z,,,,,,,2026-02-02T17:19:52.359Z,,public,342.4,292,30.4,20,10dec-p2-319-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A3K6VSX08,Reported by Snothile Dlamini,false
320,Partial Outage of VISA Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"We experience multiple node disconnections from Visa network impacting all Visa transactions for all VE clients:

prod_visa_3a
Start: 2025-11-12 23:01:30 UTC
End: 2025-11-12 23:09:19 UTC

prod_vsea_3a
Start: 2025-11-12 23:01:30 UTC
End: 2025-11-12 23:09:20 UTC

prod_vwin_5a
Start: 2025-11-12 23:01:30 UTC
End: 2025-11-12 23:09:20 UTC",Daniel Velado,Daniel Velado,,,,VoucherEngine,,,,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9374335155/2025-12-11+-+P1+-+Full+Outage+of+VISA+Transaction+Processing,2025-12-11T23:39:27.420Z,2025-12-11T23:39:27.420Z,,,,,,,,,,,,,2026-02-02T17:19:27.951Z,,public,231.2,148.6,54.7,27.9,11dec-p3-critical-320-partial-outage-of-visa-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A37GHPTLL,Reported by Daniel Velado,false
321,Partial Outage of Transaction Processing,Service Incident,Closed,P4,"A subset of transactions, mainly for GoTyme bin, intermittently failed between 04:20 and 05:46 UTC. A blocking query involving high-volume INSERT operations caused intermittent failures in transaction processing.
The blocking query cleared without manual intervention. Error rates returned to normal, and the incident bridge was closed. Transaction processing has since returned to normal.",Vicnesh Baskar,Vicnesh Baskar,Frendi Muhamad,,Ardiansyah Ar,VoucherEngine,,No manual intervention,,Paymentology,,,,,,,,,,,,2025-12-13T05:13:28.666Z,2025-12-13T05:13:28.666Z,,,,,2025-12-13T05:57:49.805Z,,2025-12-13T05:57:49.805Z,,2025-12-13T06:16:22.956Z,,2025-12-13T06:16:22.956Z,2661,2025-12-13T06:16:23.077Z,,public,137.4,137.4,0,0,13dec-p4-321-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A46UZ8EM6,Reported by Vicnesh Baskar,true
322,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On 13 December 2025 at 17:49 UTC, monitoring alerted the team to FAST Active timeouts and dropped transactions. The investigation identified increased CPU utilisation and active session build-up on the Paycore DB. In line with the approved workaround (CCM-2017), the team disabled FAST Passive by updating the URL to a dummy endpoint and restarting all PayPower services (MADA and VISA). Additionally, active sessions on the database were cleared. 

This resulted in normalisation of transaction processing, but later in the day, further issues were observed — despite the FAST Passive URL remaining disabled. The same workaround (restart + DB session clearing) was applied again, and processing returned to normal. The final continuous failure was recorded between 03:32 and 03:33 UTC on 15 December. After that, the system remained stable, and the actual FAST Passive URL was re-enabled on 14 December at 18:30 UTC. 

Since then, transaction processing has remained healthy, with only one brief window of FAST Passive response failure noted. The runbooks have been updated with the expected actions. 

Close monitoring is in place, and the root cause analysis is underway in parallel with queries to the client if there were any changes on their side (e.g., authorisation volume spikes or updates to FAST Active handling) that may have contributed to this recurring pattern.",Vicnesh Baskar,Vicnesh Baskar,,,,Banking.Live,D360,,Partial Outage of Transaction Processing,Paymentology,https://teams.microsoft.com/meet/34450635469104?p=gF0yLF5XbjqeM9Gupq,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9468444749/2025-12-14+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-12-14T15:56:28.124Z,2025-12-14T15:56:28.124Z,,,,,2025-12-15T06:32:56.678Z,,2025-12-14T17:55:46.690Z,,2025-12-15T06:51:59.014Z,,2025-12-15T06:51:59.014Z,52588,2026-02-01T19:13:20.481Z,,public,880.1,591.7,106.8,181.6,14dec-p3-critical-322-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A3LUPEZPW,Reported by Vicnesh Baskar,true
323,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P2,"We identified two brief instances of Rule Engine errors that partially impacted Mox transactions.

1st occurrence:
Start Time: 15-Dec-2025 16:21:07
End Time: 15-Dec-2025 16:22:17
Duration: 70 seconds
Processed transactions: 73
Advice messages: 53
Impacted transactions: 42

2nd occurrence:
Start time: 2025-12-15 17:25:14
End time:  2025-12-15 17:26:27
Duration: 73 seconds
Processed transactions: 51
Advice messages: 27
Impacted transactions: 20",Daniel Velado,Daniel Velado,,,,Banking.Live,MOX,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9391079513/2025-12-16+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-12-16T00:18:57.362Z,2025-12-16T00:18:57.362Z,,,,,2025-12-16T00:19:15.286Z,,2025-12-16T00:19:15.286Z,,2025-12-16T20:19:58.802Z,,,17,2026-02-01T19:09:44.700Z,,public,216.1,181.2,14.9,20,16dec-p2-323-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A3WF4ST36,Reported by Daniel Velado,false
324,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On 13 December 2025 at 16:33 UTC, the support team was alerted via a ticket from the scheme about authorisation issues impacting multiple BINs. The initial investigation revealed that transactions were failing due to timeout errors.

Transaction processing normalised after 16:47 UTC with no further timeout issues observed. No configuration or infrastructure changes were linked to the incident, and no alerts were triggered by internal monitoring.

Subsequent review indicated elevated disk latency on the PayCore database during the incident period, potentially caused by a spike in read operations. The issue was resolved without any manual remediation.",Vicnesh Baskar,Vicnesh Baskar,,,,Banking.Live,,No manual actions taken,Partial Outage of Transaction Processing,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9469526047/2025-12-16+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-12-16T05:14:18.198Z,2025-12-16T05:14:18.198Z,,,,,2025-12-16T05:17:45.485Z,,2025-12-16T05:17:45.485Z,,2025-12-16T05:18:01.815Z,,2025-12-16T05:18:01.815Z,207,2026-01-14T10:17:21.250Z,,public,18,18,0,0,16dec-p3-critical-324-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A3MMGK5NF,Reported by Vicnesh Baskar,true
325,Partial outage of transdactions processing,Service Incident,Post-Incident Tasks & Documentation,P1,VE suffered from an issue that led to a partial outage of transaction processing. Issue was resolved by changing the name of a procedure that allowed backed transactions to clear and processing to start again.,Bartosz Grenda,Bartosz Grenda,,,,VoucherEngine,,Renamed the name of a problematic procedure in the DB,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9393340534/2025-12-16+-+P1+-+Partial+outage+of+transdactions+processing,2025-12-16T09:24:44.151Z,2025-12-16T09:24:44.151Z,,,,,2025-12-16T11:12:05.877Z,,2025-12-16T11:12:05.877Z,,,,,6441,2025-12-16T11:13:20.358Z,,public,2388.7,2189.5,122.9,76.1,16dec-p1-325-partial-outage-of-transdactions-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A4PMJKGP2,Reported by Bartosz Grenda,false
326,Partial Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P2,Partial outage of transaction processing processing was observed from 09:22 UTC - 13:05 UTC on Saturday 20th December. Service was restored after AFS Payroute was restarted. Investigation into the root cause is ongoing.,Kaisar Mahmood,Kaisar Mahmood,,,,Banking.Live,Dopay LLC,Service was restored after AFS Payroute was restarted.,Partial outage of transaction processing ,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9418408014/2025-12-20+-+P2+-+Partial+Outage+of+Transaction+Processing,2025-12-20T13:34:19.415Z,2025-12-20T13:34:19.415Z,,,,,2025-12-22T09:41:21.962Z,2025-12-20T09:22:00.000Z,2025-12-20T13:46:07.671Z,,,,,158822,2026-02-01T19:07:41.574Z,,public,314.4,233.2,50.4,30.8,20dec-p2-326-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A4TQJ0DPC,Reported by Kaisar Mahmood,false
327,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On 21 December 2025 at 15:01 UTC, the Production Support Team was alerted about intermittent timeouts and transaction aborts on the 1 out of 4 MADA PayRoutes for the client, detected via monitoring alerts and client-reported issues. Initial investigation showed errors related to thread pool exhaustion in the **Payroute service**, causing transaction processing failures.

The **Payroute and PayPower services** were restarted, and transaction processing was normalised after **22:40 UTC**, and no further issues were observed. Investigation is ongoing on the root cause of thread exhaustion as well as a detailed impact assessment.",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Kevin Valle,Banking.Live,D360,Restart of the problematic PayRoute and Paypower services,Partial Outage of Transaction Processing,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_ZTUwNjVlMWEtYmZiMy00ODRiLWE1YjUtYTBjMjQ3ZDgwNGE1%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%225bc8ffce-74fa-4e51-af76-275e9877517e%22%7d,,,,,,,,D360,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9418211566/2025-12-21+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-12-21T23:11:44.926Z,2025-12-21T23:11:44.926Z,,,,,2025-12-21T23:35:46.633Z,2025-12-21T15:45:00.000Z,2025-12-21T23:35:46.633Z,,2025-12-22T01:24:51.367Z,,2025-12-22T01:24:51.367Z,1441,2025-12-22T04:21:58.259Z,,public,505.3,421,20,64.3,21dec-p3-critical-327-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A51NHNCKE,Reported by Vicnesh Baskar,true
328,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On 22 Dec at 05:14 UTC, the support team was alerted that 1 out of 2 GoTyme nodes connecting to Bancnet was down. Internal checks confirmed that **one of two PaySwitch nodes (PROD_GOTY_6C)** experienced repeated disconnections. Although the node re-established network connectivity, it **did not resume processing traffic**, resulting in intermittent transaction timeouts and reversals.
Further investigation identified that the GoTyme environment had been operating on a **backup VPN connection** since the previous outage. The VPN exhibited **high volatility**, with frequent socket disconnects and reconnections. Under these conditions, PaySwitch endpoints were observed to **lose internal session state following repeated TCP timeouts**, requiring **manual endpoint restarts** to restore normal transaction processing.
This behaviour was not isolated to a single node and was observed on **both PROD_GOTY_6B and PROD_GOTY_6C**, though impact was more pronounced on PROD_GOTY_6C.
While Paymentology strongly recommended switching back to the **Primary MPLS connection** (the intended design state), the client required time to assess risk and determine an appropriate change window due to business criticality. During this period, additional disconnection windows occurred, each requiring manual intervention.
To address both the **underlying network instability** and the **application-level recovery limitation**, two corrective actions were executed:

• Migration of traffic back to the **Primary MPLS connection**

• Deployment of a **PaySwitch patch** to improve resilience to repeated disconnections and reconnections

Following completion of both changes, the environment has remained stable with no further observed impact.",Vicnesh Baskar,Vicnesh Baskar,Heru Nugroho,,Aneesh S,VoucherEngine,GoTyme,Switch back to primary connection and patch to better handle disconnections,Partial Outage of ATM and POS Transactions,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9469231138/2025-12-22+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2025-12-22T05:46:10.355Z,2025-12-22T05:46:10.355Z,,,,,2025-12-24T08:05:42.672Z,,2025-12-22T06:44:43.937Z,2025-12-24T08:05:42.672Z,2025-12-24T08:07:17.389Z,,2025-12-24T08:07:17.389Z,181172,2026-01-14T10:13:45.533Z,,public,986.8,951,10,25.9,22dec-p3-critical-328-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A530GQADS,Reported by Vicnesh Baskar,true
329,complete outage of transaction processing for VE,Service Incident,Fixing,P1,We seing issues with transaction processing for VE,Bartosz Grenda,Bartosz Grenda,,,,VoucherEngine,,,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9424536013/2025-12-23+-+P1+-+complete+outage+of+transaction+processing+for+VE,2025-12-23T09:17:53.843Z,2025-12-23T09:17:53.843Z,,,,,,2025-12-23T08:08:00.000Z,2025-12-23T09:33:19.833Z,,,,,,2026-02-02T17:17:42.367Z,,public,1761.9,1623.1,107.2,31.6,23dec-p1-329-complete-outage-of-transaction-processing-for-ve,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A57A7PH18,Reported by Bartosz Grenda,false
330,Partial Outage of Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue**: There was a partial outage of transaction processing for three minutes.

**Impact**: Transaction processing was partially unavailable for all clients using the Voucher Engine platform from 11:08 to 11:11 UTC on 24th December.

**Preliminary Causes**: query (UPDATE dbo.TransactionMetaDataSET) which runs as part of stored procedure (dbo.sp.savemetadata) caused blocking on the database which caused an interruption to transaction processing.

**Steps to resolve**: The issue auto-resolved with no action taken by support teams. Transactions have been processing as expected since 11:11 UTC.",Kaisar Mahmood,Kaisar Mahmood,,,,VoucherEngine,,Issue auto resolved,Partial of Outage Transaction Processing from 11:08 – 11:11 UTC 24th December. ,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9425453150/2025-12-24+-+P1+-+Partial+Outage+of+Transaction+Processing,2025-12-24T11:33:26.086Z,2025-12-24T11:33:26.086Z,,,,,2025-12-24T15:00:49.692Z,2025-12-24T11:08:00.000Z,2025-12-24T15:00:49.692Z,,,,,12443,2025-12-24T15:03:45.672Z,,public,316.2,316.2,0,0,24dec-p1-330-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A5EBCMNP4,Reported by Kaisar Mahmood,false
331,Full Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"Client reported transaction declines and suspects a disconnection with PT at 08:20 SAST / 06:47 UTC. Further investigation showed the node PROD_ALTC_4B was up but not receiving any transactions since 07:44 SAST / 05:44 UTC.

The node was restarted, and **transaction processing normalised since 08:37 SAST / 06:37 UTC**. Monitoring and root cause analysis are underway.",Vicnesh Baskar,Vicnesh Baskar,Frendi Muhamad,,Aneesh S,VoucherEngine,,Restart of node PROD_ALTC_4B,,TBD,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9424799460/2025-12-25+-+P1+-+Full+Outage+of+Transaction+Processing,2025-12-25T06:49:21.666Z,2025-12-25T06:49:21.666Z,,,,,2025-12-25T07:01:24.666Z,,2025-12-25T07:01:24.666Z,,2025-12-26T07:30:46.567Z,,,723,2026-02-01T19:01:53.718Z,,public,462.6,442.6,20,0,25dec-p1-331-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A6C450U48,Reported by Vicnesh Baskar,false
332,Full Outage of MADA Transaction Processing,Service Incident,Closed,P3 - Critical,"Between 14:05 and 14:09 UTC, PayRoute experienced a complete disconnection from the MADA network. This resulted in a brief period where MADA transactions could not be processed.

Connectivity was successfully restored at 14:09:30 UTC. We have confirmed that traffic is processing normally across all four PayRoutes, and all services are back to a healthy state. This incident has been resolved.

The Infrastructure team confirmed that PayRoute received continuous connection resets from the MADA endpoint. As we do not have a direct channel with MADA, the issue has been escalated to the client (D360) via Zendesk ticket #1082770. The client will engage MADA to investigate the cause of these resets on their end.",Snothile Dlamini,Snothile Dlamini,,,,Banking.Live,D360,,Full outage of MADA transaction processing,Third-Party - Client,,,,,,,,,D360,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9468903480/2026-01-01+-+P3+-+Critical+-+Full+Outage+of+MADA+Transaction+Processing,2026-01-01T14:49:36.159Z,2026-01-01T14:49:36.159Z,,,,,2026-01-01T15:02:43.207Z,,2026-01-01T15:02:43.207Z,,2026-01-02T15:42:41.330Z,,2026-01-02T15:42:41.330Z,787,2026-01-14T10:07:26.277Z,,public,282.5,211.2,61.3,10,01jan-p3-critical-332-full-outage-of-mada-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A6GJAUV0C,Reported by Snothile Dlamini,true
333,increased amount of declines on SH02,Service Incident,Post-Incident Tasks & Documentation,P2,"**Issue**: From 14:15 UTC, the SH02 environment experienced a high number of transaction declines due to rules engine errors.

**Impact**: Partial outage of transaction processing on the SH02 environment from 14:15 to 14:35 UTC, resulting in 3,624 declined transactions for multiple clients including Rain, Nomo, and Teya.

**Causes**: A spike in database process errors caused the rules engine to fail, leading to transaction declines.

**Steps to resolve**: The database team killed and restarted all database processes, restoring CPU utilization to normal and resuming transaction processing by 14:35 UTC.",Bartosz Grenda,Bartosz Grenda,,,,Banking.Live,,kill and restart db queries causing high CPU usage,Multiple clients impacted,Paymentology,,,,,,,,,SH-02,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9433645102/2026-01-02+-+P2+-+increased+amount+of+declines+on+SH02,2026-01-02T14:28:42.444Z,2026-01-02T14:28:42.444Z,,,,,2026-01-02T15:13:51.406Z,2026-01-02T13:28:00.000Z,2026-01-02T15:13:51.406Z,,,,,2708,2026-01-02T15:41:58.016Z,,public,690.2,581.6,62.9,45.7,02jan-p2-333-increased-amount-of-declines-on-sh02,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A619C18MV,Reported by Bartosz Grenda,false
334,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"**Issue**: High latency and database delays led to a partial outage of transaction processing on the VoucherEngine platform.

**Impact**: Transaction processing on the VoucherEngine platform experienced a brief partial outage between 04:31 and 04:32 UTC. No manual intervention was required, and processing returned to normal automatically.

**Causes**: A blocking database query from the dailyFloatSummaryStatement job caused delays in transaction processing.

**Steps to resolve**: The system automatically recovered and transaction processing normalised without manual intervention. Ongoing investigation and a Jira ticket have been created for further analysis.",Vicnesh Baskar,Vicnesh Baskar,Frendi Muhamad,,Dominic Wee,VoucherEngine,,Automatically recovered,,Paymentology,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9468739633/2026-01-03+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2026-01-03T04:58:44.913Z,2026-01-03T04:58:44.913Z,,,,,2026-01-03T05:16:38.271Z,,2026-01-03T05:16:38.271Z,,2026-01-03T05:16:54.117Z,,2026-01-03T05:16:54.117Z,1073,2026-01-14T10:02:04.372Z,,public,136.4,136.4,0,0,03jan-p3-critical-334-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A6HCFL2T0,Reported by Vicnesh Baskar,true
335,Partial Outage of Create Card API,Service Incident,Root Cause Analysis and Corrective Actions,P2,"On 06 Jan 2026 at 01:03 UTC, monitoring alerts indicated elevated database resource utilisation on the SharedUAE platform. Initial investigation identified intermittent failures on the Create Card API, with application logs reporting database connection errors during the impact window.

The application experienced `Could not open JDBC Connection for transaction` errors, resulting in partial Create Card request failures. A stop/start was done on the PayAPI nodes as a stabilisation measure, after which database connections normalised and error rates reduced.

API processing stabilised, and the last exception was observed at 03:03 UTC. The Create Card API continued to process requests normally under sustained load.",Vicnesh Baskar,Vicnesh Baskar,Rahadian Pratama,,Huy Pham,Banking.Live,"Ziina, Huru, Mango Point, Vaultspay",Stop/Start PayAPI nodes,Some Card Creation API calls are failing due to a surge in number of requests,Paymentology,,,,,,,,,SH-UAE,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9438658819/2026-01-06+-+P2+-+Partial+Outage+of+Create+Card+API,2026-01-06T01:55:32.342Z,2026-01-06T01:55:32.342Z,,,,,2026-01-06T03:35:21.572Z,,2026-01-06T03:35:21.572Z,,2026-01-06T07:38:25.939Z,,,5989,2026-01-06T07:38:26.399Z,,public,486.9,466.1,10.8,10,06jan-p2-335-partial-outage-of-create-card-api,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A6SM4NHDH,Reported by Vicnesh Baskar,false
336,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P3 - Critical,"Multiple Crimson alerts fired off due to a spike in 9XXX errors and higher XML-RPC latency on the deductauth method across several Crimson stacks and hosts.

During investigation, timeouts from VE were observed. Further analysis identified blocking queries on the Primary DB, which contributed to elevated latency and intermittent transaction failures. The issue self-resolved, with metrics returning to normal levels.

Jira raised to DBA Team for root cause: DEFECT-4630.",Snothile Dlamini,Snothile Dlamini,,,,VoucherEngine,,,,Paymentology,,,,,,,,,,,,2026-01-06T12:37:43.855Z,2026-01-06T12:37:43.855Z,,,,,2026-01-06T12:38:15.395Z,2026-01-06T09:16:00.000Z,2026-01-06T12:38:15.395Z,,,,,31,2026-01-06T12:38:16.661Z,,public,229.2,229.2,0,0,06jan-p3-critical-336-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A71UTR0JW,Reported by Snothile Dlamini,false
337,Full Outage of MADA Transaction Processing,Service Incident,Closed,P4,"**Issue**: A full outage of MADA transaction processing occurred due to repeated socket disconnections between PayRoute servers and MADA.

**Impact**: All MADA transactions failed between 23:07 and 23:09 UTC. After recovery actions, transaction processing returned to normal.

**Causes**: Socket disconnections between PayRoute servers and MADA caused transaction processing failures. The issue was observed only for Tweeq.

**Steps to resolve**: We restarted PayRoute services for Tweeq, restoring connections and normal transaction processing. Enhanced monitoring has been set up to ensure ongoing stability.",Vicnesh Baskar,Vicnesh Baskar,Sonia Seh,,Huy Pham,Banking.Live,Tweeq,PR restart,Scheme disconnection affecting transaction processing,TBD,,,,,,,,,TWEEQ,,,2026-01-08T23:35:19.038Z,2026-01-08T23:35:19.038Z,,,,,2026-01-09T00:07:27.317Z,,2026-01-09T00:07:27.317Z,,2026-01-09T00:07:48.389Z,,2026-01-09T00:07:48.389Z,1928,2026-01-09T00:41:44.225Z,,public,176.2,82.8,0,93.4,08jan-p4-337-full-outage-of-mada-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A7LNBNQ3G,Reported by Vicnesh Baskar,true
340,Partial Outage of MADA Transactions and API Processing,Service Incident,Monitoring,P1,"**Issue**: HSM failures caused a partial outage on MADA transactions and APIs that connect to PKS for D360

**Impact**: Some MADA transactions for D360 were declined due to invalid MAC errors, resulting in a partial outage for D360 customers. APIs pws_create_card, pws_get_card_image, sc_get_cvv2, pws_mada_authorize_service, v2_pws_card_renewal and pws_mada_notification were also affected as they connect to PKS.

**Causes**: Failures in both HSM modules prevented them from connecting to PKS, Key validation and expiry dates were confirmed as correct. Investigation is still ongoing regarding the HSM failures

**Steps to resolve**: Since connectivity to the main HSM IP address was not possible, we failed over to DR HSM IP addresses. The configuration was changed from 10.60.13.10 to 10.90.13.10 (DR) on both PKS hosts and the paykeyserv Java service was restarted. After these changes, MADA transactions began processing successfully, and normal service was restored.

Start Time: 02:40 UTC
End time:  04:00 UTC ",Daniel Velado,Daniel Velado,,,,Banking.Live,D360,,,Client,https://teams.microsoft.com/meet/36961007275082?p=xRGbodJ8VCR0wIEz00,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9475424294/2026-01-15+-+P1+-+Partial+Outage+of+MADA+Transactions,2026-01-15T03:05:25.158Z,2026-01-15T03:05:25.158Z,,,,,2026-01-15T04:23:34.541Z,,2026-01-15T04:23:34.541Z,,2026-01-15T06:53:05.963Z,,,4689,2026-01-30T16:09:22.841Z,,public,1464.8,1099.9,118.1,246.8,15jan-p1-340-partial-outage-of-mada-transactions-and-api-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0A8SDSU678,Reported by Daniel Velado,false
341,Full Outage of CORTEX Transaction Processing ,Service Incident,Post-Incident Tasks & Documentation,P1,"Client BOJ experienced a disconnection from CORTEX between 15:27 UTC and 16:27 UTC.
The PayRoute reconnection configuration setting did not attempt to reconnect, and the service was only reestablished after a manual restart of the application.",Daniel Velado,Daniel Velado,,,,Banking.Live,Bank of Jordan,,,TBD,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9488629821/2026-01-18+-+P1+-+Full+Outage+of+CORTEX+Transaction+Processing,2026-01-18T20:41:02.397Z,2026-01-18T20:41:02.397Z,,,,,2026-01-18T20:41:21.900Z,,2026-01-18T20:41:21.900Z,,,,,19,2026-01-30T16:11:19.710Z,,public,406.8,260.2,136.6,10,18jan-p1-341-full-outage-of-cortex-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0AAAB4FQC8,Reported by Daniel Velado,false
342,Full Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"On 22 Jan 2026 at 03:00 UTC, immediately following the conclusion of an approved change risk window for the BL 3.4 release, a full outage of transaction processing was observed. Monitoring alerts indicated that authorisation requests were failing, resulting in transactions being unable to process.

Investigation identified that a trailing whitespace character was present in the authorisation request URL. This whitespace had existed before the release and had been handled correctly by the system. Following the release, the whitespace resulted in request parsing failures, impacting transaction processing.
The issue was resolved by correcting the request handling and restarting the affected transaction processing service.

Transaction processing normalised at 03:05 UTC, resulting in an outage duration of approximately five minutes. Post-recovery validation was completed, and heightened monitoring is currently in place.",Vicnesh Baskar,Vicnesh Baskar,Sonia Seh,,Huy Pham,Banking.Live,TymeBank SA,Configuration change,Full Outage of Transaction Processing,Paymentology,,,,,,,,,TYMEBANK,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9503997956/2026-01-22+-+P1+-+Full+Outage+of+Transaction+Processing,2026-01-22T03:07:31.655Z,2026-01-22T03:07:31.655Z,,,,,2026-01-22T03:49:21.795Z,,2026-01-22T03:49:21.795Z,,,,,2510,2026-01-22T07:37:22.457Z,,public,393.2,341.8,7.8,43.6,22jan-p1-342-full-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0AA44QAEG5,Reported by Vicnesh Baskar,false
344,Partial Outage of Transaction Processing,Service Incident,Closed,P3 - Critical,"On 23 January 2026 at 03:40 UTC, Paymentology monitoring systems detected elevated latency impacting transaction processing. Initial investigation identified intermittent transaction timeouts occurring during short windows, correlated with elevated database sessions and blocking behaviour linked to a scheduled processing query running longer than expected.

Transaction processing performance normalised during the investigation period, with timeouts no longer observed before mitigation. However, analysis confirmed that the underlying query continued to spawn multiple concurrent sessions despite recompilation.

As a precautionary stabilisation measure, a controlled restart of the scheduler server was performed at approximately 04:35 UTC. Following the restart, the query execution returned to normal.

Transaction processing remained stable throughout and post-mitigation, with no further timeouts observed. Heightened monitoring is in place to ensure continued stability.",Vicnesh Baskar,Vicnesh Baskar,Megalatha Ramakrishnan,,Wilson Keneshiro,VoucherEngine,,Restart of scheduler server,Intermittern transaction timeouts,Paymentology,https://teams.microsoft.com/l/meetup-join/19%3ameeting_NGM1OTRkYmEtOTMzYi00ZTI3LWI2MzYtOTM1ZGY3ZmIzOGI2%40thread.v2/0?context=%7b%22Tid%22%3a%224f7797e2-a59f-4381-ad8e-b81223603c9d%22%2c%22Oid%22%3a%225bc8ffce-74fa-4e51-af76-275e9877517e%22%7d,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9508094039/2026-01-23+-+P3+-+Critical+-+Partial+Outage+of+Transaction+Processing,2026-01-23T04:56:01.052Z,2026-01-23T04:56:01.052Z,,,,,2026-01-23T06:12:46.632Z,,2026-01-23T06:12:46.632Z,,2026-01-23T06:13:14.617Z,,2026-01-23T06:13:14.617Z,4605,2026-01-23T07:33:16.213Z,,public,282.3,282.3,0,0,23jan-p3-critical-344-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0AAGJU63NE,Reported by Vicnesh Baskar,true
345,Partial Outage of Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"**Issue**: A full outage of transaction processing occurred due to both Mastercard connectivity links (4IR and 00B) being unavailable.

**Impact**: All Mastercard clients except MOX and ISB were unable to process transactions for 38 minutes, from 12:38 to 13:15 UTC. Transaction processing has since resumed for all clients.

**Causes**: A scheduled maintenance on the Mastercard side made the 4IR link unavailable. Unexpectedly, the 00B link also became unavailable, resulting in a full outage.

**Steps to resolve**: Transaction validation was completed for all clients. The 00B link reconnected automatically, restoring service without manual intervention.",Vicnesh Baskar,Vicnesh Baskar,Zoja Mitric,,Rajesh Subramanian,Banking.Live,Old Mutual Bank / Olympus,Auto reconnection,Full outage of transaction processing,Third-Party - Client,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9513239958/2026-01-24+-+P1+-+Full+Outage+of+Transaction+Processing,2026-01-24T13:46:36.934Z,2026-01-24T13:46:36.934Z,,,,,2026-01-24T14:20:59.955Z,2026-01-24T13:41:00.000Z,2026-01-24T14:20:59.955Z,,2026-01-26T07:02:23.810Z,,,2063,2026-01-30T16:15:38.782Z,,public,470.9,325.3,124.9,20.7,24jan-p1-345-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0AB46UH30R,Reported by Vicnesh Baskar,false
346,Brief disruption of transaction processing,Service Incident,Closed,P3 - Critical,"There was a brief disturbance with Virtual WAN between AWS and Azure which caused the connection to fall back to a standby connection. This caused a brief transaction processing issue for a max of 2 minutes for Visa clients on SH02, ADQ, BAE.

Once the failover was complete all transaction processing has returned to normal. No action was needed from our side.",Bartosz Grenda,Bartosz Grenda,,,,Banking.Live,,No action was needed from Paymentology. Failover worked as intended.,,Third-Party - Paymentology,,,,,,,,,"SH-02, WIO, BAE",,,2026-01-26T10:12:36.526Z,2026-01-26T10:12:36.526Z,,,,,2026-01-26T10:21:40.903Z,,2026-01-26T10:21:40.903Z,,2026-01-26T10:24:58.967Z,,2026-01-26T10:24:58.967Z,544,2026-02-02T12:09:01.688Z,,public,126,126,0,0,26jan-p3-critical-346-brief-disruption-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0AB1KQLS9J,Reported by Bartosz Grenda,true
347,Full Outage of Jonet Transaction Processing,Service Incident,Root Cause Analysis and Corrective Actions,P1,"On 28 January 2026 at 01:43 UTC, the support team was alerted to a connectivity issue between PayRoute and the Cortex interface, following a socket timeout and subsequent disconnection. Initial investigation showed that while PayRoute automatically reconnected to Cortex within seconds, transaction authorisations did not resume as expected after the reconnection.

Further monitoring identified an absence of authorisation traffic despite the interface being technically connected. As a corrective action, Jonet PayRoute was manually restarted on 28 January 2026 at 01:47 UTC, after which transaction processing normalised immediately with no further issues observed.

A follow-up investigation will be conducted with the relevant engineering team to determine why authorisations did not resume after the automatic reconnection and to prevent recurrence.",Vicnesh Baskar,Vicnesh Baskar,Sonia Seh,,Dominic Wee,Banking.Live,Bank of Jordan,Payroute restart,Full Outage of Jonet Transaction Processing,TBD,,,,,,,,,BOJ,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9529459046/2026-01-28+-+P1+-+Full+Outage+of+Jonet+Transaction+Processing,2026-01-28T02:27:41.685Z,2026-01-28T02:27:41.685Z,,,,,2026-01-28T02:38:52.725Z,2026-01-27T14:11:00.000Z,2026-01-28T02:38:52.725Z,,2026-01-28T05:57:07.100Z,,,671,2026-01-30T16:17:44.692Z,,public,761.6,559.3,172.5,30,28jan-p1-347-full-outage-of-jonet-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0ABGKDS2JY,Reported by Vicnesh Baskar,false
348,Partial Outage of Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"**Issue**: Transaction processing was partially impacted for Inswitch/Banco Agricola, with 9120 system errors recorded on BIN 522730 for campaign 2022.

**Impact**: Transactions for Inswitch/Banco Agricola on BIN 522730, limited to campaignID=2022, were declining with error code 9120. 9120 system errors were observed from 19:46 to 21:21 UTC.

**Causes**: A campaign setting was changed to enable Aconite EMV validation for Inswitch Banco Agricola Physical Companion Card at 19:45 UTC. This caused internal validation errors due to missing data and BIN not set up on Aconite, resulting in transactions declining with error code 9120.

**Steps to resolve**: We disabled the Aconite EMV validation setting at 21:20 UTC, which stopped the declines and allowed transactions to resume. The campaign setting was enabled again as the BIN has been correctly set up on Aconite. ",Daniel Velado,Daniel Velado,,,,VoucherEngine,Inswitch,,,Paymentology,https://teams.microsoft.com/meet/39577708342938?p=ZsMb9FAMGLOAbwJSyM,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9533587644/2026-01-28+-+P1+-+Partial+Outage+of+Transaction+Processing,2026-01-28T20:43:26.298Z,2026-01-28T20:43:26.298Z,,,,,2026-01-29T15:29:08.018Z,,2026-01-28T21:32:31.709Z,,,,,2945,2026-02-02T09:24:05.680Z,,public,1050.8,968,82.8,0,28jan-p1-348-partial-outage-of-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0AB4LC49FH,Reported by Daniel Velado,false
349,Partial Outage of MC Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P2,"On January 28th at 22:31 UTC, Paymentology Monitoring systems triggered regarding an increase in advice messages for MOX.
Advice messages started to arrive at 22:16:23, and transactions resumed at 22:18:07 without Paymentology intervention.
Preliminary investigation showed that there was more than 7 seconds' delay between the transaction initiation time in Terminal ( de7) and the transaction request we received in our system. 
Due to this delay, some transactions are being timed out.
We had a similar case before,e which was raised to MC (Case#06171960), and a new case will be created for them to investigate today's occurrence. ",Daniel Velado,Daniel Velado,,,,Banking.Live,MOX,,,TBD,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9536667651/2026-01-28+-+P1+-+Partial+Outage+of+MC+Transaction+Processing,2026-01-28T23:54:40.130Z,2026-01-28T23:54:40.130Z,,,,,2026-01-28T23:55:11.121Z,,2026-01-28T23:55:11.121Z,,,,,30,2026-01-30T16:21:07.357Z,,public,162.5,100.3,11,51.2,28jan-p2-349-partial-outage-of-mc-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0ABQQSVDJQ,Reported by Daniel Velado,false
350,Partial Outage of MC Transaction Processing,Service Incident,Post-Incident Tasks & Documentation,P1,"ISB experienced a socket disconnection from MC 00B endpoint for ISB between 14:41:47 and  14:44:27
Successful transactions: 181.
Impacted transactions: 151.
An issue has been raised with MC, as there is no firewall between our switching application and the MIP. ",Daniel Velado,Daniel Velado,,,,Banking.Live,Islandsbanki,,,TBD,,,,,,,,,,,https://paymentology.atlassian.net/wiki/spaces/TS/pages/9540698147/2026-01-29+-+P1+-+Partial+Outage+of+MC+Transaction+Processing,2026-01-29T17:30:54.514Z,2026-01-29T17:30:54.514Z,,,,,2026-01-29T17:31:08.979Z,,2026-01-29T17:31:08.979Z,,,,,14,2026-01-30T18:37:35.603Z,,public,152.6,137.7,0,14.9,29jan-p1-350-partial-outage-of-mc-transaction-processing,https://slack.com/app_redirect?team=T03SMJWS8&channel=C0AC6CJ5HB3,Reported by Daniel Velado,false
